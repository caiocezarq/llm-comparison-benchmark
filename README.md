# ğŸ¤– Framework de ComparaÃ§Ã£o de Modelos de Linguagem (LLMs)

<p align="center">

<a>
  <img src="https://img.shields.io/badge/Status-Ativo-success?style=for-the-badge">
</a>
<a>
  <img src="https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge&logo=python&logoColor=white">
</a>
<a href="https://opensource.org/licenses/MIT">
  <img src="https://img.shields.io/badge/LicenÃ§a-MIT-yellow?style=for-the-badge">
</a>
<a>
  <img src="https://img.shields.io/badge/EvidentlyAI-Enabled-purple?style=for-the-badge">
</a>

</p>

Framework desenvolvido para **comparar e avaliar modelos de linguagem (LLMs)** de forma **objetiva**, utilizando **mÃ©tricas acadÃªmicas**, **benchmarks padronizados** e **relatÃ³rios estruturados**.  
Ideal para **pesquisa acadÃªmica**, **anÃ¡lise de performance** e **seleÃ§Ã£o de modelos para produÃ§Ã£o**.

---

## ğŸ¯ Principais Recursos

- ExecuÃ§Ã£o automatizada de modelos usando prompts padronizados
- CÃ¡lculo de mÃ©tricas: **BLEU**, **ROUGE**, **BERTScore**
- Benchmarks: **MMLU** (conhecimento geral) e **HellaSwag** (senso comum)
- AnÃ¡lise de distribuiÃ§Ã£o e consistÃªncia com **EvidentlyAI**
- Sistema de **ranking comparativo** (normalizaÃ§Ã£o min-max)
- RelatÃ³rios **por modelo** + **Consolidado final**
- Estrutura modular com **reprodutibilidade garantida**

---

## ğŸ§± Arquitetura do Projeto

```
llm-comparison-benchmark/
â”œâ”€â”€ main.py                     # ExecuÃ§Ã£o principal
â”œâ”€â”€ .env.example                # Exemplo de configuraÃ§Ã£o
â”œâ”€â”€ requirements.txt            # DependÃªncias
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py               # ConfiguraÃ§Ãµes centrais
â”‚   â”œâ”€â”€ pipeline.py             # ExecuÃ§Ã£o dos prompts
â”‚   â”œâ”€â”€ models.py               # Wrappers para APIs
â”‚   â”œâ”€â”€ utils.py                # FunÃ§Ãµes auxiliares
â”‚   â””â”€â”€ logger.py               # Sistema de logs
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ prompts.json            # Prompts estruturados
â”‚   â””â”€â”€ benchmarks.json         # Benchmarks MMLU / HellaSwag
â”‚
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ analysis.py             # ConsolidaÃ§Ã£o das mÃ©tricas
â”‚   â”œâ”€â”€ ranking_system.py       # GeraÃ§Ã£o de rankings
â”‚   â”œâ”€â”€ bleu_rouge.py           # BLEU / ROUGE
â”‚   â”œâ”€â”€ bertscore.py            # BERTScore
â”‚   â”œâ”€â”€ mmlu.py                 # MMLU
â”‚   â”œâ”€â”€ hellaswag.py            # HellaSwag
â”‚   â””â”€â”€ evidently_reports.py    # RelatÃ³rios EvidentlyAI
â”‚
â””â”€â”€ results/                    # Resultados versionados
```

---

## ğŸ¤– Modelos Suportados

| Modelo | API | Categoria |
|---|---|---|
| **LLaMA 3.x** | Groq | Open Source |
| **Qwen 3** | Groq | Open Source |
| **GPT-OSS 20B / 120B** | OpenAI | Open Weight |
| **Gemini Flash / Flash-Lite** | Google AI | ProprietÃ¡rio |

---

## âš™ï¸ InstalaÃ§Ã£o

```bash
git clone https://github.com/caiocezarq/llm-comparison-benchmark
cd llm-comparison-benchmark
pip install -r requirements.txt
```

Configurar chaves:

```bash
cp .env.example .env
```

Preencha:

```
GROQ_API_KEY=
OPENAI_API_KEY=
GOOGLE_API_KEY=
```

---

## ğŸš€ Como Executar

Executar pipeline completo:

```bash
python main.py
```

Rodar somente anÃ¡lise:

```bash
python -m analysis.analysis
```

Gerar somente rankings:

```bash
python -m analysis.ranking_system
```

---

## ğŸ“Š MÃ©tricas e Benchmarks Implementados

| Tipo | Nome | PropÃ³sito |
|---|---|---|
| Similaridade LÃ©xica | **BLEU / ROUGE** | Avalia proximidade linguÃ­stica |
| Similaridade SemÃ¢ntica | **BERTScore** | Mede equivalÃªncia de significado |
| Conhecimento Geral | **MMLU** | Avalia entendimento multitarefa |
| RaciocÃ­nio de Senso Comum | **HellaSwag** | Avalia coerÃªncia contextual |
| ConsistÃªncia de Texto | **EvidentlyAI** | DistribuiÃ§Ã£o, drift e qualidade |

---

## ğŸ“ SaÃ­das do Sistema

```
results/
â””â”€â”€ resultado_YYYYMMDD_HHMMSS/
    â”œâ”€â”€ respostas.csv
    â”œâ”€â”€ metricas_normalizadas.json
    â”œâ”€â”€ relatorio_consolidado.md
    â””â”€â”€ modelo_X/
         â”œâ”€â”€ relatorio_individual.md
         â””â”€â”€ evidently_reports/*.html
```

---

## ğŸ“ Uso AcadÃªmico

âœ” Ideal para **TCC, artigos, dissertaÃ§Ãµes e relatÃ³rios tÃ©cnicos**  
âœ” Metodologia reprodutÃ­vel  
âœ” Resultados exportÃ¡veis e citÃ¡veis  
âœ” Benchmarks amplamente utilizados na literatura

---

## ğŸ¤ Contribuindo

```bash
git checkout -b feature/minha-feature
git commit -m "DescriÃ§Ã£o clara"
git push origin feature/minha-feature
```

Depois Ã© sÃ³ abrir um **Pull Request**.

---

## ğŸ“„ LicenÃ§a

DistribuÃ­do sob licenÃ§a **MIT** â€” uso livre para fins acadÃªmicos e comerciais.
