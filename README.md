# ü§ñ Sistema de Compara√ß√£o e An√°lise de Modelos LLM

Sistema completo para compara√ß√£o de modelos de linguagem (LLM) com m√©tricas acad√™micas, an√°lise de qualidade de dados e relat√≥rios consolidados.

## üìã Resumo

Este projeto implementa um pipeline automatizado para testar e comparar diferentes modelos de linguagem atrav√©s de prompts padronizados, calculando m√©tricas de qualidade acad√™mica (BLEU, ROUGE, BERTScore) e an√°lises de qualidade de dados usando Evidently AI. O sistema gera relat√≥rios detalhados por modelo e um relat√≥rio consolidado final.

## üéØ Objetivo

- **Comparar performance** de diferentes modelos LLM em tarefas padronizadas
- **Avaliar qualidade** das respostas usando m√©tricas acad√™micas reconhecidas
- **Analisar dados** com ferramentas profissionais de qualidade
- **Gerar relat√≥rios** consolidados para tomada de decis√£o
- **Automatizar** todo o processo de teste e an√°lise

## üìÅ Estrutura do Projeto

```
LLM/
‚îú‚îÄ‚îÄ üìÑ main.py                          # Ponto de entrada principal
‚îú‚îÄ‚îÄ üìÑ requirements.txt                 # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ README.md                        # Este arquivo
‚îú‚îÄ‚îÄ üìÑ .env.example                     # Exemplo de vari√°veis de ambiente
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                             # C√≥digo fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ config.py                    # Configura√ß√µes centralizadas
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pipeline.py                  # Pipeline de execu√ß√£o dos LLMs
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ utils.py                     # Utilit√°rios e fun√ß√µes auxiliares
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ logger.py                    # Sistema de logging
‚îÇ
‚îú‚îÄ‚îÄ üìÅ analysis/                        # Sistema de an√°lise
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ analysis.py                  # Orquestrador principal de an√°lise
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ bleu_rouge.py               # C√°lculo de m√©tricas BLEU/ROUGE
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ bertscore.py                # C√°lculo de m√©tricas BERTScore
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ evidently_reports.py        # Gera√ß√£o de relat√≥rios Evidently AI
‚îÇ
‚îú‚îÄ‚îÄ üìÅ results/                         # Resultados das execu√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ resultado_1/                # Execu√ß√£o 1
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ resultado_2/                # Execu√ß√£o 2
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ resultado_N/                # Execu√ß√£o N
‚îÇ
‚îî‚îÄ‚îÄ üìÅ analysis/                        # An√°lises consolidadas
    ‚îî‚îÄ‚îÄ üìÅ analise_consolidada_YYYYMMDD_HHMMSS/
        ‚îú‚îÄ‚îÄ üìÑ relatorio_consolidado.md
        ‚îú‚îÄ‚îÄ üìÅ modelo_X/
        ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ relatorio_modelo_X.md
        ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ evidently_reports/
        ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ evidently_qualidade.html
        ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ evidently_texto.html
        ‚îî‚îÄ‚îÄ üìÑ metricas_consolidadas.json
```

## ‚öôÔ∏è Configura√ß√µes Necess√°rias

### 1. Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# APIs dos Modelos
GROQ_API_KEY=sua_chave_groq_aqui
OPENAI_API_KEY=sua_chave_openai_aqui
GOOGLE_API_KEY=sua_chave_google_aqui

# Configura√ß√µes de Execu√ß√£o
NUMERO_EXECUCOES=3
TIMEOUT_ENTRE_EXECUCOES=30
TIMEOUT_ENTRE_PERGUNTAS=3
```

### 2. Instala√ß√£o de Depend√™ncias

```bash
pip install -r requirements.txt
```

### 3. Configura√ß√£o de Modelos

Edite `src/config.py` para ajustar:
- Lista de modelos a testar
- Par√¢metros de gera√ß√£o (max_tokens, temperature)
- Timeouts e limites de taxa

## üìÑ Descri√ß√£o dos Arquivos

### Arquivos Principais

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `main.py` | **Ponto de entrada** - Executa m√∫ltiplas execu√ß√µes da pipeline e inicia an√°lise consolidada |
| `requirements.txt` | **Depend√™ncias** - Lista de pacotes Python necess√°rios |
| `.env.example` | **Template** - Exemplo de configura√ß√£o de vari√°veis de ambiente |

### C√≥digo Fonte (`src/`)

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `config.py` | **Configura√ß√µes** - Centraliza todas as configura√ß√µes do sistema (modelos, timeouts, paths) |
| `pipeline.py` | **Pipeline Principal** - Executa prompts contra m√∫ltiplos LLMs e coleta respostas |
| `utils.py` | **Utilit√°rios** - Fun√ß√µes para salvamento de dados, gerenciamento de pastas e formata√ß√£o |
| `logger.py` | **Logging** - Sistema de logs estruturado para monitoramento e debug |

### Sistema de An√°lise (`analysis/`)

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `analysis.py` | **Orquestrador** - Classe principal que coordena toda a an√°lise consolidada |
| `bleu_rouge.py` | **M√©tricas BLEU/ROUGE** - Calcula m√©tricas de qualidade de tradu√ß√£o/summariza√ß√£o |
| `bertscore.py` | **M√©tricas BERTScore** - Calcula similaridade sem√¢ntica usando embeddings BERT |
| `evidently_reports.py` | **Relat√≥rios Evidently** - Gera an√°lises de qualidade de dados em HTML |

## ü§ñ Modelos Utilizados

### Modelos Locais (Groq)
- **llama3_8b** - `llama-3.1-8b-instant`
- **llama3_70b** - `llama-3.3-70b-versatile`
- **qwen_32b** - `qwen/qwen3-32b`
- **deepseek_70b** - `deepseek-r1-distill-llama-70b`

### Modelos Open Source (Groq)
- **gpt_oss_20b** - `openai/gpt-oss-20b`
- **gpt_oss_120b** - `openai/gpt-oss-120b`

### Modelos Google (Gemini)
- **gemini_1_5_flash** - `models/gemini-1.5-flash`
- **gemini_2_5_flash_lite** - `models/gemini-2.5-flash-lite`

## üìä M√©tricas Implementadas

### M√©tricas Acad√™micas
- **BLEU Score** - Precis√£o de n-gramas para avalia√ß√£o de qualidade
- **ROUGE-1** - Sobreposi√ß√£o de unigramas
- **ROUGE-2** - Sobreposi√ß√£o de bigramas  
- **ROUGE-L** - Sobreposi√ß√£o de subsequ√™ncia mais longa
- **BERTScore** - Similaridade sem√¢ntica usando embeddings BERT

### M√©tricas de Qualidade (Evidently AI)
- **Distribui√ß√£o de Comprimento** - An√°lise do tamanho das respostas
- **Contagem de Palavras** - Estat√≠sticas de vocabul√°rio
- **Qualidade do Texto** - Detec√ß√£o de problemas de formata√ß√£o
- **Drift de Dados** - Detec√ß√£o de mudan√ßas na distribui√ß√£o
- **M√©tricas Estat√≠sticas** - M√©dia, mediana, desvio padr√£o, outliers

## üîç Sistema de An√°lise

### 1. Coleta de Dados
- Executa prompts padronizados contra todos os modelos
- Coleta respostas com metadados (timestamp, comprimento, flags de erro)
- Salva resultados em formato CSV e JSON

### 2. Processamento
- Consolida dados de m√∫ltiplas execu√ß√µes por modelo
- Filtra respostas v√°lidas vs. com erro
- Calcula m√©tricas acad√™micas e de qualidade

### 3. Gera√ß√£o de Relat√≥rios
- **Relat√≥rios por Modelo** - An√°lise individual detalhada
- **Relat√≥rios Evidently AI** - An√°lises de qualidade em HTML
- **Relat√≥rio Consolidado** - Compara√ß√£o final com ranking

### 4. Ranking e Compara√ß√£o
- Score composto baseado em m√©tricas acad√™micas
- Fator de confiabilidade baseado em taxa de sucesso
- Penaliza√ß√µes para modelos com poucas respostas v√°lidas

## üöÄ Como Usar

### Execu√ß√£o Completa
```bash
python main.py
```

### Execu√ß√£o da An√°lise Apenas
```bash
python -m analysis.analysis
```

### Configura√ß√£o Personalizada
1. Edite `src/config.py` para ajustar modelos e par√¢metros
2. Configure vari√°veis de ambiente no `.env`
3. Execute `python main.py`

## üìà Exemplo de Sa√≠da

### Relat√≥rio Consolidado
```markdown
# üìä Relat√≥rio Consolidado de An√°lise de Modelos LLM

**Data da An√°lise**: 04/01/2025 20:58:15
**Total de Respostas**: 480
**Modelos Testados**: 8
**Execu√ß√µes**: resultado_1, resultado_2, resultado_3

## üèÜ Ranking dos Modelos

1. **qwen_32b** - Score: 8.45
2. **deepseek_70b** - Score: 8.32
3. **llama3_70b** - Score: 8.18
...

## üìä M√©tricas por Modelo

### qwen_32b
- **BLEU Score**: 0.234
- **ROUGE-1**: 0.456
- **BERTScore**: 0.789
- **Taxa de Sucesso**: 95.8%
```

## üõ†Ô∏è Depend√™ncias

### Principais
- `pandas>=1.5.0` - Manipula√ß√£o de dados
- `numpy>=1.21.0` - Computa√ß√£o num√©rica
- `transformers>=4.30.0` - Modelos BERT para BERTScore
- `evaluate>=0.4.0` - M√©tricas de avalia√ß√£o
- `evidently[llm]>=0.7.14` - An√°lise de qualidade de dados

### APIs
- `groq>=0.9.0` - API Groq para modelos locais
- `openai>=1.0.0` - API OpenAI
- `google-generativeai>=0.8.0` - API Google Gemini

### Utilit√°rios
- `beautifulsoup4>=4.12.0` - Parsing HTML
- `python-dotenv>=1.0.0` - Gerenciamento de vari√°veis de ambiente
- `requests>=2.28.0` - Requisi√ß√µes HTTP

## üîß Configura√ß√µes Avan√ßadas

### Ajuste de Modelos
```python
# src/config.py
MODELOS_GROQ = {
    "llama3_8b": "llama-3.1-8b-instant",
    "llama3_70b": "llama-3.3-70b-versatile",
    # Adicione novos modelos aqui
}
```

### Ajuste de Prompts
```python
# src/config.py
PROMPTS = [
    "O que √© intelig√™ncia artificial e como ela est√° transformando o mundo?",
    "Explique a diferen√ßa entre machine learning e deep learning.",
    # Adicione novos prompts aqui
]
```

### Ajuste de M√©tricas
```python
# analysis/analysis.py
def _calcular_ranking_modelos(self, metricas_por_modelo):
    # Ajuste os pesos das m√©tricas aqui
    peso_bleu = 0.2
    peso_rouge = 0.3
    peso_bertscore = 0.5
```

## üêõ Troubleshooting

### Problemas Comuns

1. **Erro de API Key**
   - Verifique se as chaves est√£o corretas no `.env`
   - Confirme se as APIs est√£o ativas

2. **Erro de Depend√™ncias**
   - Execute `pip install -r requirements.txt`
   - Verifique a vers√£o do Python (3.8+)

3. **Erro de Mem√≥ria**
   - Reduza `NUMERO_EXECUCOES` no config
   - Use modelos menores primeiro

4. **Rate Limits**
   - Aumente `TIMEOUT_ENTRE_PERGUNTAS`
   - Execute menos modelos por vez

## üìù Logs e Debug

O sistema gera logs detalhados em:
- Console durante execu√ß√£o
- Arquivos de log em `logs/` (se configurado)
- Relat√≥rios de erro em `results/resultado_X/`

## ü§ù Contribui√ß√£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para detalhes.

## üìû Suporte

Para d√∫vidas ou problemas:
1. Verifique a se√ß√£o Troubleshooting
2. Consulte os logs de erro
3. Abra uma issue no reposit√≥rio

---

**Desenvolvido com ‚ù§Ô∏è para compara√ß√£o e an√°lise de modelos LLM**