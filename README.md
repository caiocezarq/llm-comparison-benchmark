# ü§ñ Sistema de Compara√ß√£o e An√°lise de Modelos LLM

Sistema completo para compara√ß√£o de modelos de linguagem (LLM) com m√©tricas acad√™micas, an√°lise de qualidade de dados, sistema de ranking comparativo e relat√≥rios consolidados.

## üìã Resumo

Este projeto implementa um pipeline automatizado para testar e comparar diferentes modelos de linguagem atrav√©s de prompts padronizados, calculando m√©tricas de qualidade acad√™mica (BLEU, ROUGE, BERTScore) e an√°lises de qualidade de dados usando Evidently AI. O sistema gera relat√≥rios detalhados por modelo, rankings comparativos e um relat√≥rio consolidado final com an√°lise qualitativa.

## üéØ Objetivo

- **Comparar performance** de diferentes modelos LLM em tarefas padronizadas
- **Avaliar qualidade** das respostas usando m√©tricas acad√™micas reconhecidas
- **Testar benchmarks** padronizados (MMLU, HellaSwag) para avalia√ß√£o comparativa
- **Analisar dados** com ferramentas profissionais de qualidade (Evidently AI)
- **Gerar rankings** comparativos com normaliza√ß√£o e an√°lise qualitativa
- **Automatizar** todo o processo de teste, an√°lise e compara√ß√£o
- **Fornecer insights** acad√™micos para tomada de decis√£o

## üìÅ Estrutura do Projeto

```
LLMv3/
‚îú‚îÄ‚îÄ üìÑ main.py                          # Ponto de entrada principal
‚îú‚îÄ‚îÄ üìÑ requirements.txt                 # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ README.md                        # Este arquivo
‚îú‚îÄ‚îÄ üìÑ .env.example                     # Exemplo de vari√°veis de ambiente
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                             # C√≥digo fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ config.py                    # Configura√ß√µes centralizadas
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pipeline.py                  # Pipeline de execu√ß√£o dos LLMs
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ models.py                    # Implementa√ß√£o dos modelos
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ utils.py                     # Utilit√°rios e fun√ß√µes auxiliares
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ logger.py                    # Sistema de logging
‚îÇ
‚îú‚îÄ‚îÄ üìÅ prompts/                         # Prompts e benchmarks
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ prompts.json                 # 20 prompts padronizados estruturados
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ benchmarks.json              # Benchmarks padronizados
‚îÇ
‚îú‚îÄ‚îÄ üìÅ analysis/                        # Sistema de an√°lise avan√ßada
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py                  # Pacote de an√°lise
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ analysis.py                  # Orquestrador principal de an√°lise
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ ranking_system.py            # Sistema de ranking comparativo
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ benchmarks.py                # Classe base para benchmarks
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ mmlu.py                      # Calculadora MMLU
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ hellaswag.py                 # Calculadora HellaSwag
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ bleu_rouge.py               # C√°lculo de m√©tricas BLEU/ROUGE
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ bertscore.py                # C√°lculo de m√©tricas BERTScore
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ evidently_reports.py        # Gera√ß√£o de relat√≥rios Evidently AI
‚îÇ
‚îú‚îÄ‚îÄ üìÅ results/                         # Resultados das execu√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ resultado_1/                # Execu√ß√£o 1
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ resultado_2/                # Execu√ß√£o 2
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ resultado_N/                # Execu√ß√£o N
‚îÇ
‚îú‚îÄ‚îÄ üìÅ analysis/                        # An√°lises consolidadas
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ analise_consolidada_YYYYMMDD_HHMMSS/
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ relatorio_consolidado.md
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ rankings.md              # Rankings comparativos
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ normalized_metrics.json  # M√©tricas normalizadas
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ generate_rankings.py     # Script de reprodu√ß√£o
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ modelo_X/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ relatorio_modelo_X.md
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ dados_modelo_X.csv
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ evidently_reports/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ evidently_qualidade.html
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ evidently_texto.html
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ metricas_consolidadas.json
‚îÇ
‚îî‚îÄ‚îÄ üìÅ docs/                            # Documenta√ß√£o
    ‚îî‚îÄ‚îÄ üìÑ DOCUMENTACAO_TCC_ANALISE_LLM.md
```

## ‚öôÔ∏è Configura√ß√µes Necess√°rias

### 1. Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# APIs dos Modelos
GROQ_API_KEY=sua_chave_groq_aqui
OPENAI_API_KEY=sua_chave_openai_aqui
GOOGLE_API_KEY=sua_chave_google_aqui

# Configura√ß√µes de Execu√ß√£o
NUMERO_EXECUCOES=3
TIMEOUT_ENTRE_EXECUCOES=30
TIMEOUT_ENTRE_PERGUNTAS=3
```

### 2. Instala√ß√£o de Depend√™ncias

```bash
pip install -r requirements.txt
```

### 3. Configura√ß√£o de Modelos

Edite `src/config.py` para ajustar:
- Lista de modelos a testar
- Par√¢metros de gera√ß√£o (max_tokens, temperature)
- Timeouts e limites de taxa
- Inclus√£o de benchmarks (INCLUDE_BENCHMARKS)

## üèÜ Benchmarks Padronizados

### **MMLU (Massive Multitask Language Understanding)**
- **Descri√ß√£o**: Avalia√ß√£o de conhecimento em m√∫ltiplas disciplinas
- **M√©tricas**: Accuracy por subject e geral
- **Implementa√ß√£o**: `analysis/mmlu.py`
- **Configura√ß√£o**: Ativado via `INCLUDE_BENCHMARKS=True`

### **HellaSwag (Commonsense Reasoning)**
- **Descri√ß√£o**: Racioc√≠nio de senso comum e completamento de cen√°rios
- **M√©tricas**: Accuracy geral
- **Implementa√ß√£o**: `analysis/hellaswag.py`
- **Configura√ß√£o**: Ativado via `INCLUDE_BENCHMARKS=True`

### **Configura√ß√£o de Benchmarks**
```python
# src/config.py
INCLUDE_BENCHMARKS = True  # Incluir benchmarks na execu√ß√£o
BENCHMARKS_FOLDER = "prompts"
BENCHMARKS_FILE = "benchmarks.json"
```

## üìÑ Descri√ß√£o dos Arquivos

### Arquivos Principais

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `main.py` | **Ponto de entrada** - Executa m√∫ltiplas execu√ß√µes da pipeline e inicia an√°lise consolidada |
| `requirements.txt` | **Depend√™ncias** - Lista de pacotes Python necess√°rios |
| `.env.example` | **Template** - Exemplo de configura√ß√£o de vari√°veis de ambiente |

### C√≥digo Fonte (`src/`)

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `config.py` | **Configura√ß√µes** - Centraliza todas as configura√ß√µes do sistema (modelos, timeouts, paths) |
| `pipeline.py` | **Pipeline Principal** - Executa prompts contra m√∫ltiplos LLMs e coleta respostas |
| `models.py` | **Modelos** - Implementa√ß√£o dos runners para diferentes APIs (Groq, OpenAI, Google) |
| `utils.py` | **Utilit√°rios** - Fun√ß√µes para salvamento de dados, gerenciamento de pastas e formata√ß√£o |
| `logger.py` | **Logging** - Sistema de logs estruturado para monitoramento e debug |

### Sistema de An√°lise (`analysis/`)

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `analysis.py` | **Orquestrador** - Classe principal que coordena toda a an√°lise consolidada |
| `ranking_system.py` | **Sistema de Ranking** - Gera rankings comparativos com normaliza√ß√£o e an√°lise qualitativa |
| `bleu_rouge.py` | **M√©tricas BLEU/ROUGE** - Calcula m√©tricas de qualidade de tradu√ß√£o/summariza√ß√£o |
| `bertscore.py` | **M√©tricas BERTScore** - Calcula similaridade sem√¢ntica usando embeddings BERT |
| `evidently_reports.py` | **Relat√≥rios Evidently** - Gera an√°lises de qualidade de dados em HTML |

## ü§ñ Modelos Utilizados

### Modelos Locais (Groq)
- **llama3_8b** - `llama-3.1-8b-instant`
- **llama3_70b** - `llama-3.3-70b-versatile`
- **qwen_32b** - `qwen/qwen3-32b`
- **deepseek_70b** - `deepseek-r1-distill-llama-70b`

### Modelos Open Source (Groq)
- **gpt_oss_20b** - `openai/gpt-oss-20b`
- **gpt_oss_120b** - `openai/gpt-oss-120b`

### Modelos Google (Gemini)
- **gemini_1_5_flash** - `models/gemini-1.5-flash` ‚ö†Ô∏è *Alta taxa de erro (49.5%)*
- **gemini_2_5_flash_lite** - `models/gemini-2.5-flash-lite`

## üìä M√©tricas Implementadas

### M√©tricas Acad√™micas
- **BLEU Score** - Precis√£o de n-gramas para avalia√ß√£o de qualidade
- **ROUGE-1** - Sobreposi√ß√£o de unigramas
- **ROUGE-2** - Sobreposi√ß√£o de bigramas (corrigido)
- **ROUGE-L** - Sobreposi√ß√£o de subsequ√™ncia mais longa
- **BERTScore** - Similaridade sem√¢ntica usando embeddings BERT

### Benchmarks Acad√™micos
- **MMLU** - Massive Multitask Language Understanding
- **HellaSwag** - Commonsense Reasoning

### M√©tricas de Qualidade (Evidently AI)
- **Distribui√ß√£o de Comprimento** - An√°lise do tamanho das respostas
- **Contagem de Palavras** - Estat√≠sticas de vocabul√°rio
- **Qualidade do Texto** - Detec√ß√£o de problemas de formata√ß√£o
- **Drift de Dados** - Detec√ß√£o de mudan√ßas na distribui√ß√£o
- **M√©tricas Estat√≠sticas** - M√©dia, mediana, desvio padr√£o, outliers

### Sistema de Ranking
- **Normaliza√ß√£o Min-Max** - Escala 0-1 para compara√ß√£o justa
- **Rankings Individuais** - Por cada m√©trica espec√≠fica
- **Rankings Consolidados** - Por categoria (Acad√™mico, Evidently AI, Geral)
- **An√°lise Qualitativa** - Insights e correla√ß√µes entre m√©tricas
- **Filtro Autom√°tico** - Exclus√£o de modelos com alta taxa de erro
- **Insights Executivos** - Gera√ß√£o autom√°tica de recomenda√ß√µes

## üîç Sistema de An√°lise

### 1. Coleta de Dados
- Executa 20 prompts padronizados e estruturados contra todos os modelos
- Coleta respostas com metadados (timestamp, comprimento, flags de erro)
- Salva resultados em formato CSV e JSON
- Detecta automaticamente modelos problem√°ticos

### 2. Processamento
- Consolida dados de m√∫ltiplas execu√ß√µes por modelo
- Filtra respostas v√°lidas vs. com erro
- Calcula m√©tricas acad√™micas e de qualidade
- Executa benchmarks MMLU e HellaSwag
- Analisa correla√ß√µes entre m√©tricas

### 3. Sistema de Ranking
- **Normaliza√ß√£o** de m√©tricas para escala 0-1
- **Rankings individuais** por cada m√©trica
- **Rankings consolidados** por categoria
- **An√°lise qualitativa** com correla√ß√µes e insights

### 4. Gera√ß√£o de Relat√≥rios
- **Relat√≥rios por Modelo** - An√°lise individual detalhada
- **Relat√≥rios Evidently AI** - An√°lises de qualidade em HTML
- **Relat√≥rio Consolidado** - Compara√ß√£o final com ranking
- **Rankings Comparativos** - Tabelas e an√°lises normalizadas
- **An√°lise de Correla√ß√µes** - Identifica√ß√£o de consist√™ncia entre m√©tricas
- **Insights Executivos** - Recomenda√ß√µes autom√°ticas

## üöÄ Como Usar

### Execu√ß√£o Completa
```bash
python main.py
```

### Execu√ß√£o com Benchmarks
```bash
# Ativar benchmarks em src/config.py
INCLUDE_BENCHMARKS = True
python main.py
```

### Execu√ß√£o da An√°lise Apenas
```bash
python -m analysis.analysis
```

### Execu√ß√£o do Sistema de Ranking
```bash
python -m analysis.ranking_system
```

### Configura√ß√£o Personalizada
1. Edite `src/config.py` para ajustar modelos e par√¢metros
2. Configure vari√°veis de ambiente no `.env`
3. Execute `python main.py`

## üìà Exemplo de Sa√≠da

### Relat√≥rio Consolidado
```markdown
# üìä Relat√≥rio Consolidado de An√°lise de Modelos LLM

**Data da An√°lise**: 06/01/2025 12:09:40
**Total de Respostas**: 480
**Modelos Testados**: 8
**Execu√ß√µes**: resultado_1, resultado_2, resultado_3

## üèÜ Ranking dos Modelos

1. **qwen_32b** - Score: 8.45
2. **deepseek_70b** - Score: 8.32
3. **llama3_70b** - Score: 8.18
...

## üìä M√©tricas por Modelo

### qwen_32b
- **BLEU Score**: 0.234
- **ROUGE-1**: 0.456
- **BERTScore**: 0.789
- **Taxa de Sucesso**: 95.8%
```

### Rankings Comparativos
```markdown
# üèÜ Rankings Comparativos de Modelos LLM

## Rankings por M√©trica Individual

### BLEU
| Modelo | Score Normalizado | Rank |
|--------|------------------|------|
| qwen_32b | 0.8500 | 1 |
| deepseek_70b | 0.8200 | 2 |
...

## Rankings Consolidados por Categoria

### Score Acad√™mico
| Modelo | Score | Rank |
|--------|-------|------|
| qwen_32b | 0.8450 | 1 |
| deepseek_70b | 0.8200 | 2 |
...
```

## üõ†Ô∏è Depend√™ncias

### Principais
- `pandas>=1.5.0` - Manipula√ß√£o de dados
- `numpy>=1.21.0` - Computa√ß√£o num√©rica
- `transformers>=4.30.0` - Modelos BERT para BERTScore
- `evaluate>=0.4.0` - M√©tricas de avalia√ß√£o
- `evidently[llm]>=0.7.14` - An√°lise de qualidade de dados

### APIs
- `groq>=0.9.0` - API Groq para modelos locais
- `openai>=1.0.0` - API OpenAI
- `google-generativeai>=0.8.0` - API Google Gemini

### Utilit√°rios
- `beautifulsoup4>=4.12.0` - Parsing HTML
- `python-dotenv>=1.0.0` - Gerenciamento de vari√°veis de ambiente
- `requests>=2.28.0` - Requisi√ß√µes HTTP
- `scikit-learn>=1.3.0` - Normaliza√ß√£o e an√°lise estat√≠stica

## üîß Configura√ß√µes Avan√ßadas

### Ajuste de Modelos
```python
# src/config.py
MODELOS_GROQ = {
    "llama3_8b": "llama-3.1-8b-instant",
    "llama3_70b": "llama-3.3-70b-versatile",
    # Adicione novos modelos aqui
}
```

### Ajuste de Prompts
```python
# src/config.py
PROMPTS = [
    "O que √© intelig√™ncia artificial e como ela est√° transformando o mundo?",
    "Explique a diferen√ßa entre machine learning e deep learning.",
    # Adicione novos prompts aqui
]
```

### Ajuste de M√©tricas
```python
# analysis/analysis.py
def _calcular_ranking_modelos(self, metricas_por_modelo):
    # Ajuste os pesos das m√©tricas aqui
    peso_bleu = 0.15
    peso_rouge = 0.20
    peso_bertscore = 0.25
    peso_confiabilidade = 0.10
```

## üÜï Funcionalidades Avan√ßadas

### Sistema de Ranking Comparativo
- **Normaliza√ß√£o autom√°tica** de m√©tricas para compara√ß√£o justa
- **Rankings individuais** por cada m√©trica espec√≠fica
- **Rankings consolidados** por categoria (Acad√™mico, Evidently AI, Geral)
- **An√°lise qualitativa** com correla√ß√µes e insights
- **Filtro autom√°tico** de modelos problem√°ticos

### An√°lise Qualitativa
- **Modelo mais consistente** (menor varia√ß√£o)
- **Modelo com maior fidelidade** (melhor BERTScore)
- **Modelo mais confi√°vel** (maior taxa de sucesso)
- **An√°lise de correla√ß√µes** entre m√©tricas acad√™micas e Evidently AI
- **Compara√ß√£o Open Source vs Propriet√°rios**
- **Insights executivos** autom√°ticos

### Relat√≥rios Avan√ßados
- **Rankings em Markdown** com tabelas formatadas e emojis
- **M√©tricas normalizadas em JSON** para reprodutibilidade
- **Scripts de gera√ß√£o** para reproduzir an√°lises
- **An√°lise de outliers** e distribui√ß√µes estat√≠sticas
- **Formata√ß√£o visual** melhorada com descri√ß√µes e contexto

### Benchmarks Acad√™micos
- **MMLU** - Avalia√ß√£o de conhecimento geral
- **HellaSwag** - Avalia√ß√£o de racioc√≠nio de senso comum
- **Extra√ß√£o autom√°tica** de respostas A, B, C, D
- **Valida√ß√£o robusta** de respostas de m√∫ltipla escolha

## üêõ Troubleshooting

### Problemas Comuns

1. **Erro de API Key**
   - Verifique se as chaves est√£o corretas no `.env`
   - Confirme se as APIs est√£o ativas

2. **Erro de Depend√™ncias**
   - Execute `pip install -r requirements.txt`
   - Verifique a vers√£o do Python (3.8+)

3. **Erro de Mem√≥ria**
   - Reduza `NUMERO_EXECUCOES` no config
   - Use modelos menores primeiro

4. **Rate Limits**
   - Aumente `TIMEOUT_ENTRE_PERGUNTAS`
   - Execute menos modelos por vez

5. **Erro no BERTScore**
   - Instale: `pip install bert-score`
   - Verifique se h√° GPU dispon√≠vel para acelerar

6. **Erro no Evidently AI**
   - Instale: `pip install evidently[llm]`
   - Verifique se h√° dados suficientes para an√°lise

7. **Problemas com ROUGE-2**
   - Sistema corrigido automaticamente
   - Verifique logs para debugging

8. **Modelos com alta taxa de erro**
   - Sistema detecta automaticamente
   - Modelos problem√°ticos s√£o exclu√≠dos da an√°lise principal

## üìù Logs e Debug

O sistema gera logs detalhados em:
- Console durante execu√ß√£o
- Arquivos de log em `logs/` (se configurado)
- Relat√≥rios de erro em `results/resultado_X/`
- An√°lises consolidadas em `analysis/analise_consolidada_*/`

## üéì Uso Acad√™mico

### Para Trabalhos de TCC/Mestrado
- **Dados normalizados** em `normalized_metrics.json`
- **Rankings reproduz√≠veis** com scripts Python
- **M√©tricas acad√™micas** padronizadas (BLEU, ROUGE, BERTScore)
- **Benchmarks acad√™micos** (MMLU, HellaSwag)
- **An√°lise estat√≠stica** com Evidently AI
- **An√°lise de correla√ß√µes** entre m√©tricas
- **Relat√≥rios detalhados** para documenta√ß√£o
- **Documenta√ß√£o TCC** completa em `docs/`

### Reproduzibilidade
- Scripts de gera√ß√£o autom√°tica de rankings
- Configura√ß√µes centralizadas e versionadas
- Logs detalhados de execu√ß√£o
- M√©tricas normalizadas para compara√ß√£o justa

## ü§ù Contribui√ß√£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para detalhes.

## üìû Suporte

Para d√∫vidas ou problemas:
1. Verifique a se√ß√£o Troubleshooting
2. Consulte os logs de erro
3. Abra uma issue no reposit√≥rio

---

**Desenvolvido com ‚ù§Ô∏è para compara√ß√£o e an√°lise de modelos LLM**

*Sistema completo de avalia√ß√£o acad√™mica com m√©tricas padronizadas, an√°lise de qualidade de dados e ranking comparativo para pesquisa em modelos de linguagem.*