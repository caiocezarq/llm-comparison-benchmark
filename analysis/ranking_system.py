#!/usr/bin/env python3
"""
Sistema de Ranking Comparativo de Modelos LLM
Extrai mÃ©tricas dos relatÃ³rios .md e gera rankings consolidados.
"""

import pandas as pd
import numpy as np
import os
import re
import json
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import sys

# Adicionar o diretÃ³rio pai ao path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config import get_config

class RankingSystem:
    """Sistema de ranking comparativo de modelos LLM."""
    
    def __init__(self):
        self.config = get_config()
        self.pasta_analysis = "analysis"
        
        # MÃ©tricas acadÃªmicas
        self.academic_metrics = [
            "BLEU",
            "ROUGE-1", 
            "ROUGE-2",
            "ROUGE-L",
            "BERTScore"
        ]
        
        # MÃ©tricas Evidently AI
        self.evidently_metrics = [
            "Respostas VÃ¡lidas",
            "Taxa de Validade", 
            "Comprimento MÃ©dio",
            "Palavras MÃ©dias",
            "ConsistÃªncia de Comprimento"
        ]
        
        # MÃ©tricas de Benchmarks
        self.benchmark_metrics = [
            "MMLU Accuracy",
            "HellaSwag Accuracy"
        ]
    
    def extrair_metricas_de_relatorios(self, pasta_analise: str) -> Dict[str, Dict]:
        """
        Extrai mÃ©tricas dos relatÃ³rios .md de cada modelo.
        
        Args:
            pasta_analise: Caminho da pasta de anÃ¡lise consolidada
            
        Returns:
            DicionÃ¡rio com mÃ©tricas por modelo
        """
        print("ğŸ“Š Extraindo mÃ©tricas dos relatÃ³rios .md...")
        
        metricas_por_modelo = {}
        
        # Encontrar pastas de modelos
        for item in os.listdir(pasta_analise):
            if item.startswith("modelo_"):
                modelo = item.replace("modelo_", "")
                pasta_modelo = os.path.join(pasta_analise, item)
                
                # Procurar relatÃ³rio do modelo
                arquivo_relatorio = os.path.join(pasta_modelo, f"relatorio_{modelo}.md")
                
                if os.path.exists(arquivo_relatorio):
                    print(f"ğŸ” Processando {modelo}...")
                    metricas = self._extrair_metricas_do_relatorio(arquivo_relatorio)
                    if metricas:
                        metricas_por_modelo[modelo] = metricas
                        print(f"âœ… {modelo}: {len(metricas)} mÃ©tricas extraÃ­das")
                    else:
                        print(f"âš ï¸ {modelo}: Nenhuma mÃ©trica extraÃ­da")
                else:
                    print(f"âŒ {modelo}: RelatÃ³rio nÃ£o encontrado")
        
        return metricas_por_modelo
    
    def extrair_metricas_benchmarks(self, pasta_analise: str) -> Dict[str, Dict]:
        """
        Extrai mÃ©tricas de benchmarks dos arquivos JSON.
        
        Args:
            pasta_analise: Caminho da pasta de anÃ¡lise
            
        Returns:
            DicionÃ¡rio com mÃ©tricas de benchmarks por modelo
        """
        metricas_benchmarks = {}
        
        # Procurar arquivo de mÃ©tricas consolidadas
        arquivo_metricas = os.path.join(pasta_analise, "metricas_consolidadas.json")
        
        if not os.path.exists(arquivo_metricas):
            print(f"âš ï¸ Arquivo {arquivo_metricas} nÃ£o encontrado")
            return {}
        
        try:
            with open(arquivo_metricas, 'r', encoding='utf-8') as f:
                dados = json.load(f)
            
            # Extrair mÃ©tricas de benchmarks
            for modelo, metricas in dados.items():
                if 'benchmarks' in metricas:
                    metricas_benchmarks[modelo] = {}
                    
                    # MMLU
                    if 'mmlu' in metricas['benchmarks']:
                        mmlu_data = metricas['benchmarks']['mmlu']
                        metricas_benchmarks[modelo]['MMLU Accuracy'] = mmlu_data.get('accuracy', 0.0)
                        metricas_benchmarks[modelo]['MMLU Total Questions'] = mmlu_data.get('total_questions', 0)
                        metricas_benchmarks[modelo]['MMLU Correct Answers'] = mmlu_data.get('correct_answers', 0)
                    
                    # HellaSwag
                    if 'hellaswag' in metricas['benchmarks']:
                        hellaswag_data = metricas['benchmarks']['hellaswag']
                        metricas_benchmarks[modelo]['HellaSwag Accuracy'] = hellaswag_data.get('accuracy', 0.0)
                        metricas_benchmarks[modelo]['HellaSwag Total Questions'] = hellaswag_data.get('total_questions', 0)
                        metricas_benchmarks[modelo]['HellaSwag Correct Answers'] = hellaswag_data.get('correct_answers', 0)
        
        except Exception as e:
            print(f"âŒ Erro ao extrair mÃ©tricas de benchmarks: {e}")
            return {}
        
        return metricas_benchmarks
    
    def _extrair_metricas_do_relatorio(self, arquivo_relatorio: str) -> Dict[str, float]:
        """
        Extrai mÃ©tricas especÃ­ficas de um relatÃ³rio .md.
        
        Args:
            arquivo_relatorio: Caminho do arquivo .md
            
        Returns:
            DicionÃ¡rio com mÃ©tricas extraÃ­das
        """
        try:
            with open(arquivo_relatorio, 'r', encoding='utf-8') as f:
                conteudo = f.read()
            
            metricas = {}
            
            # PadrÃµes para extrair mÃ©tricas acadÃªmicas
            padroes_academicas = {
                "BLEU": r"BLEU Score[:\s]*([0-9.]+)",
                "ROUGE-1": r"ROUGE-1[:\s]*([0-9.]+)",
                "ROUGE-2": r"ROUGE-2[:\s]*([0-9.]+)", 
                "ROUGE-L": r"ROUGE-L[:\s]*([0-9.]+)",
                "BERTScore": r"BERTScore[:\s]*([0-9.]+)"
            }
            
            # PadrÃµes para extrair mÃ©tricas Evidently AI
            padroes_evidently = {
                "Respostas VÃ¡lidas": r"Respostas VÃ¡lidas[:\s]*([0-9]+)",
                "Taxa de Validade": r"Taxa de Validade[:\s]*([0-9.]+)%?",
                "Comprimento MÃ©dio": r"Comprimento MÃ©dio[:\s]*([0-9.]+)",
                "Palavras MÃ©dias": r"Palavras MÃ©dias[:\s]*([0-9.]+)",
                "ConsistÃªncia de Comprimento": r"ConsistÃªncia de Comprimento[:\s]*([0-9.]+)%?"
            }
            
            # Extrair mÃ©tricas acadÃªmicas
            for metrica, padrao in padroes_academicas.items():
                match = re.search(padrao, conteudo, re.IGNORECASE)
                if match:
                    try:
                        valor = float(match.group(1))
                        metricas[metrica] = valor
                    except ValueError:
                        print(f"âš ï¸ Erro ao converter {metrica}: {match.group(1)}")
            
            # Extrair mÃ©tricas Evidently AI
            for metrica, padrao in padroes_evidently.items():
                match = re.search(padrao, conteudo, re.IGNORECASE)
                if match:
                    try:
                        valor = float(match.group(1))
                        # Converter percentual para decimal se necessÃ¡rio
                        if metrica == "Taxa de Validade" and valor > 1:
                            valor = valor / 100
                        metricas[metrica] = valor
                    except ValueError:
                        print(f"âš ï¸ Erro ao converter {metrica}: {match.group(1)}")
            
            # Calcular consistÃªncia de comprimento se nÃ£o encontrada diretamente
            if "ConsistÃªncia de Comprimento" not in metricas:
                # Tentar calcular a partir do CV se disponÃ­vel
                cv_match = re.search(r"CV[:\s]*([0-9.]+)%", conteudo, re.IGNORECASE)
                if cv_match:
                    try:
                        cv = float(cv_match.group(1))
                        # Inverter CV para ranking (menor CV = maior consistÃªncia)
                        metricas["ConsistÃªncia de Comprimento"] = max(0, 100 - cv)
                    except ValueError:
                        pass
            
            return metricas
            
        except Exception as e:
            print(f"âŒ Erro ao extrair mÃ©tricas de {arquivo_relatorio}: {e}")
            return {}
    
    def normalizar_metricas(self, metricas_por_modelo: Dict[str, Dict]) -> pd.DataFrame:
        """
        Normaliza mÃ©tricas para escala 0-1 (quanto maior melhor).
        
        Args:
            metricas_por_modelo: DicionÃ¡rio com mÃ©tricas por modelo
            
        Returns:
            DataFrame com mÃ©tricas normalizadas
        """
        print("ğŸ”„ Normalizando mÃ©tricas...")
        
        # Converter para DataFrame
        df = pd.DataFrame.from_dict(metricas_por_modelo, orient='index')
        
        # Preencher valores ausentes com 0
        df = df.fillna(0)
        
        # Normalizar mÃ©tricas (quanto maior melhor)
        for coluna in df.columns:
            if coluna in self.academic_metrics + self.evidently_metrics + self.benchmark_metrics:
                # Filtrar valores vÃ¡lidos (nÃ£o nulos e nÃ£o infinitos)
                valores_validos = df[coluna].replace([np.inf, -np.inf], np.nan).dropna()
                
                if len(valores_validos) == 0:
                    # Se nÃ£o hÃ¡ valores vÃ¡lidos, usar 0
                    df[f"Normalized {coluna}"] = 0.0
                    continue
                
                max_val = valores_validos.max()
                min_val = valores_validos.min()
                
                if max_val > min_val:
                    # NormalizaÃ§Ã£o min-max
                    df[f"Normalized {coluna}"] = (df[coluna] - min_val) / (max_val - min_val)
                    # Garantir que valores invÃ¡lidos sejam 0
                    df[f"Normalized {coluna}"] = df[f"Normalized {coluna}"].fillna(0.0)
                    df[f"Normalized {coluna}"] = df[f"Normalized {coluna}"].replace([np.inf, -np.inf], 0.0)
                else:
                    # Se todos os valores sÃ£o iguais e nÃ£o zero, usar 1.0
                    # Se todos sÃ£o zero, usar 0.0
                    if max_val > 0:
                        df[f"Normalized {coluna}"] = 1.0
                    else:
                        df[f"Normalized {coluna}"] = 0.0
        
        return df
    
    def gerar_rankings_individuais(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """
        Gera rankings por cada mÃ©trica individual.
        
        Args:
            df: DataFrame com mÃ©tricas normalizadas
            
        Returns:
            DicionÃ¡rio com rankings por mÃ©trica
        """
        print("ğŸ† Gerando rankings individuais...")
        
        rankings = {}
        
        # Rankings por mÃ©tricas acadÃªmicas
        for metrica in self.academic_metrics:
            coluna_normalizada = f"Normalized {metrica}"
            if coluna_normalizada in df.columns:
                ranking = df.sort_values(by=coluna_normalizada, ascending=False)[
                    ["Modelo", coluna_normalizada]
                ].reset_index(drop=True)
                ranking["Rank"] = ranking.index + 1
                rankings[metrica] = ranking
        
        # Rankings por mÃ©tricas Evidently AI
        for metrica in self.evidently_metrics:
            coluna_normalizada = f"Normalized {metrica}"
            if coluna_normalizada in df.columns:
                ranking = df.sort_values(by=coluna_normalizada, ascending=False)[
                    ["Modelo", coluna_normalizada]
                ].reset_index(drop=True)
                ranking["Rank"] = ranking.index + 1
                rankings[metrica] = ranking
        
        return rankings
    
    def gerar_rankings_consolidados(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """
        Gera rankings consolidados por categoria.
        
        Args:
            df: DataFrame com mÃ©tricas normalizadas
            
        Returns:
            DicionÃ¡rio com rankings consolidados
        """
        print("ğŸ“Š Gerando rankings consolidados...")
        
        rankings = {}
        
        # Score AcadÃªmico
        colunas_academicas = [f"Normalized {metrica}" for metrica in self.academic_metrics 
                             if f"Normalized {metrica}" in df.columns]
        if colunas_academicas:
            df["Score AcadÃªmico"] = df[colunas_academicas].mean(axis=1)
            ranking_academico = df.sort_values(by="Score AcadÃªmico", ascending=False)[
                ["Modelo", "Score AcadÃªmico"]
            ].reset_index(drop=True)
            ranking_academico["Rank"] = ranking_academico.index + 1
            rankings["Score AcadÃªmico"] = ranking_academico
        
        # Score Evidently AI
        colunas_evidently = [f"Normalized {metrica}" for metrica in self.evidently_metrics 
                            if f"Normalized {metrica}" in df.columns]
        if colunas_evidently:
            df["Score Evidently AI"] = df[colunas_evidently].mean(axis=1)
            ranking_evidently = df.sort_values(by="Score Evidently AI", ascending=False)[
                ["Modelo", "Score Evidently AI"]
            ].reset_index(drop=True)
            ranking_evidently["Rank"] = ranking_evidently.index + 1
            rankings["Score Evidently AI"] = ranking_evidently
        
        # Score Geral
        todas_colunas = colunas_academicas + colunas_evidently
        if todas_colunas:
            df["Score Geral"] = df[todas_colunas].mean(axis=1)
            ranking_geral = df.sort_values(by="Score Geral", ascending=False)[
                ["Modelo", "Score Geral"]
            ].reset_index(drop=True)
            ranking_geral["Rank"] = ranking_geral.index + 1
            rankings["Score Geral"] = ranking_geral
        
        return rankings
    
    def gerar_analise_qualitativa(self, df: pd.DataFrame, rankings: Dict[str, pd.DataFrame]) -> str:
        """
        Gera anÃ¡lise qualitativa dos resultados.
        
        Args:
            df: DataFrame com mÃ©tricas normalizadas
            rankings: DicionÃ¡rio com rankings
            
        Returns:
            String com anÃ¡lise qualitativa
        """
        print("ğŸ“ Gerando anÃ¡lise qualitativa...")
        
        analise = []
        analise.append("## ğŸ” AnÃ¡lise Qualitativa")
        analise.append("")
        
        # Modelo mais consistente (menor variaÃ§Ã£o)
        if "Normalized ConsistÃªncia de Comprimento" in df.columns:
            mais_consistente = df.loc[df["Normalized ConsistÃªncia de Comprimento"].idxmax(), "Modelo"]
            analise.append(f"### ğŸ¯ Modelo Mais Consistente: {mais_consistente}")
            analise.append("- Menor variaÃ§Ã£o no comprimento das respostas")
            analise.append("- Maior estabilidade de performance")
            analise.append("")
        
        # Modelo com maior fidelidade de texto (melhor BERTScore)
        if "Normalized BERTScore" in df.columns:
            melhor_bertscore = df.loc[df["Normalized BERTScore"].idxmax(), "Modelo"]
            analise.append(f"### ğŸ§  Modelo com Maior Fidelidade de Texto: {melhor_bertscore}")
            analise.append("- Melhor similaridade semÃ¢ntica com referÃªncias")
            analise.append("- Maior qualidade de conteÃºdo gerado")
            analise.append("")
        
        # Modelo com menor dispersÃ£o (melhor confiabilidade)
        if "Normalized Taxa de Validade" in df.columns:
            mais_confiavel = df.loc[df["Normalized Taxa de Validade"].idxmax(), "Modelo"]
            analise.append(f"### ğŸ›¡ï¸ Modelo Mais ConfiÃ¡vel: {mais_confiavel}")
            analise.append("- Maior taxa de respostas vÃ¡lidas")
            analise.append("- Menor incidÃªncia de erros")
            analise.append("")
        
        # Modelo mais detalhado (maior comprimento)
        if "Normalized Comprimento MÃ©dio" in df.columns:
            mais_detalhado = df.loc[df["Normalized Comprimento MÃ©dio"].idxmax(), "Modelo"]
            analise.append(f"### ğŸ“ Modelo Mais Detalhado: {mais_detalhado}")
            analise.append("- Respostas mais longas e detalhadas")
            analise.append("- Maior riqueza de informaÃ§Ã£o")
            analise.append("")
        
        # AnÃ¡lise de correlaÃ§Ãµes
        analise.append("### ğŸ“ˆ AnÃ¡lise de CorrelaÃ§Ãµes")
        analise.append("")
        
        # CorrelaÃ§Ã£o entre mÃ©tricas acadÃªmicas e Evidently AI
        colunas_academicas = [f"Normalized {metrica}" for metrica in self.academic_metrics 
                             if f"Normalized {metrica}" in df.columns]
        colunas_evidently = [f"Normalized {metrica}" for metrica in self.evidently_metrics 
                            if f"Normalized {metrica}" in df.columns]
        
        if colunas_academicas and colunas_evidently:
            score_academico = df[colunas_academicas].mean(axis=1)
            score_evidently = df[colunas_evidently].mean(axis=1)
            correlacao = np.corrcoef(score_academico, score_evidently)[0, 1]
            
            analise.append(f"- **CorrelaÃ§Ã£o AcadÃªmico vs Evidently AI**: {correlacao:.3f}")
            
            if correlacao > 0.7:
                analise.append("  - Forte correlaÃ§Ã£o positiva: modelos bons academicamente tambÃ©m sÃ£o bons em qualidade de dados")
            elif correlacao > 0.3:
                analise.append("  - CorrelaÃ§Ã£o moderada: alguma relaÃ§Ã£o entre mÃ©tricas acadÃªmicas e qualidade de dados")
            else:
                analise.append("  - CorrelaÃ§Ã£o fraca: mÃ©tricas acadÃªmicas e qualidade de dados sÃ£o independentes")
            analise.append("")
        
        # RecomendaÃ§Ãµes finais
        analise.append("### ğŸ’¡ RecomendaÃ§Ãµes")
        analise.append("")
        
        if "Score Geral" in df.columns:
            melhor_geral = df.loc[df["Score Geral"].idxmax(), "Modelo"]
            analise.append(f"**ğŸ† Modelo Recomendado**: {melhor_geral}")
            analise.append("- Melhor score geral considerando todas as mÃ©tricas")
            analise.append("- EquilÃ­brio entre qualidade acadÃªmica e confiabilidade")
            analise.append("")
        
        # AnÃ¡lise de modelos open source vs proprietÃ¡rios
        modelos_open_source = [m for m in df.index if any(oss in m.lower() for oss in ['llama', 'gpt_oss', 'qwen', 'deepseek'])]
        modelos_proprietarios = [m for m in df.index if any(prop in m.lower() for prop in ['gemini'])]
        
        if modelos_open_source and modelos_proprietarios:
            score_oss = df.loc[modelos_open_source, "Score Geral"].mean() if "Score Geral" in df.columns else 0
            score_prop = df.loc[modelos_proprietarios, "Score Geral"].mean() if "Score Geral" in df.columns else 0
            
            analise.append("### ğŸ”“ vs ğŸ”’ Open Source vs ProprietÃ¡rios")
            analise.append("")
            analise.append(f"- **Score MÃ©dio Open Source**: {score_oss:.3f}")
            analise.append(f"- **Score MÃ©dio ProprietÃ¡rios**: {score_prop:.3f}")
            
            if score_oss > score_prop:
                analise.append("- **ConclusÃ£o**: Modelos open source superam os proprietÃ¡rios em performance geral")
            elif score_prop > score_oss:
                analise.append("- **ConclusÃ£o**: Modelos proprietÃ¡rios superam os open source em performance geral")
            else:
                analise.append("- **ConclusÃ£o**: Performance similar entre modelos open source e proprietÃ¡rios")
            analise.append("")
        
        return "\n".join(analise)
    
    def salvar_rankings(self, rankings_individuais: Dict[str, pd.DataFrame], 
                       rankings_consolidados: Dict[str, pd.DataFrame],
                       df_normalizado: pd.DataFrame,
                       analise_qualitativa: str,
                       pasta_destino: str) -> str:
        """
        Salva todos os rankings em arquivos Markdown e JSON.
        
        Args:
            rankings_individuais: Rankings por mÃ©trica individual
            rankings_consolidados: Rankings consolidados
            df_normalizado: DataFrame com mÃ©tricas normalizadas
            analise_qualitativa: AnÃ¡lise qualitativa
            pasta_destino: Pasta de destino
            
        Returns:
            Caminho do arquivo principal de rankings
        """
        print("ğŸ’¾ Salvando rankings...")
        
        # Criar pasta de destino se nÃ£o existir
        os.makedirs(pasta_destino, exist_ok=True)
        
        # Arquivo principal de rankings
        arquivo_rankings = os.path.join(pasta_destino, "rankings.md")
        
        with open(arquivo_rankings, 'w', encoding='utf-8') as f:
            f.write("# ğŸ† Rankings Comparativos de Modelos LLM\n\n")
            f.write(f"**Data da AnÃ¡lise**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n\n")
            
            # Rankings por mÃ©trica individual
            f.write("## Rankings por MÃ©trica Individual\n\n")
            
            for metrica, ranking in rankings_individuais.items():
                f.write(f"### {metrica}\n")
                f.write(ranking.to_markdown(index=False))
                f.write("\n\n")
            
            # Rankings consolidados
            f.write("## Rankings Consolidados por Categoria\n\n")
            
            for categoria, ranking in rankings_consolidados.items():
                f.write(f"### {categoria}\n")
                f.write(ranking.to_markdown(index=False))
                f.write("\n\n")
            
            # AnÃ¡lise qualitativa
            f.write(analise_qualitativa)
        
        # Salvar mÃ©tricas normalizadas em JSON
        arquivo_json = os.path.join(pasta_destino, "normalized_metrics.json")
        df_normalizado.to_json(arquivo_json, orient='records', indent=2, force_ascii=False)
        
        # Salvar script de geraÃ§Ã£o de rankings
        script_rankings = os.path.join(pasta_destino, "generate_rankings.py")
        self._gerar_script_rankings(script_rankings)
        
        print(f"âœ… Rankings salvos em: {arquivo_rankings}")
        print(f"âœ… MÃ©tricas normalizadas em: {arquivo_json}")
        print(f"âœ… Script de geraÃ§Ã£o em: {script_rankings}")
        
        return arquivo_rankings
    
    def _gerar_script_rankings(self, caminho_script: str):
        """
        Gera script Python para reproduzir os rankings.
        
        Args:
            caminho_script: Caminho do arquivo de script
        """
        script_content = '''import json
import pandas as pd

with open("normalized_metrics.json", "r", encoding="utf-8") as f:
    data = json.load(f)

df = pd.DataFrame(data)

# Define metric categories
academic_metrics = [
    "Normalized BLEU",
    "Normalized ROUGE-1",
    "Normalized ROUGE-2", 
    "Normalized ROUGE-L",
    "Normalized BERTScore"
]
evidently_ai_metrics = [
    "Normalized Respostas VÃ¡lidas",
    "Normalized Taxa de Validade",
    "Normalized Comprimento MÃ©dio",
    "Normalized Palavras MÃ©dias",
    "Normalized ConsistÃªncia de Comprimento"
]

# --- Ranking por cada mÃ©trica individual ---
individual_rankings = {}
for col in academic_metrics + evidently_ai_metrics:
    if col in df.columns:
        individual_rankings[col] = df.sort_values(by=col, ascending=False)[["Modelo", col]].reset_index(drop=True)
        individual_rankings[col]["Rank"] = individual_rankings[col].index + 1

# --- Ranking consolidado por categoria ---
if academic_metrics:
    df["Score AcadÃªmico"] = df[academic_metrics].mean(axis=1)
    academic_ranking = df.sort_values(by="Score AcadÃªmico", ascending=False)[[
        "Modelo", "Score AcadÃªmico"]].reset_index(drop=True)
    academic_ranking["Rank"] = academic_ranking.index + 1

if evidently_ai_metrics:
    df["Score Evidently AI"] = df[evidently_ai_metrics].mean(axis=1)
    evidently_ai_ranking = df.sort_values(by="Score Evidently AI", ascending=False)[[
        "Modelo", "Score Evidently AI"]].reset_index(drop=True)
    evidently_ai_ranking["Rank"] = evidently_ai_ranking.index + 1

# --- Ranking geral ---
all_metrics = academic_metrics + evidently_ai_metrics
if all_metrics:
    df["Score Geral"] = df[all_metrics].mean(axis=1)
    general_ranking = df.sort_values(by="Score Geral", ascending=False)[[
        "Modelo", "Score Geral"]].reset_index(drop=True)
    general_ranking["Rank"] = general_ranking.index + 1

# --- Salvar resultados em arquivos Markdown ---
with open("rankings.md", "w", encoding="utf-8") as f:
    f.write("# ğŸ† Rankings Comparativos de Modelos LLM\\n\\n")

    f.write("## Rankings por MÃ©trica Individual\\n\\n")
    for metric, ranking_df in individual_rankings.items():
        f.write(f"### {metric.replace('Normalized ', '')}\\n")
        f.write(ranking_df.to_markdown(index=False))
        f.write("\\n\\n")

    f.write("## Rankings Consolidados por Categoria\\n\\n")
    if 'academic_ranking' in locals():
        f.write("### Score AcadÃªmico\\n")
        f.write(academic_ranking.to_markdown(index=False))
        f.write("\\n\\n")
    if 'evidently_ai_ranking' in locals():
        f.write("### Score Evidently AI\\n")
        f.write(evidently_ai_ranking.to_markdown(index=False))
        f.write("\\n\\n")

    f.write("## Ranking Geral\\n\\n")
    if 'general_ranking' in locals():
        f.write(general_ranking.to_markdown(index=False))
        f.write("\\n\\n")

print("Rankings gerados e salvos em rankings.md")
'''
        
        with open(caminho_script, 'w', encoding='utf-8') as f:
            f.write(script_content)
    
    def executar_analise_ranking(self, pasta_analise: str) -> str:
        """
        Executa anÃ¡lise completa de ranking.
        
        Args:
            pasta_analise: Pasta de anÃ¡lise consolidada
            
        Returns:
            Caminho do arquivo de rankings gerado
        """
        print("ğŸš€ Iniciando AnÃ¡lise de Ranking")
        print("=" * 60)
        
        # Extrair mÃ©tricas dos relatÃ³rios
        metricas_por_modelo = self.extrair_metricas_de_relatorios(pasta_analise)
        
        # Extrair mÃ©tricas de benchmarks
        metricas_benchmarks = self.extrair_metricas_benchmarks(pasta_analise)
        
        if not metricas_por_modelo:
            print("âŒ Nenhuma mÃ©trica extraÃ­da dos relatÃ³rios")
            return None
        
        # Combinar mÃ©tricas acadÃªmicas e benchmarks
        metricas_combinadas = {}
        for modelo in metricas_por_modelo:
            metricas_combinadas[modelo] = metricas_por_modelo[modelo].copy()
            if modelo in metricas_benchmarks:
                metricas_combinadas[modelo].update(metricas_benchmarks[modelo])
        
        # Normalizar mÃ©tricas
        df_normalizado = self.normalizar_metricas(metricas_combinadas)
        
        # Gerar rankings individuais
        rankings_individuais = self.gerar_rankings_individuais(df_normalizado)
        
        # Gerar rankings consolidados
        rankings_consolidados = self.gerar_rankings_consolidados(df_normalizado)
        
        # Gerar anÃ¡lise qualitativa
        analise_qualitativa = self.gerar_analise_qualitativa(df_normalizado, rankings_consolidados)
        
        # Salvar resultados
        pasta_rankings = os.path.join(pasta_analise, "rankings")
        arquivo_rankings = self.salvar_rankings(
            rankings_individuais, 
            rankings_consolidados,
            df_normalizado,
            analise_qualitativa,
            pasta_rankings
        )
        
        print(f"\nğŸ’¾ AnÃ¡lise de ranking salva em: {pasta_rankings}")
        print(f"ğŸ“„ Arquivo principal: {arquivo_rankings}")
        
        return arquivo_rankings

def executar_ranking(pasta_analise: str) -> str:
    """
    FunÃ§Ã£o principal para executar anÃ¡lise de ranking.
    
    Args:
        pasta_analise: Pasta de anÃ¡lise consolidada
        
    Returns:
        Caminho do arquivo de rankings gerado
    """
    ranking_system = RankingSystem()
    return ranking_system.executar_analise_ranking(pasta_analise)

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        pasta_analise = sys.argv[1]
    else:
        # Procurar pasta de anÃ¡lise mais recente
        pasta_analysis = "analysis"
        if os.path.exists(pasta_analysis):
            pastas = [d for d in os.listdir(pasta_analysis) if d.startswith("analise_consolidada_")]
            if pastas:
                pasta_analise = os.path.join(pasta_analysis, sorted(pastas)[-1])
            else:
                print("âŒ Nenhuma pasta de anÃ¡lise encontrada")
                sys.exit(1)
        else:
            print("âŒ Pasta analysis nÃ£o encontrada")
            sys.exit(1)
    
    executar_ranking(pasta_analise)
