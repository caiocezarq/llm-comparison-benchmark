#!/usr/bin/env python3
"""
Sistema de An√°lise Consolidada de Modelos LLM
Varre pasta results, agrupa por modelo e gera an√°lises completas.
"""

import pandas as pd
import numpy as np
import os
import sys
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import json

# Adicionar o diret√≥rio pai ao path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.config import get_config

class AnalysisSystem:
    """Sistema principal de an√°lise consolidada."""
    
    def __init__(self):
        self.config = get_config()
        self.pasta_analysis = "analysis"
        self.pasta_resultados = self.config.PASTA_RESULTADOS
        self.prefixo_execucao = self.config.PREFIXO_EXECUCAO
    
    def encontrar_execucoes(self) -> List[str]:
        """Encontra todas as execu√ß√µes dispon√≠veis na pasta de resultados."""
        if not os.path.exists(self.pasta_resultados):
            print(f"‚ùå Pasta {self.pasta_resultados} n√£o encontrada")
            return []
        
        execucoes = []
        for item in os.listdir(self.pasta_resultados):
            if item.startswith(self.prefixo_execucao):
                caminho_execucao = os.path.join(self.pasta_resultados, item)
                if os.path.isdir(caminho_execucao):
                    execucoes.append(caminho_execucao)
        
        execucoes.sort()
        return execucoes
    
    def carregar_dados_execucao(self, caminho_execucao: str) -> Optional[pd.DataFrame]:
        """Carrega dados de uma execu√ß√£o espec√≠fica."""
        arquivo_csv = os.path.join(caminho_execucao, "resultados_todos.csv")
        
        if not os.path.exists(arquivo_csv):
            print(f"‚ùå Arquivo {arquivo_csv} n√£o encontrado")
            return None
        
        try:
            df = pd.read_csv(arquivo_csv, encoding=self.config.ENCODING_CSV)
            print(f"‚úÖ Dados carregados: {len(df)} registros de {caminho_execucao}")
            return df
        except Exception as e:
            print(f"‚ùå Erro ao carregar {arquivo_csv}: {e}")
            return None

    def consolidar_dados_por_modelo(self, execucoes: List[str]) -> Dict[str, pd.DataFrame]:
        """Consolida dados agrupando por modelo."""
        print("üîÑ Consolidando dados por modelo...")
        
        dados_por_modelo = {}
        
        for execucao in execucoes:
            nome_execucao = os.path.basename(execucao)
            print(f"üìÅ Processando {nome_execucao}...")
            
            # Carregar dados da execu√ß√£o
            df = self.carregar_dados_execucao(execucao)
            if df is None:
                continue
            
            # Adicionar coluna de execu√ß√£o
            df['execucao'] = nome_execucao
            
            # Agrupar por modelo
            for modelo in df['model'].unique():
                df_modelo = df[df['model'] == modelo].copy()
                
                if modelo not in dados_por_modelo:
                    dados_por_modelo[modelo] = []
                
                dados_por_modelo[modelo].append(df_modelo)
        
        # Concatenar dados de cada modelo
        dados_consolidados = {}
        for modelo, lista_dfs in dados_por_modelo.items():
            if lista_dfs:
                dados_consolidados[modelo] = pd.concat(lista_dfs, ignore_index=True)
                print(f"‚úÖ {modelo}: {len(dados_consolidados[modelo])} registros consolidados")
        
        return dados_consolidados
    
    def calcular_metricas_academicas(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict, str]:
        """Calcula todas as m√©tricas acad√™micas (BLEU, ROUGE, BERTScore)."""
        print("üìä Calculando m√©tricas acad√™micas...")
        
        # BLEU e ROUGE
        try:
            # Tentar import relativo (quando chamado via main.py)
            from .bleu_rouge import calcular_bleu_rouge_completo
        except ImportError:
            # Fallback para import absoluto (quando executado diretamente)
            from bleu_rouge import calcular_bleu_rouge_completo
        
        try:
            df_bleu_rouge, metricas_bleu_rouge, relatorio_bleu_rouge = calcular_bleu_rouge_completo(df)
        except Exception as e:
            print(f"‚ùå Erro ao calcular BLEU/ROUGE: {e}")
            return df, {}, "Erro ao calcular BLEU/ROUGE"
        
        # BERTScore
        try:
            # Tentar import relativo (quando chamado via main.py)
            from .bertscore import calcular_bertscore_completo
        except ImportError:
            # Fallback para import absoluto (quando executado diretamente)
            from bertscore import calcular_bertscore_completo
        
        try:
            df_bertscore, metricas_bertscore, relatorio_bertscore = calcular_bertscore_completo(df_bleu_rouge)
        except Exception as e:
            print(f"‚ùå Erro ao calcular BERTScore: {e}")
            return df_bleu_rouge, metricas_bleu_rouge, relatorio_bleu_rouge
        
        # Calcular m√©tricas agregadas por modelo
        metricas_agregadas_dict = self._calcular_metricas_agregadas(df_bertscore)
        
        # Extrair m√©tricas do modelo atual (assumindo que h√° apenas um modelo no DataFrame)
        if metricas_agregadas_dict:
            modelo_atual = df_bertscore['model'].iloc[0]
            metricas_agregadas = metricas_agregadas_dict.get(modelo_atual, {})
        else:
            metricas_agregadas = {}
        
        # Combinar relat√≥rios
        relatorio_completo = f"{relatorio_bleu_rouge}\n\n{relatorio_bertscore}"
        
        return df_bertscore, metricas_agregadas, relatorio_completo
    
    def calcular_metricas_evidently(self, df: pd.DataFrame) -> Dict:
        """Calcula m√©tricas do Evidently AI para cada modelo."""
        print("üìà Calculando m√©tricas Evidently AI...")
        
        metricas_evidently = {}
        
        # Preparar dados
        df_evidently = df.copy()
        
        # Usar campos de comprimento se dispon√≠veis (nova pipeline), sen√£o calcular
        if 'response_length' in df_evidently.columns:
            df_evidently['text_length'] = df_evidently['response_length']
        else:
            df_evidently['text_length'] = df_evidently['prediction'].astype(str).str.len()
        
        if 'word_count' in df_evidently.columns:
            # Manter word_count se j√° existe
            pass
        else:
            df_evidently['word_count'] = df_evidently['prediction'].astype(str).str.split().str.len()
        
        # Usar campo is_error se dispon√≠vel, sen√£o calcular is_valid
        if self._usar_campo_is_error(df):
            df_evidently['is_valid'] = ~df_evidently['is_error']
        else:
            df_evidently['is_valid'] = ~df_evidently['prediction'].apply(self._eh_resposta_invalida)
        
        # Calcular m√©tricas
        total_respostas = len(df_evidently)
        respostas_validas = len(df_evidently[df_evidently['is_valid']])
        taxa_validas = respostas_validas / total_respostas if total_respostas > 0 else 0
        
        df_validas = df_evidently[df_evidently['is_valid']]
        
        metricas_evidently = {
            'total_respostas': total_respostas,
            'respostas_validas': respostas_validas,
            'taxa_validas': taxa_validas,
            'comprimento_medio': df_validas['text_length'].mean() if len(df_validas) > 0 else 0,
            'comprimento_std': df_validas['text_length'].std() if len(df_validas) > 0 else 0,
            'palavras_medias': df_validas['word_count'].mean() if len(df_validas) > 0 else 0,
            'palavras_std': df_validas['word_count'].std() if len(df_validas) > 0 else 0
        }
        
        return metricas_evidently
    
    def _calcular_metricas_agregadas(self, df: pd.DataFrame) -> Dict:
        """Calcula m√©tricas agregadas por modelo."""
        if 'model' not in df.columns:
            return {}

        metricas_agregadas = {}

        for modelo in df['model'].unique():
            df_modelo = df[df['model'] == modelo]
            
            # Filtrar apenas respostas v√°lidas - usar campo is_error se dispon√≠vel
            if self._usar_campo_is_error(df):
                df_validas = df_modelo[~df_modelo['is_error']]
            else:
                df_validas = df_modelo[~df_modelo['prediction'].apply(self._eh_resposta_invalida)]
            
            if len(df_validas) == 0:
                metricas_agregadas[modelo] = {
                    'bleu_medio': 0.0,
                    'rouge1_medio': 0.0,
                    'rouge2_medio': 0.0,
                    'rougeL_medio': 0.0,
                    'bertscore_f1_medio': 0.0,
                    'total_respostas': len(df_modelo),
                    'respostas_validas': 0,
                    'taxa_validas': 0.0
                }
                continue
            
            metricas_agregadas[modelo] = {
                'bleu_medio': df_validas['bleu_score'].mean(),
                'rouge1_medio': df_validas['rouge1_score'].mean(),
                'rouge2_medio': df_validas['rouge2_score'].mean(),
                'rougeL_medio': df_validas['rougeL_score'].mean(),
                'bertscore_f1_medio': df_validas['bertscore_f1'].mean(),
                'bleu_std': df_validas['bleu_score'].std(),
                'rouge1_std': df_validas['rouge1_score'].std(),
                'rouge2_std': df_validas['rouge2_score'].std(),
                'rougeL_std': df_validas['rougeL_score'].std(),
                'bertscore_f1_std': df_validas['bertscore_f1'].std(),
                'total_respostas': len(df_modelo),
                'respostas_validas': len(df_validas),
                'taxa_validas': len(df_validas) / len(df_modelo)
            }
            
        return metricas_agregadas
    
    def _analisar_html_evidently(self, pasta_evidently: str) -> str:
        """Analisa os relat√≥rios HTML do Evidently AI e extrai informa√ß√µes relevantes."""
        try:
            import re
            from bs4 import BeautifulSoup
            
            relatorio_html = ""
            
            # Analisar relat√≥rio de qualidade
            arquivo_qualidade = os.path.join(pasta_evidently, "evidently_qualidade.html")
            if os.path.exists(arquivo_qualidade):
                with open(arquivo_qualidade, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    relatorio_html += "### üìä An√°lise de Qualidade dos Dados\n"
                    relatorio_html += "**M√©tricas extra√≠das do relat√≥rio Evidently AI:**\n\n"
                    
                    # Extrair m√©tricas mais espec√≠ficas
                    metricas_qualidade = self._extrair_metricas_qualidade(soup)
                    if metricas_qualidade:
                        for metrica, valor in metricas_qualidade.items():
                            relatorio_html += f"- **{metrica}**: {valor}\n"
                    else:
                        relatorio_html += "- **Status**: Relat√≥rio de qualidade gerado com sucesso\n"
                        relatorio_html += "- **Conte√∫do**: An√°lise de distribui√ß√µes, valores ausentes e correla√ß√µes\n"
                    
                    relatorio_html += "\n"
            
            # Analisar relat√≥rio de texto
            arquivo_texto = os.path.join(pasta_evidently, "evidently_texto.html")
            if os.path.exists(arquivo_texto):
                with open(arquivo_texto, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    relatorio_html += "### üìù An√°lise de Texto\n"
                    relatorio_html += "**M√©tricas de an√°lise textual:**\n\n"
                    
                    # Extrair m√©tricas de texto mais espec√≠ficas
                    metricas_texto = self._extrair_metricas_texto(soup)
                    if metricas_texto:
                        for metrica, valor in metricas_texto.items():
                            relatorio_html += f"- **{metrica}**: {valor}\n"
                    else:
                        relatorio_html += "- **Status**: Relat√≥rio de texto gerado com sucesso\n"
                        relatorio_html += "- **Conte√∫do**: An√°lise de comprimento, qualidade e descritores textuais\n"
                    
                    relatorio_html += "\n"
            
            if not relatorio_html:
                relatorio_html = "### üìä An√°lise Evidently AI\n**Relat√≥rios HTML gerados com sucesso.**\n\n"
            
            return relatorio_html
            
        except Exception as e:
            return f"### üìä An√°lise Evidently AI\n**Erro ao analisar HTML**: {str(e)}\n\n"
    
    def _extrair_metricas_qualidade(self, soup) -> dict:
        """Extrai m√©tricas espec√≠ficas do relat√≥rio de qualidade."""
        import re
        metricas = {}
        
        try:
            # Procurar por tabelas com m√©tricas
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text().strip()
                        value = cells[1].get_text().strip()
                        
                        # Filtrar m√©tricas relevantes
                        if any(keyword in label.lower() for keyword in ['missing', 'null', 'count', 'mean', 'std', 'min', 'max']):
                            if re.match(r'^\d+\.?\d*$', value) or '%' in value:
                                metricas[label] = value
            
            # Procurar por elementos com classes espec√≠ficas
            for element in soup.find_all(['span', 'div'], class_=re.compile(r'value|metric|stat')):
                text = element.get_text().strip()
                if re.match(r'^\d+\.?\d*$', text) and len(text) > 0:
                    # Tentar encontrar o label associado
                    parent = element.parent
                    if parent:
                        label_elem = parent.find(['span', 'div', 'td'], class_=re.compile(r'label|name|title'))
                        if label_elem:
                            label = label_elem.get_text().strip()
                            metricas[label] = text
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair m√©tricas de qualidade: {e}")
        
        return metricas
    
    def _extrair_metricas_texto(self, soup) -> dict:
        """Extrai m√©tricas espec√≠ficas do relat√≥rio de texto."""
        import re
        metricas = {}
        
        try:
            # Procurar por m√©tricas de texto espec√≠ficas
            text_keywords = ['length', 'word', 'character', 'sentence', 'paragraph', 'quality', 'readability']
            
            for element in soup.find_all(['span', 'div', 'td']):
                text = element.get_text().strip()
                if any(keyword in text.lower() for keyword in text_keywords):
                    # Verificar se √© um valor num√©rico
                    if re.match(r'^\d+\.?\d*$', text) or '%' in text:
                        # Tentar encontrar o label
                        parent = element.parent
                        if parent:
                            label_elem = parent.find(['span', 'div', 'td'], class_=re.compile(r'label|name|title'))
                            if label_elem:
                                label = label_elem.get_text().strip()
                                metricas[label] = text
            
            # Procurar por tabelas com m√©tricas de texto
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text().strip()
                        value = cells[1].get_text().strip()
                        
                        if any(keyword in label.lower() for keyword in text_keywords):
                            if re.match(r'^\d+\.?\d*$', value) or '%' in value:
                                metricas[label] = value
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair m√©tricas de texto: {e}")
        
        return metricas
    
    def _gerar_metricas_detalhadas_evidently(self, df: pd.DataFrame) -> str:
        """Gera m√©tricas detalhadas do Evidently AI baseadas nos dados."""
        try:
            relatorio = []
            relatorio.append("### üìä M√©tricas Detalhadas Evidently AI")
            relatorio.append("**An√°lise baseada nos dados processados:**\n")

            # Preparar dados
            df_evidently = df.copy()
            
            # Usar campos de comprimento se dispon√≠veis (nova pipeline), sen√£o calcular
            if 'response_length' in df_evidently.columns:
                df_evidently['text_length'] = df_evidently['response_length']
            else:
                df_evidently['text_length'] = df_evidently['prediction'].astype(str).str.len()
            
            if 'word_count' in df_evidently.columns:
                # Manter word_count se j√° existe
                pass
            else:
                df_evidently['word_count'] = df_evidently['prediction'].astype(str).str.split().str.len()
            
            # Usar campo is_error se dispon√≠vel, sen√£o calcular is_valid
            if self._usar_campo_is_error(df):
                df_evidently['is_valid'] = ~df_evidently['is_error']
            else:
                df_evidently['is_valid'] = ~df_evidently['prediction'].apply(self._eh_resposta_invalida)
            
            # Filtrar apenas respostas v√°lidas
            df_validas = df_evidently[df_evidently['is_valid']]
            
            if len(df_validas) == 0:
                relatorio.append("- **Status**: Nenhuma resposta v√°lida encontrada")
                return "\n".join(relatorio)
            
            # M√©tricas de qualidade de dados
            relatorio.append("#### üîç Qualidade de Dados")
            relatorio.append(f"- **Total de Registros**: {len(df_evidently)}")
            relatorio.append(f"- **Respostas V√°lidas**: {len(df_validas)}")
            relatorio.append(f"- **Taxa de Validade**: {len(df_validas)/len(df_evidently):.1%}")
            relatorio.append(f"- **Respostas Inv√°lidas**: {len(df_evidently) - len(df_validas)}")
            relatorio.append("")
            
            # M√©tricas de texto
            relatorio.append("#### üìù An√°lise de Texto")
            relatorio.append(f"- **Comprimento M√©dio**: {df_validas['text_length'].mean():.1f} caracteres")
            relatorio.append(f"- **Comprimento M√≠nimo**: {df_validas['text_length'].min():.0f} caracteres")
            relatorio.append(f"- **Comprimento M√°ximo**: {df_validas['text_length'].max():.0f} caracteres")
            relatorio.append(f"- **Desvio Padr√£o**: {df_validas['text_length'].std():.1f} caracteres")
            relatorio.append("")
            
            relatorio.append(f"- **Palavras M√©dias**: {df_validas['word_count'].mean():.1f}")
            relatorio.append(f"- **Palavras M√≠nimas**: {df_validas['word_count'].min():.0f}")
            relatorio.append(f"- **Palavras M√°ximas**: {df_validas['word_count'].max():.0f}")
            relatorio.append(f"- **Desvio Padr√£o Palavras**: {df_validas['word_count'].std():.1f}")
            relatorio.append("")
            
            # An√°lise de distribui√ß√£o
            relatorio.append("#### üìä Distribui√ß√£o de Comprimento")
            q25 = df_validas['text_length'].quantile(0.25)
            q50 = df_validas['text_length'].quantile(0.50)
            q75 = df_validas['text_length'].quantile(0.75)
            
            relatorio.append(f"- **Q1 (25%)**: {q25:.1f} caracteres")
            relatorio.append(f"- **Mediana (50%)**: {q50:.1f} caracteres")
            relatorio.append(f"- **Q3 (75%)**: {q75:.1f} caracteres")
            relatorio.append(f"- **Amplitude Interquartil**: {q75 - q25:.1f} caracteres")
            relatorio.append("")
            
            # An√°lise de consist√™ncia
            cv_comprimento = (df_validas['text_length'].std() / df_validas['text_length'].mean()) * 100
            cv_palavras = (df_validas['word_count'].std() / df_validas['word_count'].mean()) * 100
            
            relatorio.append("#### ‚öñÔ∏è Consist√™ncia")
            relatorio.append(f"- **Coeficiente de Varia√ß√£o (Comprimento)**: {cv_comprimento:.1f}%")
            relatorio.append(f"- **Coeficiente de Varia√ß√£o (Palavras)**: {cv_palavras:.1f}%")
            
            if cv_comprimento < 20:
                consistencia = "‚úÖ Muito consistente"
            elif cv_comprimento < 40:
                consistencia = "‚ö†Ô∏è Moderadamente consistente"
            else:
                consistencia = "‚ùå Pouco consistente"
            
            relatorio.append(f"- **Avalia√ß√£o de Consist√™ncia**: {consistencia}")
            relatorio.append("")
            
            # An√°lise de outliers
            relatorio.append("#### üéØ An√°lise de Outliers")
            q1 = df_validas['text_length'].quantile(0.25)
            q3 = df_validas['text_length'].quantile(0.75)
            iqr = q3 - q1
            limite_inferior = q1 - 1.5 * iqr
            limite_superior = q3 + 1.5 * iqr
            
            outliers = df_validas[(df_validas['text_length'] < limite_inferior) | 
                                 (df_validas['text_length'] > limite_superior)]
            
            relatorio.append(f"- **Outliers Detectados**: {len(outliers)}")
            relatorio.append(f"- **Limite Inferior**: {limite_inferior:.1f} caracteres")
            relatorio.append(f"- **Limite Superior**: {limite_superior:.1f} caracteres")
            relatorio.append("")
            
            # Resumo estat√≠stico
            relatorio.append("#### üìà Resumo Estat√≠stico")
            relatorio.append(f"- **M√©dia Aritm√©tica**: {df_validas['text_length'].mean():.1f}")
            relatorio.append(f"- **Mediana**: {df_validas['text_length'].median():.1f}")
            relatorio.append(f"- **Moda**: {df_validas['text_length'].mode().iloc[0] if len(df_validas['text_length'].mode()) > 0 else 'N/A'}")
            relatorio.append(f"- **Vari√¢ncia**: {df_validas['text_length'].var():.1f}")
            relatorio.append(f"- **Assimetria**: {df_validas['text_length'].skew():.3f}")
            relatorio.append(f"- **Curtose**: {df_validas['text_length'].kurtosis():.3f}")
            
            return "\n".join(relatorio)
            
        except Exception as e:
            return f"### üìä M√©tricas Detalhadas Evidently AI\n**Erro ao gerar m√©tricas**: {str(e)}\n"
    
    def _eh_resposta_invalida(self, resposta: str) -> bool:
        """Verifica se uma resposta √© inv√°lida."""
        if not resposta or pd.isna(resposta):
            return True
        
        resposta_str = str(resposta).strip().lower()
        padroes_erro = [
            'erro', 'error', 'timeout', 'rate limit', 'api key',
            'authentication', 'connection', 'network', 'failed',
            'exception', 'traceback', 'null', 'none', 'undefined'
        ]
        
        return any(padrao in resposta_str for padrao in padroes_erro)
    
    def _usar_campo_is_error(self, df: pd.DataFrame) -> bool:
        """Verifica se o DataFrame tem o campo is_error (nova vers√£o da pipeline)."""
        return 'is_error' in df.columns
    
    def gerar_relatorio_por_modelo(self, modelo: str, df: pd.DataFrame, 
                                 metricas_academicas: Dict, metricas_evidently: Dict,
                                 pasta_destino: str) -> str:
        """Gera relat√≥rio individual por modelo."""
        relatorio = []
        relatorio.append(f"# ü§ñ An√°lise do Modelo: {modelo}")
        relatorio.append("")
        relatorio.append(f"**Data da An√°lise**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        relatorio.append(f"**Total de Respostas**: {len(df)}")
        relatorio.append(f"**Execu√ß√µes**: {', '.join(df['execucao'].unique())}")
        relatorio.append("")
        
        # M√©tricas acad√™micas
        relatorio.append("## üìä M√©tricas Acad√™micas")
        relatorio.append("")
        relatorio.append(f"- **BLEU Score**: {metricas_academicas.get('bleu_medio', 0):.4f}")
        relatorio.append(f"- **ROUGE-1**: {metricas_academicas.get('rouge1_medio', 0):.4f}")
        relatorio.append(f"- **ROUGE-2**: {metricas_academicas.get('rouge2_medio', 0):.4f}")
        relatorio.append(f"- **ROUGE-L**: {metricas_academicas.get('rougeL_medio', 0):.4f}")
        relatorio.append(f"- **BERTScore**: {metricas_academicas.get('bertscore_f1_medio', 0):.4f}")
        relatorio.append("")
        
        # M√©tricas Evidently AI
        relatorio.append("## üìà M√©tricas Evidently AI")
        relatorio.append("")
        relatorio.append(f"- **Respostas V√°lidas**: {metricas_evidently.get('respostas_validas', 0)}")
        relatorio.append(f"- **Taxa de Validade**: {metricas_evidently.get('taxa_validas', 0):.1%}")
        relatorio.append(f"- **Comprimento M√©dio**: {metricas_evidently.get('comprimento_medio', 0):.1f} ¬± {metricas_evidently.get('comprimento_std', 0):.1f} caracteres")
        relatorio.append(f"- **Palavras M√©dias**: {metricas_evidently.get('palavras_medias', 0):.1f} ¬± {metricas_evidently.get('palavras_std', 0):.1f}")
        relatorio.append("")
        
        # An√°lise de consist√™ncia
        comprimento_std = metricas_evidently.get('comprimento_std', 0)
        comprimento_medio = metricas_evidently.get('comprimento_medio', 0)
        
        if comprimento_medio > 0:
            cv_comprimento = (comprimento_std / comprimento_medio) * 100
            if cv_comprimento < 20:
                consistencia = "‚úÖ Muito consistente"
            elif cv_comprimento < 40:
                consistencia = "‚ö†Ô∏è Moderadamente consistente"
            else:
                consistencia = "‚ùå Pouco consistente"
            
            relatorio.append(f"- **Consist√™ncia de Comprimento**: {consistencia} (CV: {cv_comprimento:.1f}%)")
            relatorio.append("")
        
        # Avalia√ß√£o geral
        taxa_validas = metricas_evidently.get('taxa_validas', 0)
        if taxa_validas > 0.9:
            status_geral = "‚úÖ Excelente"
        elif taxa_validas > 0.7:
            status_geral = "‚ö†Ô∏è Bom"
        elif taxa_validas > 0.5:
            status_geral = "‚ö†Ô∏è Regular"
        else:
            status_geral = "‚ùå Ruim"
        
        relatorio.append(f"**Avalia√ß√£o Geral**: {status_geral}")
        relatorio.append("")
        
        return "\n".join(relatorio)
    
    def gerar_relatorio_consolidado(self, dados_por_modelo: Dict[str, pd.DataFrame],
                                  metricas_por_modelo: Dict[str, Dict],
                                  pasta_analise: str) -> str:
        """Gera relat√≥rio consolidado final."""
        print("üìù Gerando relat√≥rio consolidado...")
        
        relatorio = []
        relatorio.append("# üìä Relat√≥rio Consolidado de An√°lise de Modelos LLM")
        relatorio.append("")
        relatorio.append(f"**Data da An√°lise**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        
        # Estat√≠sticas gerais
        total_respostas = sum(len(df) for df in dados_por_modelo.values())
        total_modelos = len(dados_por_modelo)
        execucoes_unicas = set()
        for df in dados_por_modelo.values():
            execucoes_unicas.update(df['execucao'].unique())
        
        relatorio.append(f"**Total de Respostas**: {total_respostas}")
        relatorio.append(f"**Modelos Avaliados**: {total_modelos}")
        relatorio.append(f"**Execu√ß√µes Analisadas**: {len(execucoes_unicas)}")
        relatorio.append(f"**Execu√ß√µes**: {', '.join(sorted(execucoes_unicas))}")
        
        # Informa√ß√µes sobre metadados dispon√≠veis
        primeiro_df = next(iter(dados_por_modelo.values()))
        if self._usar_campo_is_error(primeiro_df):
            relatorio.append("**Metadados**: ‚úÖ Timestamp, comprimento de prompt/resposta, flags de erro")
        else:
            relatorio.append("**Metadados**: ‚ö†Ô∏è Vers√£o anterior da pipeline (sem metadados otimizados)")
        relatorio.append("")
        
        # Resumo executivo
        relatorio.append("## üìà Resumo Executivo")
        relatorio.append("")
        
        total_validas = sum(metricas_por_modelo[modelo]['evidently'].get('respostas_validas', 0) 
                           for modelo in dados_por_modelo.keys())
        taxa_geral = total_validas / total_respostas if total_respostas > 0 else 0
        
        relatorio.append(f"- **Respostas V√°lidas**: {total_validas}")
        relatorio.append(f"- **Taxa de Sucesso Geral**: {taxa_geral:.1%}")
        relatorio.append("")
        
        # Gerar rankings detalhados
        relatorio_rankings = self._gerar_rankings_detalhados(metricas_por_modelo)
        relatorio.append(relatorio_rankings)
        
        # Ranking dos modelos (vers√£o simplificada)
        relatorio.append("## üèÜ Ranking dos Modelos")
        relatorio.append("")
        
        modelos_ranking = self._calcular_ranking_modelos(metricas_por_modelo)
        
        for i, (modelo, score_composto) in enumerate(modelos_ranking, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∫"
            relatorio.append(f"### {emoji} {modelo} (Score: {score_composto:.4f})")
            relatorio.append("")
            
            # M√©tricas acad√™micas
            metricas_acad = metricas_por_modelo[modelo]['academicas']
            relatorio.append("**M√©tricas Acad√™micas:**")
            relatorio.append(f"- **BLEU Score**: {metricas_acad.get('bleu_medio', 0):.4f}")
            relatorio.append(f"- **ROUGE-1**: {metricas_acad.get('rouge1_medio', 0):.4f}")
            relatorio.append(f"- **ROUGE-2**: {metricas_acad.get('rouge2_medio', 0):.4f}")
            relatorio.append(f"- **ROUGE-L**: {metricas_acad.get('rougeL_medio', 0):.4f}")
            relatorio.append(f"- **BERTScore**: {metricas_acad.get('bertscore_f1_medio', 0):.4f}")
            relatorio.append("")
            
            # M√©tricas Evidently AI
            metricas_ev = metricas_por_modelo[modelo]['evidently']
            relatorio.append("**M√©tricas Evidently AI:**")
            relatorio.append(f"- **Respostas V√°lidas**: {metricas_ev.get('respostas_validas', 0)}")
            relatorio.append(f"- **Taxa de Validade**: {metricas_ev.get('taxa_validas', 0):.1%}")
            relatorio.append(f"- **Comprimento M√©dio**: {metricas_ev.get('comprimento_medio', 0):.1f} ¬± {metricas_ev.get('comprimento_std', 0):.1f} caracteres")
            relatorio.append(f"- **Palavras M√©dias**: {metricas_ev.get('palavras_medias', 0):.1f} ¬± {metricas_ev.get('palavras_std', 0):.1f}")
            relatorio.append("")
            relatorio.append("---")
            relatorio.append("")
        
        # An√°lise comparativa
        relatorio.append("## üìä An√°lise Comparativa")
        relatorio.append("")
        
        # Ranking por confiabilidade
        modelos_confiabilidade = [(modelo, metricas_por_modelo[modelo]['evidently'].get('taxa_validas', 0)) 
                                 for modelo in dados_por_modelo.keys()]
        modelos_confiabilidade.sort(key=lambda x: x[1], reverse=True)
        
        relatorio.append("**Ranking por Confiabilidade:**")
        for i, (modelo, taxa) in enumerate(modelos_confiabilidade, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∫"
            relatorio.append(f"{emoji} **{modelo}**: {taxa:.1%}")
            relatorio.append("")
            
        # Ranking por comprimento
        modelos_comprimento = [(modelo, metricas_por_modelo[modelo]['evidently'].get('comprimento_medio', 0)) 
                              for modelo in dados_por_modelo.keys()]
        modelos_comprimento.sort(key=lambda x: x[1], reverse=True)
        
        relatorio.append("**Ranking por Comprimento de Resposta:**")
        for i, (modelo, comprimento) in enumerate(modelos_comprimento, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∫"
            relatorio.append(f"{emoji} **{modelo}**: {comprimento:.1f} caracteres")
            relatorio.append("")
        
        # Recomenda√ß√µes
        relatorio.append("## üí° Recomenda√ß√µes")
        relatorio.append("")
        
        if modelos_ranking:
            melhor_modelo = modelos_ranking[0][0]
            relatorio.append(f"### üèÜ Modelo Recomendado: {melhor_modelo}")
            relatorio.append("")
            relatorio.append("**Justificativa:**")
            relatorio.append("- Melhor score composto considerando todas as m√©tricas")
            relatorio.append("- Equil√≠brio entre precis√£o acad√™mica e confiabilidade")
            relatorio.append("- Boa performance em m√©tricas de qualidade textual")
            relatorio.append("")
        
        if modelos_confiabilidade:
            mais_confiavel = modelos_confiabilidade[0][0]
            relatorio.append(f"### üõ°Ô∏è Modelo Mais Confi√°vel: {mais_confiavel}")
            relatorio.append(f"- Taxa de respostas v√°lidas: {modelos_confiabilidade[0][1]:.1%}")
            relatorio.append("")
        
        if modelos_comprimento:
            mais_detalhado = modelos_comprimento[0][0]
            relatorio.append(f"### üìù Modelo Mais Detalhado: {mais_detalhado}")
            relatorio.append(f"- Comprimento m√©dio: {modelos_comprimento[0][1]:.1f} caracteres")
        relatorio.append("")
        
        return "\n".join(relatorio)
    
    def _calcular_ranking_modelos(self, metricas_por_modelo: Dict[str, Dict]) -> List[tuple]:
        """Calcula ranking dos modelos baseado em score composto."""
        rankings = []
        
        for modelo, metricas in metricas_por_modelo.items():
            # M√©tricas acad√™micas
            metricas_acad = metricas['academicas']
            bleu = metricas_acad.get('bleu_medio', 0)
            rouge1 = metricas_acad.get('rouge1_medio', 0)
            rouge2 = metricas_acad.get('rouge2_medio', 0)
            rougeL = metricas_acad.get('rougeL_medio', 0)
            bertscore = metricas_acad.get('bertscore_f1_medio', 0)
            
            # M√©tricas Evidently AI
            metricas_ev = metricas['evidently']
            taxa_validas = metricas_ev.get('taxa_validas', 0)
            respostas_validas = metricas_ev.get('respostas_validas', 0)
            
            # Penalizar modelos com poucas respostas v√°lidas
            fator_confiabilidade = min(1.0, respostas_validas / 10.0)  # Penaliza se < 10 respostas v√°lidas
            
            # Score composto com penaliza√ß√£o por baixa confiabilidade
            score_composto = (
                bleu * 0.15 +
                rouge1 * 0.20 +
                rouge2 * 0.15 +
                rougeL * 0.15 +
                bertscore * 0.25 +
                taxa_validas * 0.10
            ) * fator_confiabilidade
            
            # Penaliza√ß√£o adicional para modelos com muito poucas respostas v√°lidas
            if respostas_validas < 5:
                score_composto *= 0.3  # Penaliza√ß√£o severa
            elif respostas_validas < 10:
                score_composto *= 0.6  # Penaliza√ß√£o moderada
            
            rankings.append((modelo, score_composto))
        
        return sorted(rankings, key=lambda x: x[1], reverse=True)
    
    def _gerar_rankings_detalhados(self, metricas_por_modelo: Dict[str, Dict]) -> str:
        """Gera rankings detalhados com m√©tricas normalizadas."""
        print("üèÜ Gerando rankings detalhados...")
        
        # Preparar dados para normaliza√ß√£o
        dados_metricas = []
        for modelo, metricas in metricas_por_modelo.items():
            dados_modelo = {"Modelo": modelo}
            
            # M√©tricas acad√™micas
            metricas_acad = metricas.get('academicas', {})
            dados_modelo.update({
                "BLEU": metricas_acad.get('bleu_medio', 0),
                "ROUGE-1": metricas_acad.get('rouge1_medio', 0),
                "ROUGE-2": metricas_acad.get('rouge2_medio', 0),
                "ROUGE-L": metricas_acad.get('rougeL_medio', 0),
                "BERTScore": metricas_acad.get('bertscore_f1_medio', 0)
            })
            
            # M√©tricas Evidently AI
            metricas_ev = metricas.get('evidently', {})
            dados_modelo.update({
                "Respostas V√°lidas": metricas_ev.get('respostas_validas', 0),
                "Taxa de Validade": metricas_ev.get('taxa_validas', 0),
                "Comprimento M√©dio": metricas_ev.get('comprimento_medio', 0),
                "Palavras M√©dias": metricas_ev.get('palavras_medias', 0),
                "Consist√™ncia de Comprimento": self._calcular_consistencia_comprimento(metricas_ev)
            })
            
            dados_metricas.append(dados_modelo)
        
        # Converter para DataFrame
        df = pd.DataFrame(dados_metricas)
        
        # Normalizar m√©tricas
        df_normalizado = self._normalizar_metricas(df)
        
        # Gerar rankings
        rankings_individuais = self._gerar_rankings_individuais(df_normalizado)
        rankings_consolidados = self._gerar_rankings_consolidados(df_normalizado)
        
        # Gerar relat√≥rio de rankings
        relatorio = []
        relatorio.append("## üèÜ Rankings Detalhados por M√©trica")
        relatorio.append("")
        
        # Rankings por m√©trica individual
        for metrica, ranking in rankings_individuais.items():
            relatorio.append(f"### {metrica}")
            relatorio.append("")
            relatorio.append("| Modelo | Score Normalizado | Rank |")
            relatorio.append("|--------|------------------|------|")
            
            for _, row in ranking.iterrows():
                relatorio.append(f"| {row['Modelo']} | {row[f'Normalized {metrica}']:.4f} | {row['Rank']} |")
            relatorio.append("")
        
        # Rankings consolidados
        relatorio.append("## üìä Rankings Consolidados por Categoria")
        relatorio.append("")
        
        for categoria, ranking in rankings_consolidados.items():
            relatorio.append(f"### {categoria}")
            relatorio.append("")
            relatorio.append("| Modelo | Score | Rank |")
            relatorio.append("|--------|-------|------|")
            
            for _, row in ranking.iterrows():
                relatorio.append(f"| {row['Modelo']} | {row[categoria]:.4f} | {row['Rank']} |")
            relatorio.append("")
        
        # An√°lise qualitativa
        analise_qualitativa = self._gerar_analise_qualitativa(df_normalizado, rankings_consolidados)
        relatorio.append(analise_qualitativa)
        
        return "\n".join(relatorio)
    
    def _calcular_consistencia_comprimento(self, metricas_ev: Dict) -> float:
        """Calcula consist√™ncia de comprimento baseada no coeficiente de varia√ß√£o."""
        comprimento_medio = metricas_ev.get('comprimento_medio', 0)
        comprimento_std = metricas_ev.get('comprimento_std', 0)
        
        if comprimento_medio > 0:
            cv = (comprimento_std / comprimento_medio) * 100
            # Inverter CV para ranking (menor CV = maior consist√™ncia)
            return max(0, 100 - cv)
        return 0
    
    def _normalizar_metricas(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normaliza m√©tricas para escala 0-1 (quanto maior melhor)."""
        df_normalizado = df.copy()
        
        # M√©tricas acad√™micas
        academic_metrics = ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "BERTScore"]
        # M√©tricas Evidently AI
        evidently_metrics = ["Respostas V√°lidas", "Taxa de Validade", "Comprimento M√©dio", 
                           "Palavras M√©dias", "Consist√™ncia de Comprimento"]
        
        for coluna in academic_metrics + evidently_metrics:
            if coluna in df.columns:
                max_val = df[coluna].max()
                min_val = df[coluna].min()
                
                if max_val > min_val:
                    # Normaliza√ß√£o min-max
                    df_normalizado[f"Normalized {coluna}"] = (df[coluna] - min_val) / (max_val - min_val)
                else:
                    # Se todos os valores s√£o iguais, usar 1.0
                    df_normalizado[f"Normalized {coluna}"] = 1.0
        
        return df_normalizado
    
    def _gerar_rankings_individuais(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """Gera rankings por cada m√©trica individual."""
        rankings = {}
        
        # M√©tricas acad√™micas
        academic_metrics = ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "BERTScore"]
        # M√©tricas Evidently AI
        evidently_metrics = ["Respostas V√°lidas", "Taxa de Validade", "Comprimento M√©dio", 
                           "Palavras M√©dias", "Consist√™ncia de Comprimento"]
        
        for metrica in academic_metrics + evidently_metrics:
            coluna_normalizada = f"Normalized {metrica}"
            if coluna_normalizada in df.columns:
                ranking = df.sort_values(by=coluna_normalizada, ascending=False)[
                    ["Modelo", coluna_normalizada]
                ].reset_index(drop=True)
                ranking["Rank"] = ranking.index + 1
                rankings[metrica] = ranking
        
        return rankings
    
    def _gerar_rankings_consolidados(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """Gera rankings consolidados por categoria."""
        rankings = {}
        
        # Score Acad√™mico
        academic_metrics = ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "BERTScore"]
        colunas_academicas = [f"Normalized {metrica}" for metrica in academic_metrics 
                             if f"Normalized {metrica}" in df.columns]
        if colunas_academicas:
            df["Score Acad√™mico"] = df[colunas_academicas].mean(axis=1)
            ranking_academico = df.sort_values(by="Score Acad√™mico", ascending=False)[
                ["Modelo", "Score Acad√™mico"]
            ].reset_index(drop=True)
            ranking_academico["Rank"] = ranking_academico.index + 1
            rankings["Score Acad√™mico"] = ranking_academico
        
        # Score Evidently AI
        evidently_metrics = ["Respostas V√°lidas", "Taxa de Validade", "Comprimento M√©dio", 
                           "Palavras M√©dias", "Consist√™ncia de Comprimento"]
        colunas_evidently = [f"Normalized {metrica}" for metrica in evidently_metrics 
                            if f"Normalized {metrica}" in df.columns]
        if colunas_evidently:
            df["Score Evidently AI"] = df[colunas_evidently].mean(axis=1)
            ranking_evidently = df.sort_values(by="Score Evidently AI", ascending=False)[
                ["Modelo", "Score Evidently AI"]
            ].reset_index(drop=True)
            ranking_evidently["Rank"] = ranking_evidently.index + 1
            rankings["Score Evidently AI"] = ranking_evidently
        
        # Score Geral
        todas_colunas = colunas_academicas + colunas_evidently
        if todas_colunas:
            df["Score Geral"] = df[todas_colunas].mean(axis=1)
            ranking_geral = df.sort_values(by="Score Geral", ascending=False)[
                ["Modelo", "Score Geral"]
            ].reset_index(drop=True)
            ranking_geral["Rank"] = ranking_geral.index + 1
            rankings["Score Geral"] = ranking_geral
        
        return rankings
    
    def _gerar_analise_qualitativa(self, df: pd.DataFrame, rankings: Dict[str, pd.DataFrame]) -> str:
        """Gera an√°lise qualitativa dos resultados."""
        analise = []
        analise.append("## üîç An√°lise Qualitativa")
        analise.append("")
        
        # Modelo mais consistente (menor varia√ß√£o)
        if "Normalized Consist√™ncia de Comprimento" in df.columns:
            mais_consistente = df.loc[df["Normalized Consist√™ncia de Comprimento"].idxmax(), "Modelo"]
            analise.append(f"### üéØ Modelo Mais Consistente: {mais_consistente}")
            analise.append("- Menor varia√ß√£o no comprimento das respostas")
            analise.append("- Maior estabilidade de performance")
            analise.append("")
        
        # Modelo com maior fidelidade de texto (melhor BERTScore)
        if "Normalized BERTScore" in df.columns:
            melhor_bertscore = df.loc[df["Normalized BERTScore"].idxmax(), "Modelo"]
            analise.append(f"### üß† Modelo com Maior Fidelidade de Texto: {melhor_bertscore}")
            analise.append("- Melhor similaridade sem√¢ntica com refer√™ncias")
            analise.append("- Maior qualidade de conte√∫do gerado")
            analise.append("")
        
        # Modelo com menor dispers√£o (melhor confiabilidade)
        if "Normalized Taxa de Validade" in df.columns:
            mais_confiavel = df.loc[df["Normalized Taxa de Validade"].idxmax(), "Modelo"]
            analise.append(f"### üõ°Ô∏è Modelo Mais Confi√°vel: {mais_confiavel}")
            analise.append("- Maior taxa de respostas v√°lidas")
            analise.append("- Menor incid√™ncia de erros")
            analise.append("")
        
        # Modelo mais detalhado (maior comprimento)
        if "Normalized Comprimento M√©dio" in df.columns:
            mais_detalhado = df.loc[df["Normalized Comprimento M√©dio"].idxmax(), "Modelo"]
            analise.append(f"### üìù Modelo Mais Detalhado: {mais_detalhado}")
            analise.append("- Respostas mais longas e detalhadas")
            analise.append("- Maior riqueza de informa√ß√£o")
            analise.append("")
        
        # An√°lise de correla√ß√µes
        analise.append("### üìà An√°lise de Correla√ß√µes")
        analise.append("")
        
        # Correla√ß√£o entre m√©tricas acad√™micas e Evidently AI
        colunas_academicas = [f"Normalized {metrica}" for metrica in ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "BERTScore"]
                             if f"Normalized {metrica}" in df.columns]
        colunas_evidently = [f"Normalized {metrica}" for metrica in ["Respostas V√°lidas", "Taxa de Validade", "Comprimento M√©dio", 
                           "Palavras M√©dias", "Consist√™ncia de Comprimento"]
                            if f"Normalized {metrica}" in df.columns]
        
        if colunas_academicas and colunas_evidently:
            score_academico = df[colunas_academicas].mean(axis=1)
            score_evidently = df[colunas_evidently].mean(axis=1)
            correlacao = np.corrcoef(score_academico, score_evidently)[0, 1]
            
            analise.append(f"- **Correla√ß√£o Acad√™mico vs Evidently AI**: {correlacao:.3f}")
            
            if correlacao > 0.7:
                analise.append("  - Forte correla√ß√£o positiva: modelos bons academicamente tamb√©m s√£o bons em qualidade de dados")
            elif correlacao > 0.3:
                analise.append("  - Correla√ß√£o moderada: alguma rela√ß√£o entre m√©tricas acad√™micas e qualidade de dados")
            else:
                analise.append("  - Correla√ß√£o fraca: m√©tricas acad√™micas e qualidade de dados s√£o independentes")
            analise.append("")
        
        # An√°lise de modelos open source vs propriet√°rios
        modelos_open_source = [m for m in df['Modelo'] if any(oss in m.lower() for oss in ['llama', 'gpt_oss', 'qwen', 'deepseek'])]
        modelos_proprietarios = [m for m in df['Modelo'] if any(prop in m.lower() for prop in ['gemini'])]
        
        if modelos_open_source and modelos_proprietarios and "Score Geral" in df.columns:
            score_oss = df[df['Modelo'].isin(modelos_open_source)]["Score Geral"].mean()
            score_prop = df[df['Modelo'].isin(modelos_proprietarios)]["Score Geral"].mean()
            
            analise.append("### üîì vs üîí Open Source vs Propriet√°rios")
            analise.append("")
            analise.append(f"- **Score M√©dio Open Source**: {score_oss:.3f}")
            analise.append(f"- **Score M√©dio Propriet√°rios**: {score_prop:.3f}")
            
            if score_oss > score_prop:
                analise.append("- **Conclus√£o**: Modelos open source superam os propriet√°rios em performance geral")
            elif score_prop > score_oss:
                analise.append("- **Conclus√£o**: Modelos propriet√°rios superam os open source em performance geral")
            else:
                analise.append("- **Conclus√£o**: Performance similar entre modelos open source e propriet√°rios")
            analise.append("")
        
        return "\n".join(analise)
    
    def _salvar_rankings_detalhados(self, metricas_por_modelo: Dict[str, Dict], pasta_analise: str):
        """Salva rankings detalhados em arquivos separados."""
        print("üíæ Salvando rankings detalhados...")
        
        # Preparar dados para normaliza√ß√£o
        dados_metricas = []
        for modelo, metricas in metricas_por_modelo.items():
            dados_modelo = {"Modelo": modelo}
            
            # M√©tricas acad√™micas
            metricas_acad = metricas.get('academicas', {})
            dados_modelo.update({
                "BLEU": metricas_acad.get('bleu_medio', 0),
                "ROUGE-1": metricas_acad.get('rouge1_medio', 0),
                "ROUGE-2": metricas_acad.get('rouge2_medio', 0),
                "ROUGE-L": metricas_acad.get('rougeL_medio', 0),
                "BERTScore": metricas_acad.get('bertscore_f1_medio', 0)
            })
            
            # M√©tricas Evidently AI
            metricas_ev = metricas.get('evidently', {})
            dados_modelo.update({
                "Respostas V√°lidas": metricas_ev.get('respostas_validas', 0),
                "Taxa de Validade": metricas_ev.get('taxa_validas', 0),
                "Comprimento M√©dio": metricas_ev.get('comprimento_medio', 0),
                "Palavras M√©dias": metricas_ev.get('palavras_medias', 0),
                "Consist√™ncia de Comprimento": self._calcular_consistencia_comprimento(metricas_ev)
            })
            
            dados_metricas.append(dados_modelo)
        
        # Converter para DataFrame
        df = pd.DataFrame(dados_metricas)
        
        # Normalizar m√©tricas
        df_normalizado = self._normalizar_metricas(df)
        
        # Gerar rankings
        rankings_individuais = self._gerar_rankings_individuais(df_normalizado)
        rankings_consolidados = self._gerar_rankings_consolidados(df_normalizado)
        
        # Salvar arquivo de rankings principal
        arquivo_rankings = os.path.join(pasta_analise, "rankings.md")
        with open(arquivo_rankings, 'w', encoding='utf-8') as f:
            f.write("# üèÜ Rankings Comparativos de Modelos LLM\n\n")
            f.write(f"**Data da An√°lise**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n\n")
            
            # Rankings por m√©trica individual
            f.write("## Rankings por M√©trica Individual\n\n")
            for metrica, ranking in rankings_individuais.items():
                f.write(f"### {metrica}\n")
                f.write(ranking.to_markdown(index=False))
                f.write("\n\n")
            
            # Rankings consolidados
            f.write("## Rankings Consolidados por Categoria\n\n")
            for categoria, ranking in rankings_consolidados.items():
                f.write(f"### {categoria}\n")
                f.write(ranking.to_markdown(index=False))
                f.write("\n\n")
        
        # Salvar m√©tricas normalizadas em JSON
        arquivo_json = os.path.join(pasta_analise, "normalized_metrics.json")
        df_normalizado.to_json(arquivo_json, orient='records', indent=2, force_ascii=False)
        
        # Salvar script de gera√ß√£o de rankings
        script_rankings = os.path.join(pasta_analise, "generate_rankings.py")
        self._gerar_script_rankings(script_rankings)
        
        print(f"‚úÖ Rankings salvos em: {arquivo_rankings}")
        print(f"‚úÖ M√©tricas normalizadas em: {arquivo_json}")
        print(f"‚úÖ Script de gera√ß√£o em: {script_rankings}")
    
    def _gerar_script_rankings(self, caminho_script: str):
        """Gera script Python para reproduzir os rankings."""
        script_content = '''import json
import pandas as pd

with open("normalized_metrics.json", "r", encoding="utf-8") as f:
    data = json.load(f)

df = pd.DataFrame(data)

# Define metric categories
academic_metrics = [
    "Normalized BLEU",
    "Normalized ROUGE-1",
    "Normalized ROUGE-2", 
    "Normalized ROUGE-L",
    "Normalized BERTScore"
]
evidently_ai_metrics = [
    "Normalized Respostas V√°lidas",
    "Normalized Taxa de Validade",
    "Normalized Comprimento M√©dio",
    "Normalized Palavras M√©dias",
    "Normalized Consist√™ncia de Comprimento"
]

# --- Ranking por cada m√©trica individual ---
individual_rankings = {}
for col in academic_metrics + evidently_ai_metrics:
    if col in df.columns:
        individual_rankings[col] = df.sort_values(by=col, ascending=False)[["Modelo", col]].reset_index(drop=True)
        individual_rankings[col]["Rank"] = individual_rankings[col].index + 1

# --- Ranking consolidado por categoria ---
if academic_metrics:
    df["Score Acad√™mico"] = df[academic_metrics].mean(axis=1)
    academic_ranking = df.sort_values(by="Score Acad√™mico", ascending=False)[[
        "Modelo", "Score Acad√™mico"]].reset_index(drop=True)
    academic_ranking["Rank"] = academic_ranking.index + 1

if evidently_ai_metrics:
    df["Score Evidently AI"] = df[evidently_ai_metrics].mean(axis=1)
    evidently_ai_ranking = df.sort_values(by="Score Evidently AI", ascending=False)[[
        "Modelo", "Score Evidently AI"]].reset_index(drop=True)
    evidently_ai_ranking["Rank"] = evidently_ai_ranking.index + 1

# --- Ranking geral ---
all_metrics = academic_metrics + evidently_ai_metrics
if all_metrics:
    df["Score Geral"] = df[all_metrics].mean(axis=1)
    general_ranking = df.sort_values(by="Score Geral", ascending=False)[[
        "Modelo", "Score Geral"]].reset_index(drop=True)
    general_ranking["Rank"] = general_ranking.index + 1

# --- Salvar resultados em arquivos Markdown ---
with open("rankings.md", "w", encoding="utf-8") as f:
    f.write("# üèÜ Rankings Comparativos de Modelos LLM\\n\\n")

    f.write("## Rankings por M√©trica Individual\\n\\n")
    for metric, ranking_df in individual_rankings.items():
        f.write(f"### {metric.replace('Normalized ', '')}\\n")
        f.write(ranking_df.to_markdown(index=False))
        f.write("\\n\\n")

    f.write("## Rankings Consolidados por Categoria\\n\\n")
    if 'academic_ranking' in locals():
        f.write("### Score Acad√™mico\\n")
        f.write(academic_ranking.to_markdown(index=False))
        f.write("\\n\\n")
    if 'evidently_ai_ranking' in locals():
        f.write("### Score Evidently AI\\n")
        f.write(evidently_ai_ranking.to_markdown(index=False))
        f.write("\\n\\n")

    f.write("## Ranking Geral\\n\\n")
    if 'general_ranking' in locals():
        f.write(general_ranking.to_markdown(index=False))
        f.write("\\n\\n")

print("Rankings gerados e salvos em rankings.md")
'''
        
        with open(caminho_script, 'w', encoding='utf-8') as f:
            f.write(script_content)
    
    def executar_analise_completa(self) -> str:
        """Executa an√°lise completa e retorna caminho do relat√≥rio."""
        print("üöÄ Iniciando An√°lise Consolidada")
        print("=" * 60)
        
        # Encontrar execu√ß√µes
        execucoes = self.encontrar_execucoes()
        print(f"üìÅ Encontradas {len(execucoes)} execu√ß√µes")
        
        if not execucoes:
            print("‚ùå Nenhuma execu√ß√£o encontrada para an√°lise")
            return None
        
        # Consolidar dados por modelo
        dados_por_modelo = self.consolidar_dados_por_modelo(execucoes)
        
        if not dados_por_modelo:
            print("‚ùå Nenhum modelo encontrado para an√°lise")
            return None
        
        # Criar pasta de an√°lise
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pasta_analise = os.path.join(self.pasta_analysis, f"analise_consolidada_{timestamp}")
        os.makedirs(pasta_analise, exist_ok=True)
        
        # Processar cada modelo
        metricas_por_modelo = {}
        
        for modelo, df in dados_por_modelo.items():
            print(f"\nü§ñ Processando modelo: {modelo}")
            
            # Criar pasta do modelo
            pasta_modelo = os.path.join(pasta_analise, f"modelo_{modelo}")
            os.makedirs(pasta_modelo, exist_ok=True)
            
            # Calcular m√©tricas acad√™micas
            df_com_metricas, metricas_academicas, relatorio_academicas = self.calcular_metricas_academicas(df)
            
            # Calcular m√©tricas Evidently AI
            metricas_evidently = self.calcular_metricas_evidently(df_com_metricas)
            
            # Gerar relat√≥rios Evidently AI
            try:
                # Tentar import relativo (quando chamado via main.py)
                from .evidently_reports import gerar_relatorios_evidently_completo
            except ImportError:
                # Fallback para import absoluto (quando executado diretamente)
                from evidently_reports import gerar_relatorios_evidently_completo
            
            try:
                pasta_evidently = os.path.join(pasta_modelo, "evidently_reports")
                relatorios_evidently, relatorio_evidently = gerar_relatorios_evidently_completo(df_com_metricas, pasta_evidently)
                print(f"üìä Relat√≥rios Evidently AI salvos em: {pasta_evidently}")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao gerar relat√≥rios Evidently AI: {e}")
                relatorios_evidently = {}
                relatorio_evidently = ""
            
            # Gerar relat√≥rio individual do modelo
            relatorio_modelo = self.gerar_relatorio_por_modelo(modelo, df_com_metricas, 
                                                              metricas_academicas, metricas_evidently, pasta_modelo)
            
            # Adicionar relat√≥rio Evidently AI
            if relatorio_evidently:
                relatorio_modelo += "\n\n" + relatorio_evidently
            
            # Adicionar an√°lise do HTML do Evidently AI
            pasta_evidently = os.path.join(pasta_modelo, "evidently_reports")
            if os.path.exists(pasta_evidently):
                relatorio_html = self._analisar_html_evidently(pasta_evidently)
                relatorio_modelo += "\n\n" + relatorio_html
            
            # Adicionar m√©tricas detalhadas do Evidently AI baseadas nos dados
            relatorio_detalhado = self._gerar_metricas_detalhadas_evidently(df_com_metricas)
            relatorio_modelo += "\n\n" + relatorio_detalhado
            
            # Salvar relat√≥rio do modelo
            arquivo_relatorio_modelo = os.path.join(pasta_modelo, f"relatorio_{modelo}.md")
            with open(arquivo_relatorio_modelo, 'w', encoding='utf-8') as f:
                f.write(relatorio_modelo)
            
            # Salvar dados do modelo
            df_com_metricas.to_csv(os.path.join(pasta_modelo, f"dados_{modelo}.csv"), 
                                  index=False, encoding=self.config.ENCODING_CSV)
            
            # Armazenar m√©tricas
            metricas_por_modelo[modelo] = {
                'academicas': metricas_academicas,
                'evidently': metricas_evidently
            }
            
            print(f"‚úÖ {modelo}: Relat√≥rio salvo em {arquivo_relatorio_modelo}")
        
        # Gerar relat√≥rio consolidado
        relatorio_consolidado = self.gerar_relatorio_consolidado(dados_por_modelo, metricas_por_modelo, pasta_analise)
        
        # Salvar relat√≥rio consolidado
        arquivo_relatorio_consolidado = os.path.join(pasta_analise, "relatorio_consolidado.md")
        with open(arquivo_relatorio_consolidado, 'w', encoding='utf-8') as f:
            f.write(relatorio_consolidado)
        
        # Salvar m√©tricas consolidadas
        with open(os.path.join(pasta_analise, "metricas_consolidadas.json"), 'w', encoding='utf-8') as f:
            json.dump(metricas_por_modelo, f, indent=2, ensure_ascii=False, default=str)
        
        # Gerar e salvar rankings detalhados
        self._salvar_rankings_detalhados(metricas_por_modelo, pasta_analise)
        
        print(f"\nüíæ An√°lise consolidada salva em: {pasta_analise}")
        print(f"üìÑ Relat√≥rio consolidado: {arquivo_relatorio_consolidado}")
        
        return arquivo_relatorio_consolidado

def executar_analise():
    """Fun√ß√£o principal para executar an√°lise consolidada."""
    analyzer = AnalysisSystem()
    return analyzer.executar_analise_completa()

if __name__ == "__main__":
    executar_analise()