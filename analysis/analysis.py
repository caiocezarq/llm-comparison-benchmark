#!/usr/bin/env python3
"""
Sistema de An√°lise Consolidada de Modelos LLM
Varre pasta results, agrupa por modelo e gera an√°lises completas.
"""

import pandas as pd
import numpy as np
import os
import sys
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import json

# Adicionar o diret√≥rio pai ao path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.config import get_config

# Imports para benchmarks - compatibilidade com chamada direta e via main
try:
    # Tentar import relativo (quando chamado via main.py)
    from .benchmarks import BaseBenchmark
    from .mmlu import MMLUBenchmark
    from .hellaswag import HellaSwagBenchmark
except ImportError:
    # Fallback para import absoluto (quando executado diretamente)
    from benchmarks import BaseBenchmark
    from mmlu import MMLUBenchmark
    from hellaswag import HellaSwagBenchmark

class AnalysisSystem:
    """Sistema principal de an√°lise consolidada."""
    
    def __init__(self):
        self.config = get_config()
        self.pasta_analysis = "analysis"
        self.pasta_resultados = self.config.PASTA_RESULTADOS
        self.prefixo_execucao = self.config.PREFIXO_EXECUCAO
        
        # Inicializar benchmarks
        try:
            self.benchmarks = {
                'mmlu': MMLUBenchmark(),
                'hellaswag': HellaSwagBenchmark()
            }
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao inicializar benchmarks: {e}")
            self.benchmarks = {}
    
    def encontrar_execucoes(self) -> List[str]:
        """Encontra todas as execu√ß√µes dispon√≠veis na pasta de resultados."""
        if not os.path.exists(self.pasta_resultados):
            print(f"‚ùå Pasta {self.pasta_resultados} n√£o encontrada")
            return []
        
        execucoes = []
        for item in os.listdir(self.pasta_resultados):
            if item.startswith(self.prefixo_execucao):
                caminho_execucao = os.path.join(self.pasta_resultados, item)
                if os.path.isdir(caminho_execucao):
                    execucoes.append(caminho_execucao)
        
        execucoes.sort()
        return execucoes
    
    def carregar_dados_execucao(self, caminho_execucao: str) -> Optional[pd.DataFrame]:
        """Carrega dados de uma execu√ß√£o espec√≠fica."""
        arquivo_csv = os.path.join(caminho_execucao, "resultados_todos.csv")
        
        if not os.path.exists(arquivo_csv):
            print(f"‚ùå Arquivo {arquivo_csv} n√£o encontrado")
            return None
        
        try:
            df = pd.read_csv(arquivo_csv, encoding=self.config.ENCODING_CSV)
            print(f"‚úÖ Dados carregados: {len(df)} registros de {caminho_execucao}")
            return df
        except Exception as e:
            print(f"‚ùå Erro ao carregar {arquivo_csv}: {e}")
            return None

    def consolidar_dados_por_modelo(self, execucoes: List[str]) -> Dict[str, pd.DataFrame]:
        """Consolida dados agrupando por modelo."""
        print("üîÑ Consolidando dados por modelo...")
        
        dados_por_modelo = {}
        
        for execucao in execucoes:
            nome_execucao = os.path.basename(execucao)
            print(f"üìÅ Processando {nome_execucao}...")
            
            # Carregar dados da execu√ß√£o
            df = self.carregar_dados_execucao(execucao)
            if df is None:
                continue
            
            # Adicionar coluna de execu√ß√£o
            df['execucao'] = nome_execucao
            
            # Agrupar por modelo
            for modelo in df['model'].unique():
                df_modelo = df[df['model'] == modelo].copy()
                
                if modelo not in dados_por_modelo:
                    dados_por_modelo[modelo] = []
                
                dados_por_modelo[modelo].append(df_modelo)
        
        # Concatenar dados de cada modelo
        dados_consolidados = {}
        for modelo, lista_dfs in dados_por_modelo.items():
            if lista_dfs:
                dados_consolidados[modelo] = pd.concat(lista_dfs, ignore_index=True)
                print(f"‚úÖ {modelo}: {len(dados_consolidados[modelo])} registros consolidados")
        
        return dados_consolidados
    
    def calcular_metricas_academicas(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict, str]:
        """Calcula todas as m√©tricas acad√™micas (BLEU, ROUGE, BERTScore)."""
        print("üìä Calculando m√©tricas acad√™micas...")
        
        # BLEU e ROUGE
        try:
            # Tentar import relativo (quando chamado via main.py)
            from .bleu_rouge import calcular_bleu_rouge_completo
        except ImportError:
            # Fallback para import absoluto (quando executado diretamente)
            from bleu_rouge import calcular_bleu_rouge_completo
        
        try:
            df_bleu_rouge, metricas_bleu_rouge, relatorio_bleu_rouge = calcular_bleu_rouge_completo(df)
        except Exception as e:
            print(f"‚ùå Erro ao calcular BLEU/ROUGE: {e}")
            return df, {}, "Erro ao calcular BLEU/ROUGE"
        
        # BERTScore
        try:
            # Tentar import relativo (quando chamado via main.py)
            from .bertscore import calcular_bertscore_completo
        except ImportError:
            # Fallback para import absoluto (quando executado diretamente)
            from bertscore import calcular_bertscore_completo
        
        try:
            df_bertscore, metricas_bertscore, relatorio_bertscore = calcular_bertscore_completo(df_bleu_rouge)
        except Exception as e:
            print(f"‚ùå Erro ao calcular BERTScore: {e}")
            return df_bleu_rouge, metricas_bleu_rouge, relatorio_bleu_rouge
        
        # Calcular m√©tricas agregadas por modelo
        metricas_agregadas_dict = self._calcular_metricas_agregadas(df_bertscore)
        
        # Extrair m√©tricas do modelo atual (assumindo que h√° apenas um modelo no DataFrame)
        if metricas_agregadas_dict:
            modelo_atual = df_bertscore['model'].iloc[0]
            metricas_agregadas = metricas_agregadas_dict.get(modelo_atual, {})
        else:
            metricas_agregadas = {}
        
        # Combinar relat√≥rios
        relatorio_completo = f"{relatorio_bleu_rouge}\n\n{relatorio_bertscore}"
        
        return df_bertscore, metricas_agregadas, relatorio_completo
    
    def calcular_metricas_evidently(self, df: pd.DataFrame) -> Dict:
        """Calcula m√©tricas do Evidently AI para cada modelo."""
        print("üìà Calculando m√©tricas Evidently AI...")
        
        metricas_evidently = {}
        
        # Preparar dados
        df_evidently = df.copy()
        
        # Usar campos de comprimento se dispon√≠veis (nova pipeline), sen√£o calcular
        if 'response_length' in df_evidently.columns:
            df_evidently['text_length'] = df_evidently['response_length']
        else:
            df_evidently['text_length'] = df_evidently['prediction'].astype(str).str.len()
        
        if 'word_count' in df_evidently.columns:
            # Manter word_count se j√° existe
            pass
        else:
            df_evidently['word_count'] = df_evidently['prediction'].astype(str).str.split().str.len()
        
        # Usar campo is_error se dispon√≠vel, sen√£o calcular is_valid
        if self._usar_campo_is_error(df):
            df_evidently['is_valid'] = ~df_evidently['is_error']
        else:
            df_evidently['is_valid'] = ~df_evidently['prediction'].apply(self._eh_resposta_invalida)
        
        # Calcular m√©tricas
        total_respostas = len(df_evidently)
        respostas_validas = len(df_evidently[df_evidently['is_valid']])
        taxa_validas = respostas_validas / total_respostas if total_respostas > 0 else 0
        
        df_validas = df_evidently[df_evidently['is_valid']]
        
        metricas_evidently = {
            'total_respostas': total_respostas,
            'respostas_validas': respostas_validas,
            'taxa_validas': taxa_validas,
            'comprimento_medio': df_validas['text_length'].mean() if len(df_validas) > 0 else 0,
            'comprimento_std': df_validas['text_length'].std() if len(df_validas) > 0 else 0,
            'palavras_medias': df_validas['word_count'].mean() if len(df_validas) > 0 else 0,
            'palavras_std': df_validas['word_count'].std() if len(df_validas) > 0 else 0
        }
        
        return metricas_evidently
    
    def _eh_modelo_problematico(self, df: pd.DataFrame, modelo: str) -> bool:
        """
        Verifica se um modelo tem alta taxa de erro e deve ser exclu√≠do da an√°lise principal.
        
        Args:
            df: DataFrame com dados
            modelo: Nome do modelo
            
        Returns:
            True se o modelo deve ser exclu√≠do
        """
        df_modelo = df[df['model'] == modelo]
        
        if len(df_modelo) == 0:
            return True
        
        # Verificar taxa de erro
        if 'is_error' in df.columns:
            taxa_erro = df_modelo['is_error'].mean()
        else:
            # Estimar taxa de erro baseado em respostas inv√°lidas
            respostas_invalidas = df_modelo['prediction'].apply(self._eh_resposta_invalida)
            taxa_erro = respostas_invalidas.mean()
        
        # Excluir modelos com taxa de erro > 40%
        return taxa_erro > 0.4
    
    def _calcular_metricas_agregadas(self, df: pd.DataFrame) -> Dict:
        """Calcula m√©tricas agregadas por modelo."""
        if 'model' not in df.columns:
            return {}

        metricas_agregadas = {}

        for modelo in df['model'].unique():
            # Filtrar modelos com alta taxa de erro (ex: Gemini 1.5 Flash)
            if self._eh_modelo_problematico(df, modelo):
                print(f"‚ö†Ô∏è Modelo {modelo} tem alta taxa de erro - excluindo da an√°lise principal")
                continue
            df_modelo = df[df['model'] == modelo]
            
            # Filtrar apenas respostas v√°lidas - usar campo is_error se dispon√≠vel
            if self._usar_campo_is_error(df):
                df_validas = df_modelo[~df_modelo['is_error']]
            else:
                df_validas = df_modelo[~df_modelo['prediction'].apply(self._eh_resposta_invalida)]
            
            if len(df_validas) == 0:
                metricas_agregadas[modelo] = {
                    'bleu_medio': 0.0,
                    'rouge1_medio': 0.0,
                    'rouge2_medio': 0.0,
                    'rougeL_medio': 0.0,
                    'bertscore_f1_medio': 0.0,
                    'total_respostas': len(df_modelo),
                    'respostas_validas': 0,
                    'taxa_validas': 0.0
                }
                continue
            
            metricas_agregadas[modelo] = {
                'bleu_medio': df_validas['bleu_score'].mean(),
                'rouge1_medio': df_validas['rouge1_score'].mean(),
                'rouge2_medio': df_validas['rouge2_score'].mean(),
                'rougeL_medio': df_validas['rougeL_score'].mean(),
                'bertscore_f1_medio': df_validas['bertscore_f1'].mean(),
                'bleu_std': df_validas['bleu_score'].std(),
                'rouge1_std': df_validas['rouge1_score'].std(),
                'rouge2_std': df_validas['rouge2_score'].std(),
                'rougeL_std': df_validas['rougeL_score'].std(),
                'bertscore_f1_std': df_validas['bertscore_f1'].std(),
                'total_respostas': len(df_modelo),
                'respostas_validas': len(df_validas),
                'taxa_validas': len(df_validas) / len(df_modelo)
            }
            
        return metricas_agregadas
    
    def _analisar_html_evidently(self, pasta_evidently: str) -> str:
        """Analisa os relat√≥rios HTML do Evidently AI e extrai informa√ß√µes relevantes."""
        try:
            import re
            from bs4 import BeautifulSoup
            
            relatorio_html = ""
            
            # Analisar relat√≥rio de qualidade
            arquivo_qualidade = os.path.join(pasta_evidently, "evidently_qualidade.html")
            if os.path.exists(arquivo_qualidade):
                with open(arquivo_qualidade, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    relatorio_html += "### üìä An√°lise de Qualidade dos Dados\n"
                    relatorio_html += "**M√©tricas extra√≠das do relat√≥rio Evidently AI:**\n\n"
                    
                    # Extrair m√©tricas mais espec√≠ficas
                    metricas_qualidade = self._extrair_metricas_qualidade(soup)
                    if metricas_qualidade:
                        for metrica, valor in metricas_qualidade.items():
                            relatorio_html += f"- **{metrica}**: {valor}\n"
                    else:
                        relatorio_html += "- **Status**: Relat√≥rio de qualidade gerado com sucesso\n"
                        relatorio_html += "- **Conte√∫do**: An√°lise de distribui√ß√µes, valores ausentes e correla√ß√µes\n"
                    
                    relatorio_html += "\n"
            
            # Analisar relat√≥rio de texto
            arquivo_texto = os.path.join(pasta_evidently, "evidently_texto.html")
            if os.path.exists(arquivo_texto):
                with open(arquivo_texto, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    relatorio_html += "### üìù An√°lise de Texto\n"
                    relatorio_html += "**M√©tricas de an√°lise textual:**\n\n"
                    
                    # Extrair m√©tricas de texto mais espec√≠ficas
                    metricas_texto = self._extrair_metricas_texto(soup)
                    if metricas_texto:
                        for metrica, valor in metricas_texto.items():
                            relatorio_html += f"- **{metrica}**: {valor}\n"
                    else:
                        relatorio_html += "- **Status**: Relat√≥rio de texto gerado com sucesso\n"
                        relatorio_html += "- **Conte√∫do**: An√°lise de comprimento, qualidade e descritores textuais\n"
                    
                    relatorio_html += "\n"
            
            if not relatorio_html:
                relatorio_html = "### üìä An√°lise Evidently AI\n**Relat√≥rios HTML gerados com sucesso.**\n\n"
            
            return relatorio_html
            
        except Exception as e:
            return f"### üìä An√°lise Evidently AI\n**Erro ao analisar HTML**: {str(e)}\n\n"
    
    def _extrair_metricas_qualidade(self, soup) -> dict:
        """Extrai m√©tricas espec√≠ficas do relat√≥rio de qualidade."""
        import re
        metricas = {}
        
        try:
            # Procurar por tabelas com m√©tricas
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text().strip()
                        value = cells[1].get_text().strip()
                        
                        # Filtrar m√©tricas relevantes
                        if any(keyword in label.lower() for keyword in ['missing', 'null', 'count', 'mean', 'std', 'min', 'max']):
                            if re.match(r'^\d+\.?\d*$', value) or '%' in value:
                                metricas[label] = value
            
            # Procurar por elementos com classes espec√≠ficas
            for element in soup.find_all(['span', 'div'], class_=re.compile(r'value|metric|stat')):
                text = element.get_text().strip()
                if re.match(r'^\d+\.?\d*$', text) and len(text) > 0:
                    # Tentar encontrar o label associado
                    parent = element.parent
                    if parent:
                        label_elem = parent.find(['span', 'div', 'td'], class_=re.compile(r'label|name|title'))
                        if label_elem:
                            label = label_elem.get_text().strip()
                            metricas[label] = text
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair m√©tricas de qualidade: {e}")
        
        return metricas
    
    def _extrair_metricas_texto(self, soup) -> dict:
        """Extrai m√©tricas espec√≠ficas do relat√≥rio de texto."""
        import re
        metricas = {}
        
        try:
            # Procurar por m√©tricas de texto espec√≠ficas
            text_keywords = ['length', 'word', 'character', 'sentence', 'paragraph', 'quality', 'readability']
            
            for element in soup.find_all(['span', 'div', 'td']):
                text = element.get_text().strip()
                if any(keyword in text.lower() for keyword in text_keywords):
                    # Verificar se √© um valor num√©rico
                    if re.match(r'^\d+\.?\d*$', text) or '%' in text:
                        # Tentar encontrar o label
                        parent = element.parent
                        if parent:
                            label_elem = parent.find(['span', 'div', 'td'], class_=re.compile(r'label|name|title'))
                            if label_elem:
                                label = label_elem.get_text().strip()
                                metricas[label] = text
            
            # Procurar por tabelas com m√©tricas de texto
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text().strip()
                        value = cells[1].get_text().strip()
                        
                        if any(keyword in label.lower() for keyword in text_keywords):
                            if re.match(r'^\d+\.?\d*$', value) or '%' in value:
                                metricas[label] = value
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair m√©tricas de texto: {e}")
        
        return metricas
    
    def _gerar_metricas_detalhadas_evidently(self, df: pd.DataFrame) -> str:
        """Gera m√©tricas detalhadas do Evidently AI baseadas nos dados."""
        try:
            relatorio = []
            relatorio.append("### üìä M√©tricas Detalhadas Evidently AI")
            relatorio.append("**An√°lise baseada nos dados processados:**\n")

            # Preparar dados
            df_evidently = df.copy()
            
            # Usar campos de comprimento se dispon√≠veis (nova pipeline), sen√£o calcular
            if 'response_length' in df_evidently.columns:
                df_evidently['text_length'] = df_evidently['response_length']
            else:
                df_evidently['text_length'] = df_evidently['prediction'].astype(str).str.len()
            
            if 'word_count' in df_evidently.columns:
                # Manter word_count se j√° existe
                pass
            else:
                df_evidently['word_count'] = df_evidently['prediction'].astype(str).str.split().str.len()
            
            # Usar campo is_error se dispon√≠vel, sen√£o calcular is_valid
            if self._usar_campo_is_error(df):
                df_evidently['is_valid'] = ~df_evidently['is_error']
            else:
                df_evidently['is_valid'] = ~df_evidently['prediction'].apply(self._eh_resposta_invalida)
            
            # Filtrar apenas respostas v√°lidas
            df_validas = df_evidently[df_evidently['is_valid']]
            
            if len(df_validas) == 0:
                relatorio.append("- **Status**: Nenhuma resposta v√°lida encontrada")
                return "\n".join(relatorio)
            
            # M√©tricas de qualidade de dados
            relatorio.append("#### üîç Qualidade de Dados")
            relatorio.append(f"- **Total de Registros**: {len(df_evidently)}")
            relatorio.append(f"- **Respostas V√°lidas**: {len(df_validas)}")
            relatorio.append(f"- **Taxa de Validade**: {len(df_validas)/len(df_evidently):.1%}")
            relatorio.append(f"- **Respostas Inv√°lidas**: {len(df_evidently) - len(df_validas)}")
            relatorio.append("")
            
            # M√©tricas de texto
            relatorio.append("#### üìù An√°lise de Texto")
            relatorio.append(f"- **Comprimento M√©dio**: {df_validas['text_length'].mean():.1f} caracteres")
            relatorio.append(f"- **Comprimento M√≠nimo**: {df_validas['text_length'].min():.0f} caracteres")
            relatorio.append(f"- **Comprimento M√°ximo**: {df_validas['text_length'].max():.0f} caracteres")
            relatorio.append(f"- **Desvio Padr√£o**: {df_validas['text_length'].std():.1f} caracteres")
            relatorio.append("")
            
            relatorio.append(f"- **Palavras M√©dias**: {df_validas['word_count'].mean():.1f}")
            relatorio.append(f"- **Palavras M√≠nimas**: {df_validas['word_count'].min():.0f}")
            relatorio.append(f"- **Palavras M√°ximas**: {df_validas['word_count'].max():.0f}")
            relatorio.append(f"- **Desvio Padr√£o Palavras**: {df_validas['word_count'].std():.1f}")
            relatorio.append("")
            
            # An√°lise de distribui√ß√£o
            relatorio.append("#### üìä Distribui√ß√£o de Comprimento")
            q25 = df_validas['text_length'].quantile(0.25)
            q50 = df_validas['text_length'].quantile(0.50)
            q75 = df_validas['text_length'].quantile(0.75)
            
            relatorio.append(f"- **Q1 (25%)**: {q25:.1f} caracteres")
            relatorio.append(f"- **Mediana (50%)**: {q50:.1f} caracteres")
            relatorio.append(f"- **Q3 (75%)**: {q75:.1f} caracteres")
            relatorio.append(f"- **Amplitude Interquartil**: {q75 - q25:.1f} caracteres")
            relatorio.append("")
            
            # An√°lise de consist√™ncia
            cv_comprimento = (df_validas['text_length'].std() / df_validas['text_length'].mean()) * 100
            cv_palavras = (df_validas['word_count'].std() / df_validas['word_count'].mean()) * 100
            
            relatorio.append("#### ‚öñÔ∏è Consist√™ncia")
            relatorio.append(f"- **Coeficiente de Varia√ß√£o (Comprimento)**: {cv_comprimento:.1f}%")
            relatorio.append(f"- **Coeficiente de Varia√ß√£o (Palavras)**: {cv_palavras:.1f}%")
            
            if cv_comprimento < 20:
                consistencia = "‚úÖ Muito consistente"
            elif cv_comprimento < 40:
                consistencia = "‚ö†Ô∏è Moderadamente consistente"
            else:
                consistencia = "‚ùå Pouco consistente"
            
            relatorio.append(f"- **Avalia√ß√£o de Consist√™ncia**: {consistencia}")
            relatorio.append("")
            
            # An√°lise de outliers
            relatorio.append("#### üéØ An√°lise de Outliers")
            q1 = df_validas['text_length'].quantile(0.25)
            q3 = df_validas['text_length'].quantile(0.75)
            iqr = q3 - q1
            limite_inferior = q1 - 1.5 * iqr
            limite_superior = q3 + 1.5 * iqr
            
            outliers = df_validas[(df_validas['text_length'] < limite_inferior) | 
                                 (df_validas['text_length'] > limite_superior)]
            
            relatorio.append(f"- **Outliers Detectados**: {len(outliers)}")
            relatorio.append(f"- **Limite Inferior**: {limite_inferior:.1f} caracteres")
            relatorio.append(f"- **Limite Superior**: {limite_superior:.1f} caracteres")
            relatorio.append("")
            
            # Resumo estat√≠stico
            relatorio.append("#### üìà Resumo Estat√≠stico")
            relatorio.append(f"- **M√©dia Aritm√©tica**: {df_validas['text_length'].mean():.1f}")
            relatorio.append(f"- **Mediana**: {df_validas['text_length'].median():.1f}")
            relatorio.append(f"- **Moda**: {df_validas['text_length'].mode().iloc[0] if len(df_validas['text_length'].mode()) > 0 else 'N/A'}")
            relatorio.append(f"- **Vari√¢ncia**: {df_validas['text_length'].var():.1f}")
            relatorio.append(f"- **Assimetria**: {df_validas['text_length'].skew():.3f}")
            relatorio.append(f"- **Curtose**: {df_validas['text_length'].kurtosis():.3f}")
            
            return "\n".join(relatorio)
            
        except Exception as e:
            return f"### üìä M√©tricas Detalhadas Evidently AI\n**Erro ao gerar m√©tricas**: {str(e)}\n"
    
    def _eh_resposta_invalida(self, resposta: str) -> bool:
        """Verifica se uma resposta √© inv√°lida."""
        if not resposta or pd.isna(resposta):
            return True
        
        resposta_str = str(resposta).strip().lower()
        padroes_erro = [
            'erro', 'error', 'timeout', 'rate limit', 'api key',
            'authentication', 'connection', 'network', 'failed',
            'exception', 'traceback', 'null', 'none', 'undefined'
        ]
        
        return any(padrao in resposta_str for padrao in padroes_erro)
    
    def calcular_metricas_benchmarks(self, df: pd.DataFrame) -> Dict:
        """
        Calcula m√©tricas de benchmarks (MMLU, HellaSwag) para cada modelo.
        
        Args:
            df: DataFrame com resultados dos modelos
            
        Returns:
            Dicion√°rio com m√©tricas de benchmarks por modelo
        """
        print("üèÜ Calculando m√©tricas de benchmarks...")
        
        metricas_benchmarks = {}
        
        # Verificar se h√° benchmarks dispon√≠veis
        if not self.benchmarks:
            print("‚ö†Ô∏è Nenhum benchmark dispon√≠vel")
            return {}
        
        # Verificar se h√° coluna benchmark no DataFrame
        if 'benchmark' not in df.columns:
            print("‚ö†Ô∏è Coluna 'benchmark' n√£o encontrada no DataFrame")
            return {}
        
        # Filtrar apenas linhas com benchmarks
        df_benchmarks = df[df['benchmark'].notna()].copy()
        
        if df_benchmarks.empty:
            print("‚ö†Ô∏è Nenhum dado de benchmark encontrado")
            return {}
        
        # Agrupar por modelo e benchmark
        for model in df_benchmarks['model'].unique():
            metricas_benchmarks[model] = {}
            
            for benchmark_name, benchmark_calc in self.benchmarks.items():
                try:
                    # Filtrar dados do benchmark espec√≠fico
                    df_benchmark = df_benchmarks[
                        (df_benchmarks['model'] == model) & 
                        (df_benchmarks['benchmark'] == benchmark_name)
                    ]
                    
                    if not df_benchmark.empty:
                        predictions = df_benchmark['prediction'].tolist()
                        references = df_benchmark['reference'].tolist()
                        
                        # Calcular m√©tricas do benchmark
                        metrics = benchmark_calc.calculate_metrics(predictions, references)
                        metricas_benchmarks[model][benchmark_name] = metrics
                    else:
                        metricas_benchmarks[model][benchmark_name] = {
                            "accuracy": 0.0,
                            "total_questions": 0,
                            "correct_answers": 0
                        }
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao calcular m√©tricas do benchmark {benchmark_name} para {model}: {e}")
                    metricas_benchmarks[model][benchmark_name] = {
                        "accuracy": 0.0,
                        "total_questions": 0,
                        "correct_answers": 0
                    }
        
        return metricas_benchmarks
    
    def _usar_campo_is_error(self, df: pd.DataFrame) -> bool:
        """Verifica se o DataFrame tem o campo is_error (nova vers√£o da pipeline)."""
        return 'is_error' in df.columns
    
    def gerar_relatorio_por_modelo(self, modelo: str, df: pd.DataFrame, 
                                 metricas_academicas: Dict, metricas_evidently: Dict,
                                 metricas_benchmarks: Dict, pasta_destino: str) -> str:
        """Gera relat√≥rio individual por modelo."""
        relatorio = []
        relatorio.append(f"# ü§ñ An√°lise do Modelo: {modelo}")
        relatorio.append("")
        relatorio.append(f"**Data da An√°lise**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        relatorio.append(f"**Total de Respostas**: {len(df)}")
        relatorio.append(f"**Execu√ß√µes**: {', '.join(df['execucao'].unique())}")
        relatorio.append("")
        
        # M√©tricas acad√™micas
        relatorio.append("## üìä M√©tricas Acad√™micas")
        relatorio.append("")
        relatorio.append(f"- **BLEU Score**: {metricas_academicas.get('bleu_medio', 0):.4f}")
        relatorio.append(f"- **ROUGE-1**: {metricas_academicas.get('rouge1_medio', 0):.4f}")
        relatorio.append(f"- **ROUGE-2**: {metricas_academicas.get('rouge2_medio', 0):.4f}")
        relatorio.append(f"- **ROUGE-L**: {metricas_academicas.get('rougeL_medio', 0):.4f}")
        relatorio.append(f"- **BERTScore**: {metricas_academicas.get('bertscore_f1_medio', 0):.4f}")
        relatorio.append("")
        
        # M√©tricas de Benchmarks
        if metricas_benchmarks and modelo in metricas_benchmarks:
            relatorio.append("## üèÜ M√©tricas de Benchmarks")
            relatorio.append("")
            
            # MMLU
            if 'mmlu' in metricas_benchmarks[modelo]:
                mmlu_data = metricas_benchmarks[modelo]['mmlu']
                relatorio.append("### MMLU (Massive Multitask Language Understanding)")
                relatorio.append(f"- **Accuracy**: {mmlu_data.get('accuracy', 0):.4f}")
                relatorio.append(f"- **Total de Quest√µes**: {mmlu_data.get('total_questions', 0)}")
                relatorio.append(f"- **Respostas Corretas**: {mmlu_data.get('correct_answers', 0)}")
                
                # Subjects espec√≠ficos se dispon√≠vel
                if 'subjects' in mmlu_data and mmlu_data['subjects']:
                    relatorio.append("- **Accuracy por Subject**:")
                    for subject, accuracy in mmlu_data['subjects'].items():
                        relatorio.append(f"  - {subject}: {accuracy:.4f}")
                relatorio.append("")
            
            # HellaSwag
            if 'hellaswag' in metricas_benchmarks[modelo]:
                hellaswag_data = metricas_benchmarks[modelo]['hellaswag']
                relatorio.append("### HellaSwag (Commonsense Reasoning)")
                relatorio.append(f"- **Accuracy**: {hellaswag_data.get('accuracy', 0):.4f}")
                relatorio.append(f"- **Total de Quest√µes**: {hellaswag_data.get('total_questions', 0)}")
                relatorio.append(f"- **Respostas Corretas**: {hellaswag_data.get('correct_answers', 0)}")
                relatorio.append("")
        
        # M√©tricas Evidently AI
        relatorio.append("## üìà M√©tricas Evidently AI")
        relatorio.append("")
        relatorio.append(f"- **Respostas V√°lidas**: {metricas_evidently.get('respostas_validas', 0)}")
        relatorio.append(f"- **Taxa de Validade**: {metricas_evidently.get('taxa_validas', 0):.1%}")
        relatorio.append(f"- **Comprimento M√©dio**: {metricas_evidently.get('comprimento_medio', 0):.1f} ¬± {metricas_evidently.get('comprimento_std', 0):.1f} caracteres")
        relatorio.append(f"- **Palavras M√©dias**: {metricas_evidently.get('palavras_medias', 0):.1f} ¬± {metricas_evidently.get('palavras_std', 0):.1f}")
        relatorio.append("")
        
        # An√°lise de consist√™ncia
        comprimento_std = metricas_evidently.get('comprimento_std', 0)
        comprimento_medio = metricas_evidently.get('comprimento_medio', 0)
        
        if comprimento_medio > 0:
            cv_comprimento = (comprimento_std / comprimento_medio) * 100
            if cv_comprimento < 20:
                consistencia = "‚úÖ Muito consistente"
            elif cv_comprimento < 40:
                consistencia = "‚ö†Ô∏è Moderadamente consistente"
            else:
                consistencia = "‚ùå Pouco consistente"
            
            relatorio.append(f"- **Consist√™ncia de Comprimento**: {consistencia} (CV: {cv_comprimento:.1f}%)")
            relatorio.append("")
        
        # Avalia√ß√£o geral
        taxa_validas = metricas_evidently.get('taxa_validas', 0)
        if taxa_validas > 0.9:
            status_geral = "‚úÖ Excelente"
        elif taxa_validas > 0.7:
            status_geral = "‚ö†Ô∏è Bom"
        elif taxa_validas > 0.5:
            status_geral = "‚ö†Ô∏è Regular"
        else:
            status_geral = "‚ùå Ruim"
        
        relatorio.append(f"**Avalia√ß√£o Geral**: {status_geral}")
        relatorio.append("")
        
        return "\n".join(relatorio)
    
    def gerar_relatorio_consolidado(self, dados_por_modelo: Dict[str, pd.DataFrame],
                                  metricas_por_modelo: Dict[str, Dict],
                                  pasta_analise: str) -> str:
        """Gera relat√≥rio consolidado final."""
        print("üìù Gerando relat√≥rio consolidado...")
        
        relatorio = []
        relatorio.append("# üìä Relat√≥rio Consolidado de An√°lise de Modelos LLM")
        relatorio.append("")
        relatorio.append(f"**Data da An√°lise**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        
        # Estat√≠sticas gerais
        total_respostas = sum(len(df) for df in dados_por_modelo.values())
        total_modelos = len(dados_por_modelo)
        execucoes_unicas = set()
        for df in dados_por_modelo.values():
            execucoes_unicas.update(df['execucao'].unique())
        
        # Calcular estat√≠sticas de qualidade
        respostas_validas = 0
        for df in dados_por_modelo.values():
            if 'is_error' in df.columns:
                respostas_validas += (~df['is_error']).sum()
            else:
                respostas_validas += len(df)
        
        taxa_sucesso = (respostas_validas / total_respostas) * 100 if total_respostas > 0 else 0
        
        relatorio.append("## üìä Informa√ß√µes da An√°lise")
        relatorio.append("")
        relatorio.append("| M√©trica | Valor |")
        relatorio.append("|:--------|------:|")
        relatorio.append(f"| **Total de Respostas** | {total_respostas:,} |")
        relatorio.append(f"| **Modelos Avaliados** | {total_modelos} |")
        relatorio.append(f"| **Execu√ß√µes Analisadas** | {len(execucoes_unicas)} |")
        relatorio.append(f"| **Respostas V√°lidas** | {respostas_validas:,} |")
        relatorio.append(f"| **Taxa de Sucesso** | {taxa_sucesso:.1f}% |")
        relatorio.append("")
        
        # Informa√ß√µes sobre metadados dispon√≠veis
        primeiro_df = next(iter(dados_por_modelo.values()))
        if self._usar_campo_is_error(primeiro_df):
            relatorio.append("**Metadados**: ‚úÖ Timestamp, comprimento de prompt/resposta, flags de erro")
        else:
            relatorio.append("**Metadados**: ‚ö†Ô∏è Vers√£o anterior da pipeline (sem metadados otimizados)")
        relatorio.append("")
        
        # Resumo executivo
        relatorio.append("## üìà Resumo Executivo")
        relatorio.append("")
        
        total_validas = sum(metricas_por_modelo[modelo]['evidently'].get('respostas_validas', 0) 
                           for modelo in dados_por_modelo.keys())
        taxa_geral = total_validas / total_respostas if total_respostas > 0 else 0
        
        # Gerar insights autom√°ticos
        insights = self._gerar_insights_executivos(metricas_por_modelo, taxa_geral * 100)
        relatorio.extend(insights)
        relatorio.append("")
        
        # Gerar rankings detalhados
        relatorio_rankings = self._gerar_rankings_detalhados(metricas_por_modelo)
        relatorio.append(relatorio_rankings)
        
        # Ranking dos modelos (vers√£o simplificada)
        relatorio.append("## üèÜ Ranking dos Modelos")
        relatorio.append("")
        
        modelos_ranking = self._calcular_ranking_modelos(metricas_por_modelo)
        
        for i, (modelo, score_composto) in enumerate(modelos_ranking, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∫"
            relatorio.append(f"### {emoji} {modelo} (Score: {score_composto:.4f})")
            relatorio.append("")
            
            # M√©tricas acad√™micas
            metricas_acad = metricas_por_modelo[modelo]['academicas']
            relatorio.append("**M√©tricas Acad√™micas:**")
            relatorio.append(f"- **BLEU Score**: {metricas_acad.get('bleu_medio', 0):.4f}")
            relatorio.append(f"- **ROUGE-1**: {metricas_acad.get('rouge1_medio', 0):.4f}")
            relatorio.append(f"- **ROUGE-2**: {metricas_acad.get('rouge2_medio', 0):.4f}")
            relatorio.append(f"- **ROUGE-L**: {metricas_acad.get('rougeL_medio', 0):.4f}")
            relatorio.append(f"- **BERTScore**: {metricas_acad.get('bertscore_f1_medio', 0):.4f}")
            relatorio.append("")
            
            # M√©tricas Evidently AI
            metricas_ev = metricas_por_modelo[modelo]['evidently']
            relatorio.append("**M√©tricas Evidently AI:**")
            relatorio.append(f"- **Respostas V√°lidas**: {metricas_ev.get('respostas_validas', 0)}")
            relatorio.append(f"- **Taxa de Validade**: {metricas_ev.get('taxa_validas', 0):.1%}")
            relatorio.append(f"- **Comprimento M√©dio**: {metricas_ev.get('comprimento_medio', 0):.1f} ¬± {metricas_ev.get('comprimento_std', 0):.1f} caracteres")
            relatorio.append(f"- **Palavras M√©dias**: {metricas_ev.get('palavras_medias', 0):.1f} ¬± {metricas_ev.get('palavras_std', 0):.1f}")
            relatorio.append("")
            
            # M√©tricas de Benchmarks
            if 'benchmarks' in metricas_por_modelo[modelo]:
                metricas_bench = metricas_por_modelo[modelo]['benchmarks']
                relatorio.append("**M√©tricas de Benchmarks:**")
                
                if 'mmlu' in metricas_bench:
                    mmlu_data = metricas_bench['mmlu']
                    relatorio.append(f"- **MMLU Accuracy**: {mmlu_data.get('accuracy', 0):.4f} ({mmlu_data.get('correct_answers', 0)}/{mmlu_data.get('total_questions', 0)})")
                
                if 'hellaswag' in metricas_bench:
                    hellaswag_data = metricas_bench['hellaswag']
                    relatorio.append(f"- **HellaSwag Accuracy**: {hellaswag_data.get('accuracy', 0):.4f} ({hellaswag_data.get('correct_answers', 0)}/{hellaswag_data.get('total_questions', 0)})")
                
                relatorio.append("")
            
            relatorio.append("---")
            relatorio.append("")
        
        # An√°lise comparativa
        relatorio.append("## üìä An√°lise Comparativa")
        relatorio.append("")
        
        # Ranking por confiabilidade
        modelos_confiabilidade = [(modelo, metricas_por_modelo[modelo]['evidently'].get('taxa_validas', 0)) 
                                 for modelo in dados_por_modelo.keys()]
        modelos_confiabilidade.sort(key=lambda x: x[1], reverse=True)
        
        relatorio.append("**Ranking por Confiabilidade:**")
        for i, (modelo, taxa) in enumerate(modelos_confiabilidade, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∫"
            relatorio.append(f"{emoji} **{modelo}**: {taxa:.1%}")
            relatorio.append("")
            
        # Ranking por comprimento
        modelos_comprimento = [(modelo, metricas_por_modelo[modelo]['evidently'].get('comprimento_medio', 0)) 
                              for modelo in dados_por_modelo.keys()]
        modelos_comprimento.sort(key=lambda x: x[1], reverse=True)
        
        relatorio.append("**Ranking por Comprimento de Resposta:**")
        for i, (modelo, comprimento) in enumerate(modelos_comprimento, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∫"
            relatorio.append(f"{emoji} **{modelo}**: {comprimento:.1f} caracteres")
            relatorio.append("")
        
        # Recomenda√ß√µes
        relatorio.append("## üí° Recomenda√ß√µes")
        relatorio.append("")
        
        if modelos_ranking:
            melhor_modelo = modelos_ranking[0][0]
            relatorio.append(f"### üèÜ Modelo Recomendado: {melhor_modelo}")
            relatorio.append("")
            relatorio.append("**Justificativa:**")
            relatorio.append("- Melhor score composto considerando todas as m√©tricas")
            relatorio.append("- Equil√≠brio entre precis√£o acad√™mica e confiabilidade")
            relatorio.append("- Boa performance em m√©tricas de qualidade textual")
            relatorio.append("")
        
        if modelos_confiabilidade:
            mais_confiavel = modelos_confiabilidade[0][0]
            relatorio.append(f"### üõ°Ô∏è Modelo Mais Confi√°vel: {mais_confiavel}")
            relatorio.append(f"- Taxa de respostas v√°lidas: {modelos_confiabilidade[0][1]:.1%}")
            relatorio.append("")
        
        if modelos_comprimento:
            mais_detalhado = modelos_comprimento[0][0]
            relatorio.append(f"### üìù Modelo Mais Detalhado: {mais_detalhado}")
            relatorio.append(f"- Comprimento m√©dio: {modelos_comprimento[0][1]:.1f} caracteres")
        relatorio.append("")
        
        return "\n".join(relatorio)
    
    def _analisar_correlacoes_metricas(self, metricas_por_modelo: Dict[str, Dict]) -> str:
        """
        Analisa correla√ß√µes entre diferentes m√©tricas para identificar consist√™ncia.
        
        Args:
            metricas_por_modelo: Dicion√°rio com m√©tricas por modelo
            
        Returns:
            String com an√°lise de correla√ß√µes
        """
        import numpy as np
        
        # Preparar dados para an√°lise de correla√ß√£o
        modelos = []
        bleu_scores = []
        rouge1_scores = []
        rouge2_scores = []
        rougeL_scores = []
        bertscore_scores = []
        
        for modelo, metricas in metricas_por_modelo.items():
            metricas_acad = metricas['academicas']
            modelos.append(modelo)
            bleu_scores.append(metricas_acad.get('bleu_medio', 0))
            rouge1_scores.append(metricas_acad.get('rouge1_medio', 0))
            rouge2_scores.append(metricas_acad.get('rouge2_medio', 0))
            rougeL_scores.append(metricas_acad.get('rougeL_medio', 0))
            bertscore_scores.append(metricas_acad.get('bertscore_f1_medio', 0))
        
        # Calcular correla√ß√µes
        correlacoes = []
        
        # ROUGE-1 vs BERTScore (deveria ter alta correla√ß√£o)
        corr_rouge1_bert = np.corrcoef(rouge1_scores, bertscore_scores)[0, 1]
        correlacoes.append(f"- **ROUGE-1 vs BERTScore**: {corr_rouge1_bert:.3f}")
        
        # ROUGE-2 vs ROUGE-L (deveria ter alta correla√ß√£o)
        corr_rouge2_rougeL = np.corrcoef(rouge2_scores, rougeL_scores)[0, 1]
        correlacoes.append(f"- **ROUGE-2 vs ROUGE-L**: {corr_rouge2_rougeL:.3f}")
        
        # BLEU vs ROUGE-1 (correla√ß√£o moderada esperada)
        corr_bleu_rouge1 = np.corrcoef(bleu_scores, rouge1_scores)[0, 1]
        correlacoes.append(f"- **BLEU vs ROUGE-1**: {corr_bleu_rouge1:.3f}")
        
        # An√°lise de consist√™ncia
        analise = []
        analise.append("## üìä An√°lise de Correla√ß√µes entre M√©tricas")
        analise.append("")
        analise.append("### Correla√ß√µes Calculadas:")
        analise.extend(correlacoes)
        analise.append("")
        
        # Interpreta√ß√£o
        analise.append("### Interpreta√ß√£o:")
        if corr_rouge1_bert > 0.7:
            analise.append("‚úÖ **ROUGE-1 e BERTScore** t√™m alta correla√ß√£o (consist√™ncia boa)")
        elif corr_rouge1_bert > 0.4:
            analise.append("‚ö†Ô∏è **ROUGE-1 e BERTScore** t√™m correla√ß√£o moderada")
        else:
            analise.append("‚ùå **ROUGE-1 e BERTScore** t√™m baixa correla√ß√£o (inconsist√™ncia)")
        
        if corr_rouge2_rougeL > 0.7:
            analise.append("‚úÖ **ROUGE-2 e ROUGE-L** t√™m alta correla√ß√£o (consist√™ncia boa)")
        elif corr_rouge2_rougeL > 0.4:
            analise.append("‚ö†Ô∏è **ROUGE-2 e ROUGE-L** t√™m correla√ß√£o moderada")
        else:
            analise.append("‚ùå **ROUGE-2 e ROUGE-L** t√™m baixa correla√ß√£o (inconsist√™ncia)")
        
        analise.append("")
        return "\n".join(analise)
    
    def _obter_descricao_metrica(self, metrica: str) -> str:
        """
        Retorna descri√ß√£o amig√°vel da m√©trica.
        
        Args:
            metrica: Nome da m√©trica
            
        Returns:
            Descri√ß√£o da m√©trica
        """
        descricoes = {
            'BLEU': 'Mede a similaridade entre texto gerado e refer√™ncia (0-1, maior √© melhor)',
            'ROUGE-1': 'Mede sobreposi√ß√£o de palavras individuais (0-1, maior √© melhor)',
            'ROUGE-2': 'Mede sobreposi√ß√£o de bigramas (0-1, maior √© melhor)',
            'ROUGE-L': 'Mede sobreposi√ß√£o de subsequ√™ncias mais longas (0-1, maior √© melhor)',
            'BERTScore': 'Mede similaridade sem√¢ntica usando embeddings BERT (0-1, maior √© melhor)',
            'Taxa de Validade': 'Percentual de respostas v√°lidas (0-1, maior √© melhor)',
            'Comprimento M√©dio': 'Comprimento m√©dio das respostas em caracteres',
            'Palavras M√©dias': 'N√∫mero m√©dio de palavras por resposta',
            'Consist√™ncia de Comprimento': 'Consist√™ncia no tamanho das respostas (menor desvio √© melhor)'
        }
        return descricoes.get(metrica, '')
    
    def _obter_descricao_categoria(self, categoria: str) -> str:
        """
        Retorna descri√ß√£o amig√°vel da categoria.
        
        Args:
            categoria: Nome da categoria
            
        Returns:
            Descri√ß√£o da categoria
        """
        descricoes = {
            'Score Acad√™mico': 'Combina√ß√£o de m√©tricas de qualidade de texto (BLEU, ROUGE, BERTScore)',
            'Score Evidently AI': 'M√©tricas de qualidade e consist√™ncia das respostas',
            'Score Geral': 'Score final combinando todas as m√©tricas com pesos balanceados'
        }
        return descricoes.get(categoria, '')
    
    def _gerar_insights_executivos(self, metricas_por_modelo: Dict[str, Dict], taxa_sucesso: float) -> List[str]:
        """
        Gera insights executivos baseados nas m√©tricas.
        
        Args:
            metricas_por_modelo: Dicion√°rio com m√©tricas por modelo
            taxa_sucesso: Taxa de sucesso geral
            
        Returns:
            Lista de insights
        """
        insights = []
        
        # Insight sobre taxa de sucesso
        if taxa_sucesso >= 90:
            insights.append("‚úÖ **Excelente taxa de sucesso**: {:.1f}% das respostas s√£o v√°lidas".format(taxa_sucesso))
        elif taxa_sucesso >= 80:
            insights.append("‚ö†Ô∏è **Boa taxa de sucesso**: {:.1f}% das respostas s√£o v√°lidas".format(taxa_sucesso))
        else:
            insights.append("‚ùå **Taxa de sucesso baixa**: {:.1f}% das respostas s√£o v√°lidas".format(taxa_sucesso))
        
        # Identificar melhor modelo por categoria
        melhor_academico = None
        melhor_evidently = None
        melhor_geral = None
        
        for modelo, metricas in metricas_por_modelo.items():
            if 'academicas' in metricas:
                score_acad = metricas['academicas'].get('score_composto', 0)
                if melhor_academico is None or score_acad > melhor_academico[1]:
                    melhor_academico = (modelo, score_acad)
            
            if 'evidently' in metricas:
                score_ev = metricas['evidently'].get('taxa_validas', 0)
                if melhor_evidently is None or score_ev > melhor_evidently[1]:
                    melhor_evidently = (modelo, score_ev)
        
        if melhor_academico:
            insights.append("üèÜ **Melhor modelo acad√™mico**: {} (score: {:.3f})".format(
                melhor_academico[0], melhor_academico[1]))
        
        if melhor_evidently:
            insights.append("üìä **Melhor modelo em consist√™ncia**: {} (taxa: {:.1%})".format(
                melhor_evidently[0], melhor_evidently[1]))
        
        # Identificar modelos problem√°ticos
        modelos_problematicos = []
        for modelo, metricas in metricas_por_modelo.items():
            if 'evidently' in metricas:
                taxa_validas = metricas['evidently'].get('taxa_validas', 1)
                if taxa_validas < 0.5:  # Menos de 50% de respostas v√°lidas
                    modelos_problematicos.append((modelo, taxa_validas))
        
        if modelos_problematicos:
            insights.append("‚ö†Ô∏è **Modelos com problemas**: {}".format(
                ", ".join([f"{m[0]} ({m[1]:.1%})" for m in modelos_problematicos])))
        
        return insights
    
    def _obter_emoji_rank(self, rank: int) -> str:
        """
        Retorna emoji baseado no rank.
        
        Args:
            rank: Posi√ß√£o no ranking
            
        Returns:
            Emoji correspondente
        """
        if rank == 1:
            return 'ü•á'
        elif rank == 2:
            return 'ü•à'
        elif rank == 3:
            return 'ü•â'
        elif rank <= 5:
            return 'üèÖ'
        else:
            return 'üìä'
    
    def _calcular_ranking_modelos(self, metricas_por_modelo: Dict[str, Dict]) -> List[tuple]:
        """Calcula ranking dos modelos baseado em score composto."""
        rankings = []
        
        for modelo, metricas in metricas_por_modelo.items():
            # M√©tricas acad√™micas
            metricas_acad = metricas['academicas']
            bleu = metricas_acad.get('bleu_medio', 0)
            rouge1 = metricas_acad.get('rouge1_medio', 0)
            rouge2 = metricas_acad.get('rouge2_medio', 0)
            rougeL = metricas_acad.get('rougeL_medio', 0)
            bertscore = metricas_acad.get('bertscore_f1_medio', 0)
            
            # M√©tricas Evidently AI
            metricas_ev = metricas['evidently']
            taxa_validas = metricas_ev.get('taxa_validas', 0)
            respostas_validas = metricas_ev.get('respostas_validas', 0)
            
            # Penalizar modelos com poucas respostas v√°lidas
            fator_confiabilidade = min(1.0, respostas_validas / 10.0)  # Penaliza se < 10 respostas v√°lidas
            
            # Score composto com pesos balanceados para maior consist√™ncia
            # ROUGE-1 e BERTScore t√™m pesos maiores por serem mais confi√°veis
            score_composto = (
                bleu * 0.10 +           # Reduzido: pode ser muito baixo
                rouge1 * 0.25 +         # Aumentado: mais confi√°vel
                rouge2 * 0.20 +         # Aumentado: agora corrigido
                rougeL * 0.15 +         # Mantido
                bertscore * 0.25 +      # Mantido: muito confi√°vel
                taxa_validas * 0.05     # Reduzido: n√£o deve dominar
            ) * fator_confiabilidade
            
            # Penaliza√ß√£o adicional para modelos com muito poucas respostas v√°lidas
            if respostas_validas < 5:
                score_composto *= 0.3  # Penaliza√ß√£o severa
            elif respostas_validas < 10:
                score_composto *= 0.6  # Penaliza√ß√£o moderada
            
            rankings.append((modelo, score_composto))
        
        return sorted(rankings, key=lambda x: x[1], reverse=True)
    
    def _gerar_rankings_detalhados(self, metricas_por_modelo: Dict[str, Dict]) -> str:
        """Gera rankings detalhados com m√©tricas normalizadas."""
        print("üèÜ Gerando rankings detalhados...")
        
        # Preparar dados para normaliza√ß√£o
        dados_metricas = []
        for modelo, metricas in metricas_por_modelo.items():
            dados_modelo = {"Modelo": modelo}
            
            # M√©tricas acad√™micas
            metricas_acad = metricas.get('academicas', {})
            dados_modelo.update({
                "BLEU": metricas_acad.get('bleu_medio', 0),
                "ROUGE-1": metricas_acad.get('rouge1_medio', 0),
                "ROUGE-2": metricas_acad.get('rouge2_medio', 0),
                "ROUGE-L": metricas_acad.get('rougeL_medio', 0),
                "BERTScore": metricas_acad.get('bertscore_f1_medio', 0)
            })
            
            # M√©tricas Evidently AI
            metricas_ev = metricas.get('evidently', {})
            dados_modelo.update({
                "Respostas V√°lidas": metricas_ev.get('respostas_validas', 0),
                "Taxa de Validade": metricas_ev.get('taxa_validas', 0),
                "Comprimento M√©dio": metricas_ev.get('comprimento_medio', 0),
                "Palavras M√©dias": metricas_ev.get('palavras_medias', 0),
                "Consist√™ncia de Comprimento": self._calcular_consistencia_comprimento(metricas_ev)
            })
            
            dados_metricas.append(dados_modelo)
        
        # Converter para DataFrame
        df = pd.DataFrame(dados_metricas)
        
        # Normalizar m√©tricas
        df_normalizado = self._normalizar_metricas(df)
        
        # Gerar rankings
        rankings_individuais = self._gerar_rankings_individuais(df_normalizado)
        rankings_consolidados = self._gerar_rankings_consolidados(df_normalizado)
        
        # Gerar relat√≥rio de rankings
        relatorio = []
        relatorio.append("## üèÜ Rankings Detalhados por M√©trica")
        relatorio.append("")
        
        # Rankings por m√©trica individual
        for metrica, ranking in rankings_individuais.items():
            relatorio.append(f"### {metrica}")
            relatorio.append("")
            
            # Adicionar descri√ß√£o da m√©trica
            descricao_metrica = self._obter_descricao_metrica(metrica)
            if descricao_metrica:
                relatorio.append(f"*{descricao_metrica}*")
                relatorio.append("")
            
            # Tabela com formata√ß√£o melhorada
            relatorio.append("| üèÜ | Modelo | Score | Rank |")
            relatorio.append("|:---:|:-------|------:|:----:|")
            
            for _, row in ranking.iterrows():
                # Emoji baseado no rank
                emoji_rank = self._obter_emoji_rank(row['Rank'])
                score = row[f'Normalized {metrica}']
                relatorio.append(f"| {emoji_rank} | **{row['Modelo']}** | {score:.4f} | {row['Rank']} |")
            relatorio.append("")
        
        # An√°lise de correla√ß√µes
        relatorio.append(self._analisar_correlacoes_metricas(metricas_por_modelo))
        relatorio.append("")
        
        # Rankings consolidados
        relatorio.append("## üìä Rankings Consolidados por Categoria")
        relatorio.append("")
        
        for categoria, ranking in rankings_consolidados.items():
            relatorio.append(f"### {categoria}")
            relatorio.append("")
            
            # Adicionar descri√ß√£o da categoria
            descricao_categoria = self._obter_descricao_categoria(categoria)
            if descricao_categoria:
                relatorio.append(f"*{descricao_categoria}*")
                relatorio.append("")
            
            # Tabela com formata√ß√£o melhorada
            relatorio.append("| üèÜ | Modelo | Score | Rank |")
            relatorio.append("|:---:|:-------|------:|:----:|")
            
            for _, row in ranking.iterrows():
                emoji_rank = self._obter_emoji_rank(row['Rank'])
                score = row[categoria]
                relatorio.append(f"| {emoji_rank} | **{row['Modelo']}** | {score:.4f} | {row['Rank']} |")
            relatorio.append("")
        
        # An√°lise qualitativa
        analise_qualitativa = self._gerar_analise_qualitativa(df_normalizado, rankings_consolidados)
        relatorio.append(analise_qualitativa)
        
        return "\n".join(relatorio)
    
    def _calcular_consistencia_comprimento(self, metricas_ev: Dict) -> float:
        """Calcula consist√™ncia de comprimento baseada no coeficiente de varia√ß√£o."""
        comprimento_medio = metricas_ev.get('comprimento_medio', 0)
        comprimento_std = metricas_ev.get('comprimento_std', 0)
        
        if comprimento_medio > 0:
            cv = (comprimento_std / comprimento_medio) * 100
            # Inverter CV para ranking (menor CV = maior consist√™ncia)
            return max(0, 100 - cv)
        return 0
    
    def _normalizar_metricas(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normaliza m√©tricas para escala 0-1 (quanto maior melhor)."""
        df_normalizado = df.copy()
        
        # M√©tricas acad√™micas
        academic_metrics = ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "BERTScore"]
        # M√©tricas Evidently AI
        evidently_metrics = ["Respostas V√°lidas", "Taxa de Validade", "Comprimento M√©dio", 
                           "Palavras M√©dias", "Consist√™ncia de Comprimento"]
        
        for coluna in academic_metrics + evidently_metrics:
            if coluna in df.columns:
                # Filtrar valores v√°lidos (n√£o nulos e n√£o infinitos)
                valores_validos = df[coluna].replace([np.inf, -np.inf], np.nan).dropna()
                
                if len(valores_validos) == 0:
                    # Se n√£o h√° valores v√°lidos, usar 0
                    df_normalizado[f"Normalized {coluna}"] = 0.0
                    continue
                
                max_val = valores_validos.max()
                min_val = valores_validos.min()
                
                if max_val > min_val:
                    # Normaliza√ß√£o min-max
                    df_normalizado[f"Normalized {coluna}"] = (df[coluna] - min_val) / (max_val - min_val)
                    # Garantir que valores inv√°lidos sejam 0
                    df_normalizado[f"Normalized {coluna}"] = df_normalizado[f"Normalized {coluna}"].fillna(0.0)
                    df_normalizado[f"Normalized {coluna}"] = df_normalizado[f"Normalized {coluna}"].replace([np.inf, -np.inf], 0.0)
                else:
                    # Se todos os valores s√£o iguais e n√£o zero, usar 1.0
                    # Se todos s√£o zero, usar 0.0
                    if max_val > 0:
                        df_normalizado[f"Normalized {coluna}"] = 1.0
                    else:
                        df_normalizado[f"Normalized {coluna}"] = 0.0
        
        return df_normalizado
    
    def _gerar_rankings_individuais(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """Gera rankings por cada m√©trica individual."""
        rankings = {}
        
        # M√©tricas acad√™micas
        academic_metrics = ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "BERTScore"]
        # M√©tricas Evidently AI
        evidently_metrics = ["Respostas V√°lidas", "Taxa de Validade", "Comprimento M√©dio", 
                           "Palavras M√©dias", "Consist√™ncia de Comprimento"]
        
        for metrica in academic_metrics + evidently_metrics:
            coluna_normalizada = f"Normalized {metrica}"
            if coluna_normalizada in df.columns:
                ranking = df.sort_values(by=coluna_normalizada, ascending=False)[
                    ["Modelo", coluna_normalizada]
                ].reset_index(drop=True)
                ranking["Rank"] = ranking.index + 1
                rankings[metrica] = ranking
        
        return rankings
    
    def _gerar_rankings_consolidados(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """Gera rankings consolidados por categoria."""
        rankings = {}
        
        # Score Acad√™mico
        academic_metrics = ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "BERTScore"]
        colunas_academicas = [f"Normalized {metrica}" for metrica in academic_metrics 
                             if f"Normalized {metrica}" in df.columns]
        if colunas_academicas:
            df["Score Acad√™mico"] = df[colunas_academicas].mean(axis=1)
            ranking_academico = df.sort_values(by="Score Acad√™mico", ascending=False)[
                ["Modelo", "Score Acad√™mico"]
            ].reset_index(drop=True)
            ranking_academico["Rank"] = ranking_academico.index + 1
            rankings["Score Acad√™mico"] = ranking_academico
        
        # Score Evidently AI
        evidently_metrics = ["Respostas V√°lidas", "Taxa de Validade", "Comprimento M√©dio", 
                           "Palavras M√©dias", "Consist√™ncia de Comprimento"]
        colunas_evidently = [f"Normalized {metrica}" for metrica in evidently_metrics 
                            if f"Normalized {metrica}" in df.columns]
        if colunas_evidently:
            df["Score Evidently AI"] = df[colunas_evidently].mean(axis=1)
            ranking_evidently = df.sort_values(by="Score Evidently AI", ascending=False)[
                ["Modelo", "Score Evidently AI"]
            ].reset_index(drop=True)
            ranking_evidently["Rank"] = ranking_evidently.index + 1
            rankings["Score Evidently AI"] = ranking_evidently
        
        # Score Geral
        todas_colunas = colunas_academicas + colunas_evidently
        if todas_colunas:
            df["Score Geral"] = df[todas_colunas].mean(axis=1)
            ranking_geral = df.sort_values(by="Score Geral", ascending=False)[
                ["Modelo", "Score Geral"]
            ].reset_index(drop=True)
            ranking_geral["Rank"] = ranking_geral.index + 1
            rankings["Score Geral"] = ranking_geral
        
        return rankings
    
    def _gerar_analise_qualitativa(self, df: pd.DataFrame, rankings: Dict[str, pd.DataFrame]) -> str:
        """Gera an√°lise qualitativa dos resultados."""
        analise = []
        analise.append("## üîç An√°lise Qualitativa")
        analise.append("")
        
        # Modelo mais consistente (menor varia√ß√£o)
        if "Normalized Consist√™ncia de Comprimento" in df.columns:
            mais_consistente = df.loc[df["Normalized Consist√™ncia de Comprimento"].idxmax(), "Modelo"]
            analise.append(f"### üéØ Modelo Mais Consistente: {mais_consistente}")
            analise.append("- Menor varia√ß√£o no comprimento das respostas")
            analise.append("- Maior estabilidade de performance")
            analise.append("")
        
        # Modelo com maior fidelidade de texto (melhor BERTScore)
        if "Normalized BERTScore" in df.columns:
            melhor_bertscore = df.loc[df["Normalized BERTScore"].idxmax(), "Modelo"]
            analise.append(f"### üß† Modelo com Maior Fidelidade de Texto: {melhor_bertscore}")
            analise.append("- Melhor similaridade sem√¢ntica com refer√™ncias")
            analise.append("- Maior qualidade de conte√∫do gerado")
            analise.append("")
        
        # Modelo com menor dispers√£o (melhor confiabilidade)
        if "Normalized Taxa de Validade" in df.columns:
            mais_confiavel = df.loc[df["Normalized Taxa de Validade"].idxmax(), "Modelo"]
            analise.append(f"### üõ°Ô∏è Modelo Mais Confi√°vel: {mais_confiavel}")
            analise.append("- Maior taxa de respostas v√°lidas")
            analise.append("- Menor incid√™ncia de erros")
            analise.append("")
        
        # Modelo mais detalhado (maior comprimento)
        if "Normalized Comprimento M√©dio" in df.columns:
            mais_detalhado = df.loc[df["Normalized Comprimento M√©dio"].idxmax(), "Modelo"]
            analise.append(f"### üìù Modelo Mais Detalhado: {mais_detalhado}")
            analise.append("- Respostas mais longas e detalhadas")
            analise.append("- Maior riqueza de informa√ß√£o")
            analise.append("")
        
        # An√°lise de correla√ß√µes
        analise.append("### üìà An√°lise de Correla√ß√µes")
        analise.append("")
        
        # Correla√ß√£o entre m√©tricas acad√™micas e Evidently AI
        colunas_academicas = [f"Normalized {metrica}" for metrica in ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "BERTScore"]
                             if f"Normalized {metrica}" in df.columns]
        colunas_evidently = [f"Normalized {metrica}" for metrica in ["Respostas V√°lidas", "Taxa de Validade", "Comprimento M√©dio", 
                           "Palavras M√©dias", "Consist√™ncia de Comprimento"]
                            if f"Normalized {metrica}" in df.columns]
        
        if colunas_academicas and colunas_evidently:
            score_academico = df[colunas_academicas].mean(axis=1)
            score_evidently = df[colunas_evidently].mean(axis=1)
            correlacao = np.corrcoef(score_academico, score_evidently)[0, 1]
            
            analise.append(f"- **Correla√ß√£o Acad√™mico vs Evidently AI**: {correlacao:.3f}")
            
            if correlacao > 0.7:
                analise.append("  - Forte correla√ß√£o positiva: modelos bons academicamente tamb√©m s√£o bons em qualidade de dados")
            elif correlacao > 0.3:
                analise.append("  - Correla√ß√£o moderada: alguma rela√ß√£o entre m√©tricas acad√™micas e qualidade de dados")
            else:
                analise.append("  - Correla√ß√£o fraca: m√©tricas acad√™micas e qualidade de dados s√£o independentes")
            analise.append("")
        
        # An√°lise de modelos open source vs propriet√°rios
        modelos_open_source = [m for m in df['Modelo'] if any(oss in m.lower() for oss in ['llama', 'gpt_oss', 'qwen'])]
        modelos_proprietarios = [m for m in df['Modelo'] if any(prop in m.lower() for prop in ['gemini'])]
        
        if modelos_open_source and modelos_proprietarios and "Score Geral" in df.columns:
            score_oss = df[df['Modelo'].isin(modelos_open_source)]["Score Geral"].mean()
            score_prop = df[df['Modelo'].isin(modelos_proprietarios)]["Score Geral"].mean()
            
            analise.append("### üîì vs üîí Open Source vs Propriet√°rios")
            analise.append("")
            analise.append(f"- **Score M√©dio Open Source**: {score_oss:.3f}")
            analise.append(f"- **Score M√©dio Propriet√°rios**: {score_prop:.3f}")
            
            if score_oss > score_prop:
                analise.append("- **Conclus√£o**: Modelos open source superam os propriet√°rios em performance geral")
            elif score_prop > score_oss:
                analise.append("- **Conclus√£o**: Modelos propriet√°rios superam os open source em performance geral")
            else:
                analise.append("- **Conclus√£o**: Performance similar entre modelos open source e propriet√°rios")
            analise.append("")
        
        return "\n".join(analise)
    
    def _salvar_rankings_detalhados(self, metricas_por_modelo: Dict[str, Dict], pasta_analise: str):
        """Salva rankings detalhados em arquivos separados."""
        print("üíæ Salvando rankings detalhados...")
        
        # Preparar dados para normaliza√ß√£o
        dados_metricas = []
        for modelo, metricas in metricas_por_modelo.items():
            dados_modelo = {"Modelo": modelo}
            
            # M√©tricas acad√™micas
            metricas_acad = metricas.get('academicas', {})
            dados_modelo.update({
                "BLEU": metricas_acad.get('bleu_medio', 0),
                "ROUGE-1": metricas_acad.get('rouge1_medio', 0),
                "ROUGE-2": metricas_acad.get('rouge2_medio', 0),
                "ROUGE-L": metricas_acad.get('rougeL_medio', 0),
                "BERTScore": metricas_acad.get('bertscore_f1_medio', 0)
            })
            
            # M√©tricas Evidently AI
            metricas_ev = metricas.get('evidently', {})
            dados_modelo.update({
                "Respostas V√°lidas": metricas_ev.get('respostas_validas', 0),
                "Taxa de Validade": metricas_ev.get('taxa_validas', 0),
                "Comprimento M√©dio": metricas_ev.get('comprimento_medio', 0),
                "Palavras M√©dias": metricas_ev.get('palavras_medias', 0),
                "Consist√™ncia de Comprimento": self._calcular_consistencia_comprimento(metricas_ev)
            })
            
            dados_metricas.append(dados_modelo)
        
        # Converter para DataFrame
        df = pd.DataFrame(dados_metricas)
        
        # Normalizar m√©tricas
        df_normalizado = self._normalizar_metricas(df)
        
        # Gerar rankings
        rankings_individuais = self._gerar_rankings_individuais(df_normalizado)
        rankings_consolidados = self._gerar_rankings_consolidados(df_normalizado)
        
        # Salvar arquivo de rankings principal
        arquivo_rankings = os.path.join(pasta_analise, "rankings.md")
        with open(arquivo_rankings, 'w', encoding='utf-8') as f:
            f.write("# üèÜ Rankings Comparativos de Modelos LLM\n\n")
            f.write(f"**Data da An√°lise**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n\n")
            
            # Rankings por m√©trica individual
            f.write("## Rankings por M√©trica Individual\n\n")
            for metrica, ranking in rankings_individuais.items():
                f.write(f"### {metrica}\n")
                f.write(ranking.to_markdown(index=False))
                f.write("\n\n")
            
            # Rankings consolidados
            f.write("## Rankings Consolidados por Categoria\n\n")
            for categoria, ranking in rankings_consolidados.items():
                f.write(f"### {categoria}\n")
                f.write(ranking.to_markdown(index=False))
                f.write("\n\n")
        
        # Salvar m√©tricas normalizadas em JSON
        arquivo_json = os.path.join(pasta_analise, "normalized_metrics.json")
        df_normalizado.to_json(arquivo_json, orient='records', indent=2, force_ascii=False)
        
        # Salvar script de gera√ß√£o de rankings
        script_rankings = os.path.join(pasta_analise, "generate_rankings.py")
        self._gerar_script_rankings(script_rankings)
        
        print(f"‚úÖ Rankings salvos em: {arquivo_rankings}")
        print(f"‚úÖ M√©tricas normalizadas em: {arquivo_json}")
        print(f"‚úÖ Script de gera√ß√£o em: {script_rankings}")
    
    def _gerar_script_rankings(self, caminho_script: str):
        """Gera script Python para reproduzir os rankings."""
        script_content = '''import json
import pandas as pd

with open("normalized_metrics.json", "r", encoding="utf-8") as f:
    data = json.load(f)

df = pd.DataFrame(data)

# Define metric categories
academic_metrics = [
    "Normalized BLEU",
    "Normalized ROUGE-1",
    "Normalized ROUGE-2", 
    "Normalized ROUGE-L",
    "Normalized BERTScore"
]
evidently_ai_metrics = [
    "Normalized Respostas V√°lidas",
    "Normalized Taxa de Validade",
    "Normalized Comprimento M√©dio",
    "Normalized Palavras M√©dias",
    "Normalized Consist√™ncia de Comprimento"
]

# --- Ranking por cada m√©trica individual ---
individual_rankings = {}
for col in academic_metrics + evidently_ai_metrics:
    if col in df.columns:
        individual_rankings[col] = df.sort_values(by=col, ascending=False)[["Modelo", col]].reset_index(drop=True)
        individual_rankings[col]["Rank"] = individual_rankings[col].index + 1

# --- Ranking consolidado por categoria ---
if academic_metrics:
    df["Score Acad√™mico"] = df[academic_metrics].mean(axis=1)
    academic_ranking = df.sort_values(by="Score Acad√™mico", ascending=False)[[
        "Modelo", "Score Acad√™mico"]].reset_index(drop=True)
    academic_ranking["Rank"] = academic_ranking.index + 1

if evidently_ai_metrics:
    df["Score Evidently AI"] = df[evidently_ai_metrics].mean(axis=1)
    evidently_ai_ranking = df.sort_values(by="Score Evidently AI", ascending=False)[[
        "Modelo", "Score Evidently AI"]].reset_index(drop=True)
    evidently_ai_ranking["Rank"] = evidently_ai_ranking.index + 1

# --- Ranking geral ---
all_metrics = academic_metrics + evidently_ai_metrics
if all_metrics:
    df["Score Geral"] = df[all_metrics].mean(axis=1)
    general_ranking = df.sort_values(by="Score Geral", ascending=False)[[
        "Modelo", "Score Geral"]].reset_index(drop=True)
    general_ranking["Rank"] = general_ranking.index + 1

# --- Salvar resultados em arquivos Markdown ---
with open("rankings.md", "w", encoding="utf-8") as f:
    f.write("# üèÜ Rankings Comparativos de Modelos LLM\\n\\n")

    f.write("## Rankings por M√©trica Individual\\n\\n")
    for metric, ranking_df in individual_rankings.items():
        f.write(f"### {metric.replace('Normalized ', '')}\\n")
        f.write(ranking_df.to_markdown(index=False))
        f.write("\\n\\n")

    f.write("## Rankings Consolidados por Categoria\\n\\n")
    if 'academic_ranking' in locals():
        f.write("### Score Acad√™mico\\n")
        f.write(academic_ranking.to_markdown(index=False))
        f.write("\\n\\n")
    if 'evidently_ai_ranking' in locals():
        f.write("### Score Evidently AI\\n")
        f.write(evidently_ai_ranking.to_markdown(index=False))
        f.write("\\n\\n")

    f.write("## Ranking Geral\\n\\n")
    if 'general_ranking' in locals():
        f.write(general_ranking.to_markdown(index=False))
        f.write("\\n\\n")

print("Rankings gerados e salvos em rankings.md")
'''
        
        with open(caminho_script, 'w', encoding='utf-8') as f:
            f.write(script_content)
    
    def executar_analise_completa(self) -> str:
        """Executa an√°lise completa e retorna caminho do relat√≥rio."""
        print("üöÄ Iniciando An√°lise Consolidada")
        print("=" * 60)
        
        # Encontrar execu√ß√µes
        execucoes = self.encontrar_execucoes()
        print(f"üìÅ Encontradas {len(execucoes)} execu√ß√µes")
        
        if not execucoes:
            print("‚ùå Nenhuma execu√ß√£o encontrada para an√°lise")
            return None
        
        # Consolidar dados por modelo
        dados_por_modelo = self.consolidar_dados_por_modelo(execucoes)
        
        if not dados_por_modelo:
            print("‚ùå Nenhum modelo encontrado para an√°lise")
            return None
        
        # Criar pasta de an√°lise
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pasta_analise = os.path.join(self.pasta_analysis, f"analise_consolidada_{timestamp}")
        os.makedirs(pasta_analise, exist_ok=True)
        
        # Processar cada modelo
        metricas_por_modelo = {}
        
        for modelo, df in dados_por_modelo.items():
            print(f"\nü§ñ Processando modelo: {modelo}")
            
            # Criar pasta do modelo
            pasta_modelo = os.path.join(pasta_analise, f"modelo_{modelo}")
            os.makedirs(pasta_modelo, exist_ok=True)
            
            # Calcular m√©tricas acad√™micas
            df_com_metricas, metricas_academicas, relatorio_academicas = self.calcular_metricas_academicas(df)
            
            # Calcular m√©tricas Evidently AI
            metricas_evidently = self.calcular_metricas_evidently(df_com_metricas)
            
            # Calcular m√©tricas de benchmarks
            metricas_benchmarks = self.calcular_metricas_benchmarks(df_com_metricas)
            
            # Gerar relat√≥rios Evidently AI
            try:
                # Tentar import relativo (quando chamado via main.py)
                from .evidently_reports import gerar_relatorios_evidently_completo
            except ImportError:
                # Fallback para import absoluto (quando executado diretamente)
                from evidently_reports import gerar_relatorios_evidently_completo
            
            try:
                pasta_evidently = os.path.join(pasta_modelo, "evidently_reports")
                relatorios_evidently, relatorio_evidently = gerar_relatorios_evidently_completo(df_com_metricas, pasta_evidently)
                print(f"üìä Relat√≥rios Evidently AI salvos em: {pasta_evidently}")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao gerar relat√≥rios Evidently AI: {e}")
                relatorios_evidently = {}
                relatorio_evidently = ""
            
            # Gerar relat√≥rio individual do modelo
            relatorio_modelo = self.gerar_relatorio_por_modelo(modelo, df_com_metricas, 
                                                              metricas_academicas, metricas_evidently, 
                                                              metricas_benchmarks, pasta_modelo)
            
            # Adicionar relat√≥rio Evidently AI
            if relatorio_evidently:
                relatorio_modelo += "\n\n" + relatorio_evidently
            
            # Adicionar an√°lise do HTML do Evidently AI
            pasta_evidently = os.path.join(pasta_modelo, "evidently_reports")
            if os.path.exists(pasta_evidently):
                relatorio_html = self._analisar_html_evidently(pasta_evidently)
                relatorio_modelo += "\n\n" + relatorio_html
            
            # Adicionar m√©tricas detalhadas do Evidently AI baseadas nos dados
            relatorio_detalhado = self._gerar_metricas_detalhadas_evidently(df_com_metricas)
            relatorio_modelo += "\n\n" + relatorio_detalhado
            
            # Salvar relat√≥rio do modelo
            arquivo_relatorio_modelo = os.path.join(pasta_modelo, f"relatorio_{modelo}.md")
            with open(arquivo_relatorio_modelo, 'w', encoding='utf-8') as f:
                f.write(relatorio_modelo)
            
            # Salvar dados do modelo
            df_com_metricas.to_csv(os.path.join(pasta_modelo, f"dados_{modelo}.csv"), 
                                  index=False, encoding=self.config.ENCODING_CSV)
            
            # Armazenar m√©tricas
            metricas_por_modelo[modelo] = {
                'academicas': metricas_academicas,
                'evidently': metricas_evidently,
                'benchmarks': metricas_benchmarks.get(modelo, {})
            }
            
            print(f"‚úÖ {modelo}: Relat√≥rio salvo em {arquivo_relatorio_modelo}")
        
        # Gerar relat√≥rio consolidado
        relatorio_consolidado = self.gerar_relatorio_consolidado(dados_por_modelo, metricas_por_modelo, pasta_analise)
        
        # Salvar relat√≥rio consolidado
        arquivo_relatorio_consolidado = os.path.join(pasta_analise, "relatorio_consolidado.md")
        with open(arquivo_relatorio_consolidado, 'w', encoding='utf-8') as f:
            f.write(relatorio_consolidado)
        
        # Salvar m√©tricas consolidadas
        with open(os.path.join(pasta_analise, "metricas_consolidadas.json"), 'w', encoding='utf-8') as f:
            json.dump(metricas_por_modelo, f, indent=2, ensure_ascii=False, default=str)
        
        # Gerar e salvar rankings detalhados
        self._salvar_rankings_detalhados(metricas_por_modelo, pasta_analise)
        
        print(f"\nüíæ An√°lise consolidada salva em: {pasta_analise}")
        print(f"üìÑ Relat√≥rio consolidado: {arquivo_relatorio_consolidado}")
        
        return arquivo_relatorio_consolidado

def executar_analise():
    """Fun√ß√£o principal para executar an√°lise consolidada."""
    analyzer = AnalysisSystem()
    return analyzer.executar_analise_completa()

if __name__ == "__main__":
    print("üöÄ Executando an√°lise consolidada diretamente...")
    print("=" * 60)
    
    try:
        resultado = executar_analise()
        if resultado:
            print(f"\n‚úÖ An√°lise conclu√≠da com sucesso!")
            print(f"üìÑ Relat√≥rio salvo em: {resultado}")
        else:
            print("\n‚ùå An√°lise n√£o p√¥de ser conclu√≠da")
    except Exception as e:
        print(f"\n‚ùå Erro durante a execu√ß√£o: {e}")
        import traceback
        traceback.print_exc()