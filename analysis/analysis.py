#!/usr/bin/env python3
"""
Sistema de An√°lise Consolidada de Modelos LLM
Varre pasta results, agrupa por modelo e gera an√°lises completas.
"""

import pandas as pd
import numpy as np
import os
import sys
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import json

# Adicionar o diret√≥rio pai ao path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.config import get_config

class AnalysisSystem:
    """Sistema principal de an√°lise consolidada."""
    
    def __init__(self):
        self.config = get_config()
        self.pasta_analysis = "analysis"
        self.pasta_resultados = self.config.PASTA_RESULTADOS
        self.prefixo_execucao = self.config.PREFIXO_EXECUCAO
    
    def encontrar_execucoes(self) -> List[str]:
        """Encontra todas as execu√ß√µes dispon√≠veis na pasta de resultados."""
        if not os.path.exists(self.pasta_resultados):
            print(f"‚ùå Pasta {self.pasta_resultados} n√£o encontrada")
            return []
        
        execucoes = []
        for item in os.listdir(self.pasta_resultados):
            if item.startswith(self.prefixo_execucao):
                caminho_execucao = os.path.join(self.pasta_resultados, item)
                if os.path.isdir(caminho_execucao):
                    execucoes.append(caminho_execucao)
        
        execucoes.sort()
        return execucoes
    
    def carregar_dados_execucao(self, caminho_execucao: str) -> Optional[pd.DataFrame]:
        """Carrega dados de uma execu√ß√£o espec√≠fica."""
        arquivo_csv = os.path.join(caminho_execucao, "resultados_todos.csv")
        
        if not os.path.exists(arquivo_csv):
            print(f"‚ùå Arquivo {arquivo_csv} n√£o encontrado")
            return None
        
        try:
            df = pd.read_csv(arquivo_csv, encoding=self.config.ENCODING_CSV)
            print(f"‚úÖ Dados carregados: {len(df)} registros de {caminho_execucao}")
            return df
        except Exception as e:
            print(f"‚ùå Erro ao carregar {arquivo_csv}: {e}")
            return None

    def consolidar_dados_por_modelo(self, execucoes: List[str]) -> Dict[str, pd.DataFrame]:
        """Consolida dados agrupando por modelo."""
        print("üîÑ Consolidando dados por modelo...")
        
        dados_por_modelo = {}
        
        for execucao in execucoes:
            nome_execucao = os.path.basename(execucao)
            print(f"üìÅ Processando {nome_execucao}...")
            
            # Carregar dados da execu√ß√£o
            df = self.carregar_dados_execucao(execucao)
            if df is None:
                continue
            
            # Adicionar coluna de execu√ß√£o
            df['execucao'] = nome_execucao
            
            # Agrupar por modelo
            for modelo in df['model'].unique():
                df_modelo = df[df['model'] == modelo].copy()
                
                if modelo not in dados_por_modelo:
                    dados_por_modelo[modelo] = []
                
                dados_por_modelo[modelo].append(df_modelo)
        
        # Concatenar dados de cada modelo
        dados_consolidados = {}
        for modelo, lista_dfs in dados_por_modelo.items():
            if lista_dfs:
                dados_consolidados[modelo] = pd.concat(lista_dfs, ignore_index=True)
                print(f"‚úÖ {modelo}: {len(dados_consolidados[modelo])} registros consolidados")
        
        return dados_consolidados
    
    def calcular_metricas_academicas(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict, str]:
        """Calcula todas as m√©tricas acad√™micas (BLEU, ROUGE, BERTScore)."""
        print("üìä Calculando m√©tricas acad√™micas...")
        
        # BLEU e ROUGE
        try:
            from .bleu_rouge import calcular_bleu_rouge_completo
            df_bleu_rouge, metricas_bleu_rouge, relatorio_bleu_rouge = calcular_bleu_rouge_completo(df)
        except Exception as e:
            print(f"‚ùå Erro ao calcular BLEU/ROUGE: {e}")
            return df, {}, "Erro ao calcular BLEU/ROUGE"
        
        # BERTScore
        try:
            from .bertscore import calcular_bertscore_completo
            df_bertscore, metricas_bertscore, relatorio_bertscore = calcular_bertscore_completo(df_bleu_rouge)
        except Exception as e:
            print(f"‚ùå Erro ao calcular BERTScore: {e}")
            return df_bleu_rouge, metricas_bleu_rouge, relatorio_bleu_rouge
        
        # Calcular m√©tricas agregadas por modelo
        metricas_agregadas_dict = self._calcular_metricas_agregadas(df_bertscore)
        
        # Extrair m√©tricas do modelo atual (assumindo que h√° apenas um modelo no DataFrame)
        if metricas_agregadas_dict:
            modelo_atual = df_bertscore['model'].iloc[0]
            metricas_agregadas = metricas_agregadas_dict.get(modelo_atual, {})
        else:
            metricas_agregadas = {}
        
        # Combinar relat√≥rios
        relatorio_completo = f"{relatorio_bleu_rouge}\n\n{relatorio_bertscore}"
        
        return df_bertscore, metricas_agregadas, relatorio_completo
    
    def calcular_metricas_evidently(self, df: pd.DataFrame) -> Dict:
        """Calcula m√©tricas do Evidently AI para cada modelo."""
        print("üìà Calculando m√©tricas Evidently AI...")
        
        metricas_evidently = {}
        
        # Preparar dados
        df_evidently = df.copy()
        
        # Usar campos de comprimento se dispon√≠veis (nova pipeline), sen√£o calcular
        if 'response_length' in df_evidently.columns:
            df_evidently['text_length'] = df_evidently['response_length']
        else:
            df_evidently['text_length'] = df_evidently['prediction'].astype(str).str.len()
        
        if 'word_count' in df_evidently.columns:
            # Manter word_count se j√° existe
            pass
        else:
            df_evidently['word_count'] = df_evidently['prediction'].astype(str).str.split().str.len()
        
        # Usar campo is_error se dispon√≠vel, sen√£o calcular is_valid
        if self._usar_campo_is_error(df):
            df_evidently['is_valid'] = ~df_evidently['is_error']
        else:
            df_evidently['is_valid'] = ~df_evidently['prediction'].apply(self._eh_resposta_invalida)
        
        # Calcular m√©tricas
        total_respostas = len(df_evidently)
        respostas_validas = len(df_evidently[df_evidently['is_valid']])
        taxa_validas = respostas_validas / total_respostas if total_respostas > 0 else 0
        
        df_validas = df_evidently[df_evidently['is_valid']]
        
        metricas_evidently = {
            'total_respostas': total_respostas,
            'respostas_validas': respostas_validas,
            'taxa_validas': taxa_validas,
            'comprimento_medio': df_validas['text_length'].mean() if len(df_validas) > 0 else 0,
            'comprimento_std': df_validas['text_length'].std() if len(df_validas) > 0 else 0,
            'palavras_medias': df_validas['word_count'].mean() if len(df_validas) > 0 else 0,
            'palavras_std': df_validas['word_count'].std() if len(df_validas) > 0 else 0
        }
        
        return metricas_evidently
    
    def _calcular_metricas_agregadas(self, df: pd.DataFrame) -> Dict:
        """Calcula m√©tricas agregadas por modelo."""
        if 'model' not in df.columns:
            return {}

        metricas_agregadas = {}

        for modelo in df['model'].unique():
            df_modelo = df[df['model'] == modelo]
            
            # Filtrar apenas respostas v√°lidas - usar campo is_error se dispon√≠vel
            if self._usar_campo_is_error(df):
                df_validas = df_modelo[~df_modelo['is_error']]
            else:
                df_validas = df_modelo[~df_modelo['prediction'].apply(self._eh_resposta_invalida)]
            
            if len(df_validas) == 0:
                metricas_agregadas[modelo] = {
                    'bleu_medio': 0.0,
                    'rouge1_medio': 0.0,
                    'rouge2_medio': 0.0,
                    'rougeL_medio': 0.0,
                    'bertscore_f1_medio': 0.0,
                    'total_respostas': len(df_modelo),
                    'respostas_validas': 0,
                    'taxa_validas': 0.0
                }
                continue
            
            metricas_agregadas[modelo] = {
                'bleu_medio': df_validas['bleu_score'].mean(),
                'rouge1_medio': df_validas['rouge1_score'].mean(),
                'rouge2_medio': df_validas['rouge2_score'].mean(),
                'rougeL_medio': df_validas['rougeL_score'].mean(),
                'bertscore_f1_medio': df_validas['bertscore_f1'].mean(),
                'bleu_std': df_validas['bleu_score'].std(),
                'rouge1_std': df_validas['rouge1_score'].std(),
                'rouge2_std': df_validas['rouge2_score'].std(),
                'rougeL_std': df_validas['rougeL_score'].std(),
                'bertscore_f1_std': df_validas['bertscore_f1'].std(),
                'total_respostas': len(df_modelo),
                'respostas_validas': len(df_validas),
                'taxa_validas': len(df_validas) / len(df_modelo)
            }
            
        return metricas_agregadas
    
    def _analisar_html_evidently(self, pasta_evidently: str) -> str:
        """Analisa os relat√≥rios HTML do Evidently AI e extrai informa√ß√µes relevantes."""
        try:
            import re
            from bs4 import BeautifulSoup
            
            relatorio_html = ""
            
            # Analisar relat√≥rio de qualidade
            arquivo_qualidade = os.path.join(pasta_evidently, "evidently_qualidade.html")
            if os.path.exists(arquivo_qualidade):
                with open(arquivo_qualidade, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    relatorio_html += "### üìä An√°lise de Qualidade dos Dados\n"
                    relatorio_html += "**M√©tricas extra√≠das do relat√≥rio Evidently AI:**\n\n"
                    
                    # Extrair m√©tricas mais espec√≠ficas
                    metricas_qualidade = self._extrair_metricas_qualidade(soup)
                    if metricas_qualidade:
                        for metrica, valor in metricas_qualidade.items():
                            relatorio_html += f"- **{metrica}**: {valor}\n"
                    else:
                        relatorio_html += "- **Status**: Relat√≥rio de qualidade gerado com sucesso\n"
                        relatorio_html += "- **Conte√∫do**: An√°lise de distribui√ß√µes, valores ausentes e correla√ß√µes\n"
                    
                    relatorio_html += "\n"
            
            # Analisar relat√≥rio de texto
            arquivo_texto = os.path.join(pasta_evidently, "evidently_texto.html")
            if os.path.exists(arquivo_texto):
                with open(arquivo_texto, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    relatorio_html += "### üìù An√°lise de Texto\n"
                    relatorio_html += "**M√©tricas de an√°lise textual:**\n\n"
                    
                    # Extrair m√©tricas de texto mais espec√≠ficas
                    metricas_texto = self._extrair_metricas_texto(soup)
                    if metricas_texto:
                        for metrica, valor in metricas_texto.items():
                            relatorio_html += f"- **{metrica}**: {valor}\n"
                    else:
                        relatorio_html += "- **Status**: Relat√≥rio de texto gerado com sucesso\n"
                        relatorio_html += "- **Conte√∫do**: An√°lise de comprimento, qualidade e descritores textuais\n"
                    
                    relatorio_html += "\n"
            
            if not relatorio_html:
                relatorio_html = "### üìä An√°lise Evidently AI\n**Relat√≥rios HTML gerados com sucesso.**\n\n"
            
            return relatorio_html
            
        except Exception as e:
            return f"### üìä An√°lise Evidently AI\n**Erro ao analisar HTML**: {str(e)}\n\n"
    
    def _extrair_metricas_qualidade(self, soup) -> dict:
        """Extrai m√©tricas espec√≠ficas do relat√≥rio de qualidade."""
        import re
        metricas = {}
        
        try:
            # Procurar por tabelas com m√©tricas
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text().strip()
                        value = cells[1].get_text().strip()
                        
                        # Filtrar m√©tricas relevantes
                        if any(keyword in label.lower() for keyword in ['missing', 'null', 'count', 'mean', 'std', 'min', 'max']):
                            if re.match(r'^\d+\.?\d*$', value) or '%' in value:
                                metricas[label] = value
            
            # Procurar por elementos com classes espec√≠ficas
            for element in soup.find_all(['span', 'div'], class_=re.compile(r'value|metric|stat')):
                text = element.get_text().strip()
                if re.match(r'^\d+\.?\d*$', text) and len(text) > 0:
                    # Tentar encontrar o label associado
                    parent = element.parent
                    if parent:
                        label_elem = parent.find(['span', 'div', 'td'], class_=re.compile(r'label|name|title'))
                        if label_elem:
                            label = label_elem.get_text().strip()
                            metricas[label] = text
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair m√©tricas de qualidade: {e}")
        
        return metricas
    
    def _extrair_metricas_texto(self, soup) -> dict:
        """Extrai m√©tricas espec√≠ficas do relat√≥rio de texto."""
        import re
        metricas = {}
        
        try:
            # Procurar por m√©tricas de texto espec√≠ficas
            text_keywords = ['length', 'word', 'character', 'sentence', 'paragraph', 'quality', 'readability']
            
            for element in soup.find_all(['span', 'div', 'td']):
                text = element.get_text().strip()
                if any(keyword in text.lower() for keyword in text_keywords):
                    # Verificar se √© um valor num√©rico
                    if re.match(r'^\d+\.?\d*$', text) or '%' in text:
                        # Tentar encontrar o label
                        parent = element.parent
                        if parent:
                            label_elem = parent.find(['span', 'div', 'td'], class_=re.compile(r'label|name|title'))
                            if label_elem:
                                label = label_elem.get_text().strip()
                                metricas[label] = text
            
            # Procurar por tabelas com m√©tricas de texto
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text().strip()
                        value = cells[1].get_text().strip()
                        
                        if any(keyword in label.lower() for keyword in text_keywords):
                            if re.match(r'^\d+\.?\d*$', value) or '%' in value:
                                metricas[label] = value
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair m√©tricas de texto: {e}")
        
        return metricas
    
    def _gerar_metricas_detalhadas_evidently(self, df: pd.DataFrame) -> str:
        """Gera m√©tricas detalhadas do Evidently AI baseadas nos dados."""
        try:
            relatorio = []
            relatorio.append("### üìä M√©tricas Detalhadas Evidently AI")
            relatorio.append("**An√°lise baseada nos dados processados:**\n")

            # Preparar dados
            df_evidently = df.copy()
            
            # Usar campos de comprimento se dispon√≠veis (nova pipeline), sen√£o calcular
            if 'response_length' in df_evidently.columns:
                df_evidently['text_length'] = df_evidently['response_length']
            else:
                df_evidently['text_length'] = df_evidently['prediction'].astype(str).str.len()
            
            if 'word_count' in df_evidently.columns:
                # Manter word_count se j√° existe
                pass
            else:
                df_evidently['word_count'] = df_evidently['prediction'].astype(str).str.split().str.len()
            
            # Usar campo is_error se dispon√≠vel, sen√£o calcular is_valid
            if self._usar_campo_is_error(df):
                df_evidently['is_valid'] = ~df_evidently['is_error']
            else:
                df_evidently['is_valid'] = ~df_evidently['prediction'].apply(self._eh_resposta_invalida)
            
            # Filtrar apenas respostas v√°lidas
            df_validas = df_evidently[df_evidently['is_valid']]
            
            if len(df_validas) == 0:
                relatorio.append("- **Status**: Nenhuma resposta v√°lida encontrada")
                return "\n".join(relatorio)
            
            # M√©tricas de qualidade de dados
            relatorio.append("#### üîç Qualidade de Dados")
            relatorio.append(f"- **Total de Registros**: {len(df_evidently)}")
            relatorio.append(f"- **Respostas V√°lidas**: {len(df_validas)}")
            relatorio.append(f"- **Taxa de Validade**: {len(df_validas)/len(df_evidently):.1%}")
            relatorio.append(f"- **Respostas Inv√°lidas**: {len(df_evidently) - len(df_validas)}")
            relatorio.append("")
            
            # M√©tricas de texto
            relatorio.append("#### üìù An√°lise de Texto")
            relatorio.append(f"- **Comprimento M√©dio**: {df_validas['text_length'].mean():.1f} caracteres")
            relatorio.append(f"- **Comprimento M√≠nimo**: {df_validas['text_length'].min():.0f} caracteres")
            relatorio.append(f"- **Comprimento M√°ximo**: {df_validas['text_length'].max():.0f} caracteres")
            relatorio.append(f"- **Desvio Padr√£o**: {df_validas['text_length'].std():.1f} caracteres")
            relatorio.append("")
            
            relatorio.append(f"- **Palavras M√©dias**: {df_validas['word_count'].mean():.1f}")
            relatorio.append(f"- **Palavras M√≠nimas**: {df_validas['word_count'].min():.0f}")
            relatorio.append(f"- **Palavras M√°ximas**: {df_validas['word_count'].max():.0f}")
            relatorio.append(f"- **Desvio Padr√£o Palavras**: {df_validas['word_count'].std():.1f}")
            relatorio.append("")
            
            # An√°lise de distribui√ß√£o
            relatorio.append("#### üìä Distribui√ß√£o de Comprimento")
            q25 = df_validas['text_length'].quantile(0.25)
            q50 = df_validas['text_length'].quantile(0.50)
            q75 = df_validas['text_length'].quantile(0.75)
            
            relatorio.append(f"- **Q1 (25%)**: {q25:.1f} caracteres")
            relatorio.append(f"- **Mediana (50%)**: {q50:.1f} caracteres")
            relatorio.append(f"- **Q3 (75%)**: {q75:.1f} caracteres")
            relatorio.append(f"- **Amplitude Interquartil**: {q75 - q25:.1f} caracteres")
            relatorio.append("")
            
            # An√°lise de consist√™ncia
            cv_comprimento = (df_validas['text_length'].std() / df_validas['text_length'].mean()) * 100
            cv_palavras = (df_validas['word_count'].std() / df_validas['word_count'].mean()) * 100
            
            relatorio.append("#### ‚öñÔ∏è Consist√™ncia")
            relatorio.append(f"- **Coeficiente de Varia√ß√£o (Comprimento)**: {cv_comprimento:.1f}%")
            relatorio.append(f"- **Coeficiente de Varia√ß√£o (Palavras)**: {cv_palavras:.1f}%")
            
            if cv_comprimento < 20:
                consistencia = "‚úÖ Muito consistente"
            elif cv_comprimento < 40:
                consistencia = "‚ö†Ô∏è Moderadamente consistente"
            else:
                consistencia = "‚ùå Pouco consistente"
            
            relatorio.append(f"- **Avalia√ß√£o de Consist√™ncia**: {consistencia}")
            relatorio.append("")
            
            # An√°lise de outliers
            relatorio.append("#### üéØ An√°lise de Outliers")
            q1 = df_validas['text_length'].quantile(0.25)
            q3 = df_validas['text_length'].quantile(0.75)
            iqr = q3 - q1
            limite_inferior = q1 - 1.5 * iqr
            limite_superior = q3 + 1.5 * iqr
            
            outliers = df_validas[(df_validas['text_length'] < limite_inferior) | 
                                 (df_validas['text_length'] > limite_superior)]
            
            relatorio.append(f"- **Outliers Detectados**: {len(outliers)}")
            relatorio.append(f"- **Limite Inferior**: {limite_inferior:.1f} caracteres")
            relatorio.append(f"- **Limite Superior**: {limite_superior:.1f} caracteres")
            relatorio.append("")
            
            # Resumo estat√≠stico
            relatorio.append("#### üìà Resumo Estat√≠stico")
            relatorio.append(f"- **M√©dia Aritm√©tica**: {df_validas['text_length'].mean():.1f}")
            relatorio.append(f"- **Mediana**: {df_validas['text_length'].median():.1f}")
            relatorio.append(f"- **Moda**: {df_validas['text_length'].mode().iloc[0] if len(df_validas['text_length'].mode()) > 0 else 'N/A'}")
            relatorio.append(f"- **Vari√¢ncia**: {df_validas['text_length'].var():.1f}")
            relatorio.append(f"- **Assimetria**: {df_validas['text_length'].skew():.3f}")
            relatorio.append(f"- **Curtose**: {df_validas['text_length'].kurtosis():.3f}")
            
            return "\n".join(relatorio)
            
        except Exception as e:
            return f"### üìä M√©tricas Detalhadas Evidently AI\n**Erro ao gerar m√©tricas**: {str(e)}\n"
    
    def _eh_resposta_invalida(self, resposta: str) -> bool:
        """Verifica se uma resposta √© inv√°lida."""
        if not resposta or pd.isna(resposta):
            return True
        
        resposta_str = str(resposta).strip().lower()
        padroes_erro = [
            'erro', 'error', 'timeout', 'rate limit', 'api key',
            'authentication', 'connection', 'network', 'failed',
            'exception', 'traceback', 'null', 'none', 'undefined'
        ]
        
        return any(padrao in resposta_str for padrao in padroes_erro)
    
    def _usar_campo_is_error(self, df: pd.DataFrame) -> bool:
        """Verifica se o DataFrame tem o campo is_error (nova vers√£o da pipeline)."""
        return 'is_error' in df.columns
    
    def gerar_relatorio_por_modelo(self, modelo: str, df: pd.DataFrame, 
                                 metricas_academicas: Dict, metricas_evidently: Dict,
                                 pasta_destino: str) -> str:
        """Gera relat√≥rio individual por modelo."""
        relatorio = []
        relatorio.append(f"# ü§ñ An√°lise do Modelo: {modelo}")
        relatorio.append("")
        relatorio.append(f"**Data da An√°lise**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        relatorio.append(f"**Total de Respostas**: {len(df)}")
        relatorio.append(f"**Execu√ß√µes**: {', '.join(df['execucao'].unique())}")
        relatorio.append("")
        
        # M√©tricas acad√™micas
        relatorio.append("## üìä M√©tricas Acad√™micas")
        relatorio.append("")
        relatorio.append(f"- **BLEU Score**: {metricas_academicas.get('bleu_medio', 0):.4f}")
        relatorio.append(f"- **ROUGE-1**: {metricas_academicas.get('rouge1_medio', 0):.4f}")
        relatorio.append(f"- **ROUGE-2**: {metricas_academicas.get('rouge2_medio', 0):.4f}")
        relatorio.append(f"- **ROUGE-L**: {metricas_academicas.get('rougeL_medio', 0):.4f}")
        relatorio.append(f"- **BERTScore**: {metricas_academicas.get('bertscore_f1_medio', 0):.4f}")
        relatorio.append("")
        
        # M√©tricas Evidently AI
        relatorio.append("## üìà M√©tricas Evidently AI")
        relatorio.append("")
        relatorio.append(f"- **Respostas V√°lidas**: {metricas_evidently.get('respostas_validas', 0)}")
        relatorio.append(f"- **Taxa de Validade**: {metricas_evidently.get('taxa_validas', 0):.1%}")
        relatorio.append(f"- **Comprimento M√©dio**: {metricas_evidently.get('comprimento_medio', 0):.1f} ¬± {metricas_evidently.get('comprimento_std', 0):.1f} caracteres")
        relatorio.append(f"- **Palavras M√©dias**: {metricas_evidently.get('palavras_medias', 0):.1f} ¬± {metricas_evidently.get('palavras_std', 0):.1f}")
        relatorio.append("")
        
        # An√°lise de consist√™ncia
        comprimento_std = metricas_evidently.get('comprimento_std', 0)
        comprimento_medio = metricas_evidently.get('comprimento_medio', 0)
        
        if comprimento_medio > 0:
            cv_comprimento = (comprimento_std / comprimento_medio) * 100
            if cv_comprimento < 20:
                consistencia = "‚úÖ Muito consistente"
            elif cv_comprimento < 40:
                consistencia = "‚ö†Ô∏è Moderadamente consistente"
            else:
                consistencia = "‚ùå Pouco consistente"
            
            relatorio.append(f"- **Consist√™ncia de Comprimento**: {consistencia} (CV: {cv_comprimento:.1f}%)")
            relatorio.append("")
        
        # Avalia√ß√£o geral
        taxa_validas = metricas_evidently.get('taxa_validas', 0)
        if taxa_validas > 0.9:
            status_geral = "‚úÖ Excelente"
        elif taxa_validas > 0.7:
            status_geral = "‚ö†Ô∏è Bom"
        elif taxa_validas > 0.5:
            status_geral = "‚ö†Ô∏è Regular"
        else:
            status_geral = "‚ùå Ruim"
        
        relatorio.append(f"**Avalia√ß√£o Geral**: {status_geral}")
        relatorio.append("")
        
        return "\n".join(relatorio)
    
    def gerar_relatorio_consolidado(self, dados_por_modelo: Dict[str, pd.DataFrame],
                                  metricas_por_modelo: Dict[str, Dict],
                                  pasta_analise: str) -> str:
        """Gera relat√≥rio consolidado final."""
        print("üìù Gerando relat√≥rio consolidado...")
        
        relatorio = []
        relatorio.append("# üìä Relat√≥rio Consolidado de An√°lise de Modelos LLM")
        relatorio.append("")
        relatorio.append(f"**Data da An√°lise**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        
        # Estat√≠sticas gerais
        total_respostas = sum(len(df) for df in dados_por_modelo.values())
        total_modelos = len(dados_por_modelo)
        execucoes_unicas = set()
        for df in dados_por_modelo.values():
            execucoes_unicas.update(df['execucao'].unique())
        
        relatorio.append(f"**Total de Respostas**: {total_respostas}")
        relatorio.append(f"**Modelos Avaliados**: {total_modelos}")
        relatorio.append(f"**Execu√ß√µes Analisadas**: {len(execucoes_unicas)}")
        relatorio.append(f"**Execu√ß√µes**: {', '.join(sorted(execucoes_unicas))}")
        
        # Informa√ß√µes sobre metadados dispon√≠veis
        primeiro_df = next(iter(dados_por_modelo.values()))
        if self._usar_campo_is_error(primeiro_df):
            relatorio.append("**Metadados**: ‚úÖ Timestamp, comprimento de prompt/resposta, flags de erro")
        else:
            relatorio.append("**Metadados**: ‚ö†Ô∏è Vers√£o anterior da pipeline (sem metadados otimizados)")
        relatorio.append("")
        
        # Resumo executivo
        relatorio.append("## üìà Resumo Executivo")
        relatorio.append("")
        
        total_validas = sum(metricas_por_modelo[modelo]['evidently'].get('respostas_validas', 0) 
                           for modelo in dados_por_modelo.keys())
        taxa_geral = total_validas / total_respostas if total_respostas > 0 else 0
        
        relatorio.append(f"- **Respostas V√°lidas**: {total_validas}")
        relatorio.append(f"- **Taxa de Sucesso Geral**: {taxa_geral:.1%}")
        relatorio.append("")
        
        # Ranking dos modelos
        relatorio.append("## üèÜ Ranking dos Modelos")
        relatorio.append("")
        
        modelos_ranking = self._calcular_ranking_modelos(metricas_por_modelo)
        
        for i, (modelo, score_composto) in enumerate(modelos_ranking, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∫"
            relatorio.append(f"### {emoji} {modelo} (Score: {score_composto:.4f})")
            relatorio.append("")
            
            # M√©tricas acad√™micas
            metricas_acad = metricas_por_modelo[modelo]['academicas']
            relatorio.append("**M√©tricas Acad√™micas:**")
            relatorio.append(f"- **BLEU Score**: {metricas_acad.get('bleu_medio', 0):.4f}")
            relatorio.append(f"- **ROUGE-1**: {metricas_acad.get('rouge1_medio', 0):.4f}")
            relatorio.append(f"- **ROUGE-2**: {metricas_acad.get('rouge2_medio', 0):.4f}")
            relatorio.append(f"- **ROUGE-L**: {metricas_acad.get('rougeL_medio', 0):.4f}")
            relatorio.append(f"- **BERTScore**: {metricas_acad.get('bertscore_f1_medio', 0):.4f}")
            relatorio.append("")
            
            # M√©tricas Evidently AI
            metricas_ev = metricas_por_modelo[modelo]['evidently']
            relatorio.append("**M√©tricas Evidently AI:**")
            relatorio.append(f"- **Respostas V√°lidas**: {metricas_ev.get('respostas_validas', 0)}")
            relatorio.append(f"- **Taxa de Validade**: {metricas_ev.get('taxa_validas', 0):.1%}")
            relatorio.append(f"- **Comprimento M√©dio**: {metricas_ev.get('comprimento_medio', 0):.1f} ¬± {metricas_ev.get('comprimento_std', 0):.1f} caracteres")
            relatorio.append(f"- **Palavras M√©dias**: {metricas_ev.get('palavras_medias', 0):.1f} ¬± {metricas_ev.get('palavras_std', 0):.1f}")
            relatorio.append("")
            relatorio.append("---")
            relatorio.append("")
        
        # An√°lise comparativa
        relatorio.append("## üìä An√°lise Comparativa")
        relatorio.append("")
        
        # Ranking por confiabilidade
        modelos_confiabilidade = [(modelo, metricas_por_modelo[modelo]['evidently'].get('taxa_validas', 0)) 
                                 for modelo in dados_por_modelo.keys()]
        modelos_confiabilidade.sort(key=lambda x: x[1], reverse=True)
        
        relatorio.append("**Ranking por Confiabilidade:**")
        for i, (modelo, taxa) in enumerate(modelos_confiabilidade, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∫"
            relatorio.append(f"{emoji} **{modelo}**: {taxa:.1%}")
            relatorio.append("")
            
        # Ranking por comprimento
        modelos_comprimento = [(modelo, metricas_por_modelo[modelo]['evidently'].get('comprimento_medio', 0)) 
                              for modelo in dados_por_modelo.keys()]
        modelos_comprimento.sort(key=lambda x: x[1], reverse=True)
        
        relatorio.append("**Ranking por Comprimento de Resposta:**")
        for i, (modelo, comprimento) in enumerate(modelos_comprimento, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∫"
            relatorio.append(f"{emoji} **{modelo}**: {comprimento:.1f} caracteres")
            relatorio.append("")
        
        # Recomenda√ß√µes
        relatorio.append("## üí° Recomenda√ß√µes")
        relatorio.append("")
        
        if modelos_ranking:
            melhor_modelo = modelos_ranking[0][0]
            relatorio.append(f"### üèÜ Modelo Recomendado: {melhor_modelo}")
            relatorio.append("")
            relatorio.append("**Justificativa:**")
            relatorio.append("- Melhor score composto considerando todas as m√©tricas")
            relatorio.append("- Equil√≠brio entre precis√£o acad√™mica e confiabilidade")
            relatorio.append("- Boa performance em m√©tricas de qualidade textual")
            relatorio.append("")
        
        if modelos_confiabilidade:
            mais_confiavel = modelos_confiabilidade[0][0]
            relatorio.append(f"### üõ°Ô∏è Modelo Mais Confi√°vel: {mais_confiavel}")
            relatorio.append(f"- Taxa de respostas v√°lidas: {modelos_confiabilidade[0][1]:.1%}")
            relatorio.append("")
        
        if modelos_comprimento:
            mais_detalhado = modelos_comprimento[0][0]
            relatorio.append(f"### üìù Modelo Mais Detalhado: {mais_detalhado}")
            relatorio.append(f"- Comprimento m√©dio: {modelos_comprimento[0][1]:.1f} caracteres")
        relatorio.append("")
        
        return "\n".join(relatorio)
    
    def _calcular_ranking_modelos(self, metricas_por_modelo: Dict[str, Dict]) -> List[tuple]:
        """Calcula ranking dos modelos baseado em score composto."""
        rankings = []
        
        for modelo, metricas in metricas_por_modelo.items():
            # M√©tricas acad√™micas
            metricas_acad = metricas['academicas']
            bleu = metricas_acad.get('bleu_medio', 0)
            rouge1 = metricas_acad.get('rouge1_medio', 0)
            rouge2 = metricas_acad.get('rouge2_medio', 0)
            rougeL = metricas_acad.get('rougeL_medio', 0)
            bertscore = metricas_acad.get('bertscore_f1_medio', 0)
            
            # M√©tricas Evidently AI
            metricas_ev = metricas['evidently']
            taxa_validas = metricas_ev.get('taxa_validas', 0)
            respostas_validas = metricas_ev.get('respostas_validas', 0)
            
            # Penalizar modelos com poucas respostas v√°lidas
            fator_confiabilidade = min(1.0, respostas_validas / 10.0)  # Penaliza se < 10 respostas v√°lidas
            
            # Score composto com penaliza√ß√£o por baixa confiabilidade
            score_composto = (
                bleu * 0.15 +
                rouge1 * 0.20 +
                rouge2 * 0.15 +
                rougeL * 0.15 +
                bertscore * 0.25 +
                taxa_validas * 0.10
            ) * fator_confiabilidade
            
            # Penaliza√ß√£o adicional para modelos com muito poucas respostas v√°lidas
            if respostas_validas < 5:
                score_composto *= 0.3  # Penaliza√ß√£o severa
            elif respostas_validas < 10:
                score_composto *= 0.6  # Penaliza√ß√£o moderada
            
            rankings.append((modelo, score_composto))
        
        return sorted(rankings, key=lambda x: x[1], reverse=True)
    
    def executar_analise_completa(self) -> str:
        """Executa an√°lise completa e retorna caminho do relat√≥rio."""
        print("üöÄ Iniciando An√°lise Consolidada")
        print("=" * 60)
        
        # Encontrar execu√ß√µes
        execucoes = self.encontrar_execucoes()
        print(f"üìÅ Encontradas {len(execucoes)} execu√ß√µes")
        
        if not execucoes:
            print("‚ùå Nenhuma execu√ß√£o encontrada para an√°lise")
            return None
        
        # Consolidar dados por modelo
        dados_por_modelo = self.consolidar_dados_por_modelo(execucoes)
        
        if not dados_por_modelo:
            print("‚ùå Nenhum modelo encontrado para an√°lise")
            return None
        
        # Criar pasta de an√°lise
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pasta_analise = os.path.join(self.pasta_analysis, f"analise_consolidada_{timestamp}")
        os.makedirs(pasta_analise, exist_ok=True)
        
        # Processar cada modelo
        metricas_por_modelo = {}
        
        for modelo, df in dados_por_modelo.items():
            print(f"\nü§ñ Processando modelo: {modelo}")
            
            # Criar pasta do modelo
            pasta_modelo = os.path.join(pasta_analise, f"modelo_{modelo}")
            os.makedirs(pasta_modelo, exist_ok=True)
            
            # Calcular m√©tricas acad√™micas
            df_com_metricas, metricas_academicas, relatorio_academicas = self.calcular_metricas_academicas(df)
            
            # Calcular m√©tricas Evidently AI
            metricas_evidently = self.calcular_metricas_evidently(df_com_metricas)
            
            # Gerar relat√≥rios Evidently AI
            try:
                from .evidently_reports import gerar_relatorios_evidently_completo
                pasta_evidently = os.path.join(pasta_modelo, "evidently_reports")
                relatorios_evidently, relatorio_evidently = gerar_relatorios_evidently_completo(df_com_metricas, pasta_evidently)
                print(f"üìä Relat√≥rios Evidently AI salvos em: {pasta_evidently}")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao gerar relat√≥rios Evidently AI: {e}")
                relatorios_evidently = {}
                relatorio_evidently = ""
            
            # Gerar relat√≥rio individual do modelo
            relatorio_modelo = self.gerar_relatorio_por_modelo(modelo, df_com_metricas, 
                                                              metricas_academicas, metricas_evidently, pasta_modelo)
            
            # Adicionar relat√≥rio Evidently AI
            if relatorio_evidently:
                relatorio_modelo += "\n\n" + relatorio_evidently
            
            # Adicionar an√°lise do HTML do Evidently AI
            pasta_evidently = os.path.join(pasta_modelo, "evidently_reports")
            if os.path.exists(pasta_evidently):
                relatorio_html = self._analisar_html_evidently(pasta_evidently)
                relatorio_modelo += "\n\n" + relatorio_html
            
            # Adicionar m√©tricas detalhadas do Evidently AI baseadas nos dados
            relatorio_detalhado = self._gerar_metricas_detalhadas_evidently(df_com_metricas)
            relatorio_modelo += "\n\n" + relatorio_detalhado
            
            # Salvar relat√≥rio do modelo
            arquivo_relatorio_modelo = os.path.join(pasta_modelo, f"relatorio_{modelo}.md")
            with open(arquivo_relatorio_modelo, 'w', encoding='utf-8') as f:
                f.write(relatorio_modelo)
            
            # Salvar dados do modelo
            df_com_metricas.to_csv(os.path.join(pasta_modelo, f"dados_{modelo}.csv"), 
                                  index=False, encoding=self.config.ENCODING_CSV)
            
            # Armazenar m√©tricas
            metricas_por_modelo[modelo] = {
                'academicas': metricas_academicas,
                'evidently': metricas_evidently
            }
            
            print(f"‚úÖ {modelo}: Relat√≥rio salvo em {arquivo_relatorio_modelo}")
        
        # Gerar relat√≥rio consolidado
        relatorio_consolidado = self.gerar_relatorio_consolidado(dados_por_modelo, metricas_por_modelo, pasta_analise)
        
        # Salvar relat√≥rio consolidado
        arquivo_relatorio_consolidado = os.path.join(pasta_analise, "relatorio_consolidado.md")
        with open(arquivo_relatorio_consolidado, 'w', encoding='utf-8') as f:
            f.write(relatorio_consolidado)
        
        # Salvar m√©tricas consolidadas
        with open(os.path.join(pasta_analise, "metricas_consolidadas.json"), 'w', encoding='utf-8') as f:
            json.dump(metricas_por_modelo, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nüíæ An√°lise consolidada salva em: {pasta_analise}")
        print(f"üìÑ Relat√≥rio consolidado: {arquivo_relatorio_consolidado}")
        
        return arquivo_relatorio_consolidado

def executar_analise():
    """Fun√ß√£o principal para executar an√°lise consolidada."""
    analyzer = AnalysisSystem()
    return analyzer.executar_analise_completa()

if __name__ == "__main__":
    executar_analise()